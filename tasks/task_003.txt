# Task ID: 3
# Title: Implement Redis Caching Layer
# Status: done
# Dependencies: 1
# Priority: medium
# Description: Create a caching system using Redis to store AI responses from multiple providers (OpenAI, Anthropic, Mistral, and DeepSeek) to avoid duplicate API calls, reducing costs and improving performance. Optimize for Directus integration patterns with provider-specific caching strategies.
# Details:
# Redis Caching Layer Implementation Summary

## âœ… Completed Features

### 1. Core Cache Service (`ai_response_cache.py`)
- **Multi-provider cache isolation** - Separate cache keys for OpenAI, Anthropic, Mistral, DeepSeek
- **Cost-aware TTL strategies** - Longer cache times for expensive providers (Claude Opus = 2x TTL, GPT-4 = 1.5x)
- **Intelligent compression** - Automatic zlib compression for responses >1KB
- **Provider-specific metrics** - Hit/miss tracking per provider and model
- **Directus collection support** - Collection-aware cache keys and invalidation
- **Cache warming and batch operations** - Efficient bulk caching with pipelines

### 2. Cache Integration Service (`cached_translation_service.py`)
- **Middleware layer** - Wraps existing provider router with caching
- **Cascading fallback** - Tries cache first, then AI providers in order
- **Batch translation support** - Efficient caching for multiple requests
- **Statistics and management** - Cache stats and invalidation methods

### 3. API Endpoints (`/api/cache.py`)
- `GET /api/v1/cache/stats` - Cache hit/miss statistics
- `GET /api/v1/cache/info` - Memory usage and key counts
- `DELETE /api/v1/cache/invalidate` - Targeted cache invalidation
- `DELETE /api/v1/cache/clear` - Emergency cache clearing

### 4. Application Integration
- **Lifecycle management** - Proper Redis connection handling in FastAPI
- **Docker integration** - Redis service configured in docker-compose.yml
- **Configuration** - Redis URL and cache settings in config.py

### 5. Testing Infrastructure
- **Unit tests** - Cache key generation, TTL calculation, compression
- **Integration tests** - Cache hit/miss scenarios with mocked services
- **Cost tier validation** - Ensures expensive providers get longer TTL

## ðŸ“Š Cache Configuration

### Provider Cost Tiers (TTL Multipliers):
- **Very High (2.0x)**: Claude-3 Opus
- **High (1.5x)**: GPT-4, Claude-3 Sonnet, Mistral Large  
- **Medium (1.0x)**: Claude-3 Haiku, DeepSeek models
- **Low (0.8x)**: GPT-3.5-Turbo, Mistral Tiny

### Content Type TTL:
- **Critical**: 12 hours (0.5x)
- **Standard**: 24 hours (1.0x)
- **Static**: 7 days (7.0x)
- **Temporary**: 6 hours (0.25x)

## ðŸ”§ Key Technical Decisions

1. **Async Redis client** - Uses redis.asyncio for non-blocking operations
2. **Content hashing** - MD5 hash of prompt + target language for uniqueness
3. **Compression threshold** - 1KB threshold balances performance vs storage
4. **Pipeline operations** - Batch operations use Redis pipelines for efficiency
5. **Scan-based operations** - Uses SCAN instead of KEYS for better performance
6. **Singleton pattern** - Global cache instance with proper lifecycle management

## ðŸš€ Ready for Integration

The cache layer is now ready to integrate with:
- Task #2: AI Provider Integration (can now use `CachedTranslationService`)
- Task #4: Field Mapping (collection-aware caching)
- Task #5: Translation API Endpoints (cache statistics endpoints)
- Task #6: Directus Integration (collection invalidation)

## ðŸ“ˆ Performance Benefits

- **Cost savings** - Avoids duplicate API calls to expensive providers
- **Response time** - Cached responses return instantly
- **Rate limit protection** - Reduces API requests within rate limits
- **Intelligent TTL** - Balances freshness vs cost based on provider pricing

Original implementation code:
```python
import hashlib
import json
import zlib
from redis import Redis
from typing import Dict, Any, Optional, List, Tuple

class AIResponseCache:
    def __init__(self, redis_client: Redis, default_ttl_seconds: int = 86400):
        self.redis = redis_client
        self.default_ttl = default_ttl_seconds
        self.version = 1  # For cache versioning
        
        # Define cost tiers for different providers to influence caching strategy
        self.provider_cost_tiers = {
            'openai': {
                'gpt-3.5-turbo': 'low',
                'gpt-4': 'high',
                'gpt-4-turbo': 'high'
            },
            'anthropic': {
                'claude-instant': 'medium',
                'claude-2': 'high',
                'claude-3-opus': 'very_high',
                'claude-3-sonnet': 'high',
                'claude-3-haiku': 'medium'
            },
            'mistral': {
                'mistral-tiny': 'low',
                'mistral-small': 'medium',
                'mistral-medium': 'medium',
                'mistral-large': 'high'
            },
            'deepseek': {
                'deepseek-coder': 'medium',
                'deepseek-chat': 'medium'
            }
        }
    
    def _generate_key(self, prompt: str, provider: str, model: str, collection: str = None) -> str:
        # Create a unique hash based on prompt, provider, model and optional collection
        content_hash = hashlib.md5(prompt.encode()).hexdigest()
        base_key = f"ai_response:v{self.version}:{provider}:{model}:{content_hash}"
        if collection:
            return f"{base_key}:collection:{collection}"
        return base_key
    
    def _compress_content(self, content: str) -> bytes:
        # Compress large content blocks
        return zlib.compress(content.encode('utf-8'))
    
    def _decompress_content(self, compressed_data: bytes) -> str:
        # Decompress content
        return zlib.decompress(compressed_data).decode('utf-8')
    
    def _calculate_ttl(self, content_type: str, provider: str, model: str, confidence: float = 1.0) -> int:
        # Dynamic TTL based on content type, provider cost tier, and confidence
        base_ttl = self.default_ttl
        
        # Adjust TTL based on content type
        if content_type == 'critical':
            base_ttl = 43200  # 12 hours for critical content
        elif content_type == 'static':
            base_ttl = 604800  # 7 days for static content
        
        # Adjust TTL based on provider cost tier
        cost_tier = 'medium'  # Default
        if provider in self.provider_cost_tiers and model in self.provider_cost_tiers[provider]:
            cost_tier = self.provider_cost_tiers[provider][model]
        
        # More expensive models get longer cache times to save costs
        tier_multipliers = {
            'low': 0.8,
            'medium': 1.0,
            'high': 1.5,
            'very_high': 2.0
        }
        
        tier_factor = tier_multipliers.get(cost_tier, 1.0)
        confidence_factor = max(0.5, min(1.5, confidence))  # Between 0.5 and 1.5
        
        return int(base_ttl * tier_factor * confidence_factor)
    
    async def get_cached_response(self, prompt: str, provider: str, model: str, collection: str = None) -> Optional[str]:
        key = self._generate_key(prompt, provider, model, collection)
        cached = self.redis.get(key)
        if cached:
            # Track cache hit for this provider
            self.redis.incr(f'cache:{provider}:{model}:hits')
            # Check if content is compressed
            try:
                return self._decompress_content(cached)
            except zlib.error:
                # Not compressed
                return cached.decode('utf-8')
        # Track cache miss for this provider
        self.redis.incr(f'cache:{provider}:{model}:misses')
        return None
    
    async def cache_response(self, prompt: str, provider: str, model: str, response: str, collection: str = None, content_type: str = 'standard', confidence: float = 1.0) -> None:
        key = self._generate_key(prompt, provider, model, collection)
        ttl = self._calculate_ttl(content_type, provider, model, confidence)
        
        # Compress large content
        if len(response) > 1000:
            compressed_data = self._compress_content(response)
            self.redis.set(key, compressed_data, ex=ttl)
        else:
            self.redis.set(key, response, ex=ttl)
    
    async def cache_batch_responses(self, items: List[Dict[str, Any]]) -> None:
        pipeline = self.redis.pipeline()
        for item in items:
            key = self._generate_key(
                item['prompt'], 
                item['provider'], 
                item['model'],
                item.get('collection')
            )
            ttl = self._calculate_ttl(
                item.get('content_type', 'standard'), 
                item['provider'], 
                item['model'],
                item.get('confidence', 1.0)
            )
            response = item['response']
            
            # Compress large content
            if len(response) > 1000:
                compressed_data = self._compress_content(response)
                pipeline.set(key, compressed_data, ex=ttl)
            else:
                pipeline.set(key, response, ex=ttl)
        pipeline.execute()
    
    async def invalidate_cache(self, provider: str = None, model: str = None, collection: str = None) -> int:
        # Build pattern for keys to delete
        pattern_parts = ['ai_response:v' + str(self.version)]  
        if provider:
            pattern_parts.append(provider)
        else:
            pattern_parts.append('*')
            
        if model:
            pattern_parts.append(model)
        else:
            pattern_parts.append('*')
            
        pattern_parts.append('*')  # For content hash
        
        if collection:
            pattern_parts.append('collection:' + collection)
            
        pattern = ':'.join(pattern_parts)
        
        # Get keys matching pattern
        keys = self.redis.keys(pattern)
        if keys:
            return self.redis.delete(*keys)
        return 0
    
    async def warm_cache(self, frequent_content: List[Dict[str, Any]]) -> None:
        # Pre-populate cache with frequently accessed content
        # This would typically be called by a scheduled job
        for item in frequent_content:
            cached = await self.get_cached_response(
                item['prompt'],
                item['provider'],
                item['model'],
                item.get('collection')
            )
            
            # If not in cache and we have a response, cache it
            if not cached and 'response' in item:
                await self.cache_response(
                    item['prompt'],
                    item['provider'],
                    item['model'],
                    item['response'],
                    item.get('collection'),
                    item.get('content_type', 'standard'),
                    item.get('confidence', 1.0)
                )
    
    async def get_cache_stats(self, provider: str = None, model: str = None) -> Dict[str, Any]:
        if provider and model:
            hits = int(self.redis.get(f'cache:{provider}:{model}:hits') or 0)
            misses = int(self.redis.get(f'cache:{provider}:{model}:misses') or 0)
            return {
                'provider': provider,
                'model': model,
                'hits': hits,
                'misses': misses,
                'hit_rate': hits / (hits + misses) if (hits + misses) > 0 else 0
            }
        elif provider:
            # Get stats for all models of a provider
            stats = {'provider': provider, 'models': {}, 'total_hits': 0, 'total_misses': 0}
            model_keys = self.redis.keys(f'cache:{provider}:*:hits')
            
            for key in model_keys:
                key_parts = key.decode('utf-8').split(':')
                model = key_parts[2]
                hits = int(self.redis.get(f'cache:{provider}:{model}:hits') or 0)
                misses = int(self.redis.get(f'cache:{provider}:{model}:misses') or 0)
                
                stats['models'][model] = {
                    'hits': hits,
                    'misses': misses,
                    'hit_rate': hits / (hits + misses) if (hits + misses) > 0 else 0
                }
                
                stats['total_hits'] += hits
                stats['total_misses'] += misses
            
            total = stats['total_hits'] + stats['total_misses']
            stats['overall_hit_rate'] = stats['total_hits'] / total if total > 0 else 0
            
            return stats
        else:
            # Get stats for all providers
            stats = {'providers': {}, 'overall': {'hits': 0, 'misses': 0}}
            provider_keys = self.redis.keys('cache:*:*:hits')
            
            for key in provider_keys:
                key_parts = key.decode('utf-8').split(':')
                provider = key_parts[1]
                model = key_parts[2]
                hits = int(self.redis.get(f'cache:{provider}:{model}:hits') or 0)
                misses = int(self.redis.get(f'cache:{provider}:{model}:misses') or 0)
                
                if provider not in stats['providers']:
                    stats['providers'][provider] = {'models': {}, 'total_hits': 0, 'total_misses': 0}
                
                stats['providers'][provider]['models'][model] = {
                    'hits': hits,
                    'misses': misses,
                    'hit_rate': hits / (hits + misses) if (hits + misses) > 0 else 0
                }
                
                stats['providers'][provider]['total_hits'] += hits
                stats['providers'][provider]['total_misses'] += misses
                stats['overall']['hits'] += hits
                stats['overall']['misses'] += misses
            
            # Calculate overall hit rates for each provider
            for provider in stats['providers']:
                total = stats['providers'][provider]['total_hits'] + stats['providers'][provider]['total_misses']
                stats['providers'][provider]['hit_rate'] = stats['providers'][provider]['total_hits'] / total if total > 0 else 0
            
            # Calculate overall hit rate
            total = stats['overall']['hits'] + stats['overall']['misses']
            stats['overall']['hit_rate'] = stats['overall']['hits'] / total if total > 0 else 0
            
            return stats
```

# Test Strategy:
## Completed Testing Strategy

### Unit Tests
1. âœ… Cache key generation with provider and model parameters
2. âœ… TTL calculation based on content type, provider cost tier, and confidence
3. âœ… Compression/decompression functionality for different content sizes
4. âœ… Provider cost tier validation

### Integration Tests
5. âœ… Cache hit and miss scenarios across different AI providers (OpenAI, Anthropic, Mistral, DeepSeek)
6. âœ… Cache statistics tracking per provider and model
7. âœ… Cache behavior with different content types and sizes
8. âœ… Concurrent cache access
9. âœ… Batch response caching efficiency
10. âœ… Cache invalidation for specific collections and models
11. âœ… Cache warming functionality
12. âœ… Cache versioning

### Performance Tests
13. âœ… Compression/decompression performance benchmarks
14. âœ… Redis memory usage with and without compression
15. âœ… Response time comparison: cached vs. non-cached requests
16. âœ… Pipeline operation efficiency for batch operations

### Directus Integration Tests
17. âœ… Collection-specific caching patterns
18. âœ… Collection-based cache invalidation

### Provider-Specific Tests
19. âœ… Provider-specific cache isolation
20. âœ… Cost-aware caching strategies (verify longer TTL for expensive providers)
21. âœ… Provider-specific confidence scoring impact on TTL
22. âœ… Cache behavior with different models from the same provider

All tests have been successfully implemented and passed, confirming the Redis caching layer works as expected across all required scenarios.

# Subtasks:
## 3.1. Core Cache Service Implementation [completed]
### Dependencies: None
### Description: Implement the AIResponseCache class with Redis integration, compression, and provider-specific handling
### Details:


## 3.2. Cache Integration Service [completed]
### Dependencies: None
### Description: Create cached_translation_service.py to wrap existing provider router with caching middleware
### Details:


## 3.3. API Endpoints for Cache Management [completed]
### Dependencies: None
### Description: Implement API endpoints for cache statistics, info, invalidation, and clearing
### Details:


## 3.4. Application Integration [completed]
### Dependencies: None
### Description: Set up Redis connection handling in FastAPI, Docker integration, and configuration
### Details:


## 3.5. Testing Infrastructure [completed]
### Dependencies: None
### Description: Create comprehensive test suite for cache functionality, performance, and integration
### Details:


