{
  "tasks": [
    {
      "id": 1,
      "title": "Setup FastAPI Project with Docker",
      "description": "Initialize the FastAPI project with PostgreSQL and Redis integration, and create Docker configuration for easy deployment.",
      "details": "1. Create a new FastAPI project structure\n2. Set up PostgreSQL connection using SQLAlchemy ORM\n3. Configure Redis client for caching\n4. Create Docker and docker-compose files for local development\n5. Implement health check endpoint (GET /health)\n\nProject structure:\n```\nlocplat/\n├── app/\n│   ├── __init__.py\n│   ├── main.py\n│   ├── config.py\n│   ├── models/\n│   ├── api/\n│   ├── services/\n│   └── utils/\n├── tests/\n├── Dockerfile\n├── docker-compose.yml\n├── requirements.txt\n└── README.md\n```\n\nMain dependencies:\n- fastapi\n- uvicorn\n- sqlalchemy\n- psycopg2-binary\n- redis\n- pydantic\n\nImplement basic health check endpoint in main.py:\n```python\n@app.get('/health')\ndef health_check():\n    return {'status': 'ok'}\n```",
      "testStrategy": "1. Verify FastAPI server starts correctly\n2. Confirm PostgreSQL connection is established\n3. Validate Redis connection is working\n4. Test health check endpoint returns 200 OK\n5. Ensure Docker containers build and run properly\n6. Validate environment variables are properly loaded",
      "priority": "high",
      "dependencies": [],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 2,
      "title": "Implement Translation Provider Integration",
      "description": "Create services to integrate with multiple AI providers (OpenAI as primary, Anthropic Claude as secondary, Mistral AI as tertiary, and DeepSeek as fallback) using client-provided API keys, supporting the Directus translation interface.",
      "status": "done",
      "dependencies": [
        1
      ],
      "priority": "high",
      "details": "1. Create abstract translation provider interface\n2. Implement OpenAI provider as primary using their API\n3. Implement Anthropic Claude provider as secondary\n4. Implement Mistral AI provider as tertiary\n5. Implement DeepSeek provider as final fallback\n6. Create provider router to handle cascading fallback logic\n7. Add language pair support with proper handling for Arabic (RTL) and Bosnian (Latin/Cyrillic)\n8. Support structured responses compatible with Directus translation interface\n9. Implement collection-specific translations following Directus patterns\n10. Support batch translation for multiple fields\n11. Add language direction support (LTR/RTL)\n12. Implement translation quality assessment and validation\n13. Support nested JSON structures and field mapping\n14. Ensure translation context preservation and formatting maintenance\n15. Implement provider-specific optimizations and prompt engineering\n\nProvider interface:\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any, List\nfrom enum import Enum\n\nclass LanguageDirection(Enum):\n    LTR = \"ltr\"\n    RTL = \"rtl\"\n\nclass TranslationProvider(ABC):\n    @abstractmethod\n    async def translate(self, text: str, source_lang: str, target_lang: str, api_key: str) -> str:\n        pass\n        \n    @abstractmethod\n    async def batch_translate(self, texts: List[str], source_lang: str, target_lang: str, api_key: str) -> List[str]:\n        pass\n\nclass OpenAIProvider(TranslationProvider):\n    async def translate(self, text: str, source_lang: str, target_lang: str, api_key: str) -> str:\n        # OpenAI implementation using their API\n        # Use the provided API key for authentication\n        pass\n        \n    async def batch_translate(self, texts: List[str], source_lang: str, target_lang: str, api_key: str) -> List[str]:\n        # Batch translation implementation\n        pass\n\nclass AnthropicProvider(TranslationProvider):\n    async def translate(self, text: str, source_lang: str, target_lang: str, api_key: str) -> str:\n        # Anthropic Claude implementation\n        pass\n        \n    async def batch_translate(self, texts: List[str], source_lang: str, target_lang: str, api_key: str) -> List[str]:\n        # Batch translation implementation\n        pass\n\nclass MistralProvider(TranslationProvider):\n    async def translate(self, text: str, source_lang: str, target_lang: str, api_key: str) -> str:\n        # Mistral AI implementation\n        pass\n        \n    async def batch_translate(self, texts: List[str], source_lang: str, target_lang: str, api_key: str) -> List[str]:\n        # Batch translation implementation\n        pass\n\nclass DeepSeekProvider(TranslationProvider):\n    async def translate(self, text: str, source_lang: str, target_lang: str, api_key: str) -> str:\n        # DeepSeek implementation\n        pass\n        \n    async def batch_translate(self, texts: List[str], source_lang: str, target_lang: str, api_key: str) -> List[str]:\n        # Batch translation implementation\n        pass\n\nclass ProviderRouter:\n    def __init__(self):\n        self.providers = [\n            OpenAIProvider(),      # Primary\n            AnthropicProvider(),   # Secondary\n            MistralProvider(),     # Tertiary\n            DeepSeekProvider()     # Final fallback\n        ]\n    \n    async def translate(self, text: str, source_lang: str, target_lang: str, \n                        api_keys: Dict[str, str]) -> str:\n        last_error = None\n        for i, provider in enumerate(self.providers):\n            try:\n                provider_name = provider.__class__.__name__.replace('Provider', '').lower()\n                if provider_name in api_keys and api_keys[provider_name]:\n                    return await provider.translate(text, source_lang, target_lang, api_keys[provider_name])\n            except Exception as e:\n                # Log the error\n                last_error = e\n                continue\n        raise Exception(f\"All translation providers failed. Last error: {last_error}\")\n        \n    async def batch_translate(self, texts: List[str], source_lang: str, target_lang: str,\n                             api_keys: Dict[str, str]) -> List[str]:\n        # Similar cascading fallback logic for batch translation\n        pass\n        \n    async def translate_collection(self, collection_data: Dict[str, Any], fields: List[str],\n                                  source_lang: str, target_lang: str, api_keys: Dict[str, str]) -> Dict[str, Any]:\n        # Implement collection-specific translation following Directus patterns\n        pass\n        \n    def get_language_direction(self, lang_code: str) -> LanguageDirection:\n        # Return language direction (RTL for Arabic, LTR for others)\n        rtl_languages = ['ar', 'he', 'fa', 'ur']\n        return LanguageDirection.RTL if lang_code in rtl_languages else LanguageDirection.LTR\n        \n    async def assess_quality(self, original: str, translation: str, source_lang: str, target_lang: str) -> float:\n        # Implement translation quality assessment\n        pass\n        \n    async def translate_nested_json(self, json_data: Dict[str, Any], field_mapping: Dict[str, str],\n                                  source_lang: str, target_lang: str, api_keys: Dict[str, str]) -> Dict[str, Any]:\n        # Handle nested JSON structures with field mapping\n        pass\n        \n    async def optimize_prompt(self, text: str, provider_name: str, source_lang: str, target_lang: str) -> str:\n        # Provider-specific prompt engineering optimizations\n        # Enhance context preservation and cultural sensitivity\n        pass\n```\n\nImplement GET /languages endpoint to return supported language pairs with direction information.\n\nNever store client API keys - they should be provided with each request and used only for that specific translation operation.",
      "testStrategy": "1. Unit test each provider with mock API responses\n2. Test cascading fallback mechanism when providers fail\n3. Verify correct handling of API keys for each provider\n4. Test with actual API keys in development environment\n5. Validate supported language pairs including RTL support for Arabic\n6. Test both Latin and Cyrillic script support for Bosnian\n7. Test error handling for invalid API keys\n8. Measure response times for performance benchmarking\n9. Test batch translation functionality\n10. Verify collection-specific translations follow Directus patterns\n11. Test language direction detection\n12. Validate translation quality assessment\n13. Test structured response compatibility with Directus translation interface\n14. Verify nested JSON structure handling and field mapping\n15. Test translation context preservation across different providers\n16. Validate formatting maintenance in translated content\n17. Test cultural sensitivity handling, especially for Arabic content\n18. Verify API keys are never stored and only used for the current request\n19. Test provider-specific optimizations and prompt engineering effectiveness",
      "subtasks": []
    },
    {
      "id": 3,
      "title": "Implement Redis Caching Layer",
      "description": "Create a caching system using Redis to store AI responses from multiple providers (OpenAI, Anthropic, Mistral, and DeepSeek) to avoid duplicate API calls, reducing costs and improving performance. Optimize for Directus integration patterns with provider-specific caching strategies.",
      "status": "pending",
      "dependencies": [
        1
      ],
      "priority": "medium",
      "details": "1. Design cache key generation based on content hash, prompt, model, and provider\n2. Implement Redis cache operations (get, set, delete) with provider-specific handling\n3. Add dynamic TTL (time-to-live) based on content type, model, and provider cost tier\n4. Create cache hit/miss tracking for metrics per provider\n5. Support cache invalidation for specific Directus collections and AI models\n6. Implement efficient batch response caching\n7. Add collection-specific cache patterns for Directus\n8. Implement cache warming for frequently requested content\n9. Support cache versioning for model updates\n10. Add compression for large response blocks\n11. Implement cost-aware caching strategies (longer TTL for expensive providers like Anthropic Claude)\n12. Add provider-specific confidence scoring to adjust TTL\n\nCache implementation:\n```python\nimport hashlib\nimport json\nimport zlib\nfrom redis import Redis\nfrom typing import Dict, Any, Optional, List, Tuple\n\nclass AIResponseCache:\n    def __init__(self, redis_client: Redis, default_ttl_seconds: int = 86400):\n        self.redis = redis_client\n        self.default_ttl = default_ttl_seconds\n        self.version = 1  # For cache versioning\n        \n        # Define cost tiers for different providers to influence caching strategy\n        self.provider_cost_tiers = {\n            'openai': {\n                'gpt-3.5-turbo': 'low',\n                'gpt-4': 'high',\n                'gpt-4-turbo': 'high'\n            },\n            'anthropic': {\n                'claude-instant': 'medium',\n                'claude-2': 'high',\n                'claude-3-opus': 'very_high',\n                'claude-3-sonnet': 'high',\n                'claude-3-haiku': 'medium'\n            },\n            'mistral': {\n                'mistral-tiny': 'low',\n                'mistral-small': 'medium',\n                'mistral-medium': 'medium',\n                'mistral-large': 'high'\n            },\n            'deepseek': {\n                'deepseek-coder': 'medium',\n                'deepseek-chat': 'medium'\n            }\n        }\n    \n    def _generate_key(self, prompt: str, provider: str, model: str, collection: str = None) -> str:\n        # Create a unique hash based on prompt, provider, model and optional collection\n        content_hash = hashlib.md5(prompt.encode()).hexdigest()\n        base_key = f\"ai_response:v{self.version}:{provider}:{model}:{content_hash}\"\n        if collection:\n            return f\"{base_key}:collection:{collection}\"\n        return base_key\n    \n    def _compress_content(self, content: str) -> bytes:\n        # Compress large content blocks\n        return zlib.compress(content.encode('utf-8'))\n    \n    def _decompress_content(self, compressed_data: bytes) -> str:\n        # Decompress content\n        return zlib.decompress(compressed_data).decode('utf-8')\n    \n    def _calculate_ttl(self, content_type: str, provider: str, model: str, confidence: float = 1.0) -> int:\n        # Dynamic TTL based on content type, provider cost tier, and confidence\n        base_ttl = self.default_ttl\n        \n        # Adjust TTL based on content type\n        if content_type == 'critical':\n            base_ttl = 43200  # 12 hours for critical content\n        elif content_type == 'static':\n            base_ttl = 604800  # 7 days for static content\n        \n        # Adjust TTL based on provider cost tier\n        cost_tier = 'medium'  # Default\n        if provider in self.provider_cost_tiers and model in self.provider_cost_tiers[provider]:\n            cost_tier = self.provider_cost_tiers[provider][model]\n        \n        # More expensive models get longer cache times to save costs\n        tier_multipliers = {\n            'low': 0.8,\n            'medium': 1.0,\n            'high': 1.5,\n            'very_high': 2.0\n        }\n        \n        tier_factor = tier_multipliers.get(cost_tier, 1.0)\n        confidence_factor = max(0.5, min(1.5, confidence))  # Between 0.5 and 1.5\n        \n        return int(base_ttl * tier_factor * confidence_factor)\n    \n    async def get_cached_response(self, prompt: str, provider: str, model: str, collection: str = None) -> Optional[str]:\n        key = self._generate_key(prompt, provider, model, collection)\n        cached = self.redis.get(key)\n        if cached:\n            # Track cache hit for this provider\n            self.redis.incr(f'cache:{provider}:{model}:hits')\n            # Check if content is compressed\n            try:\n                return self._decompress_content(cached)\n            except zlib.error:\n                # Not compressed\n                return cached.decode('utf-8')\n        # Track cache miss for this provider\n        self.redis.incr(f'cache:{provider}:{model}:misses')\n        return None\n    \n    async def cache_response(self, prompt: str, provider: str, model: str, response: str, collection: str = None, content_type: str = 'standard', confidence: float = 1.0) -> None:\n        key = self._generate_key(prompt, provider, model, collection)\n        ttl = self._calculate_ttl(content_type, provider, model, confidence)\n        \n        # Compress large content\n        if len(response) > 1000:\n            compressed_data = self._compress_content(response)\n            self.redis.set(key, compressed_data, ex=ttl)\n        else:\n            self.redis.set(key, response, ex=ttl)\n    \n    async def cache_batch_responses(self, items: List[Dict[str, Any]]) -> None:\n        pipeline = self.redis.pipeline()\n        for item in items:\n            key = self._generate_key(\n                item['prompt'], \n                item['provider'], \n                item['model'],\n                item.get('collection')\n            )\n            ttl = self._calculate_ttl(\n                item.get('content_type', 'standard'), \n                item['provider'], \n                item['model'],\n                item.get('confidence', 1.0)\n            )\n            response = item['response']\n            \n            # Compress large content\n            if len(response) > 1000:\n                compressed_data = self._compress_content(response)\n                pipeline.set(key, compressed_data, ex=ttl)\n            else:\n                pipeline.set(key, response, ex=ttl)\n        pipeline.execute()\n    \n    async def invalidate_cache(self, provider: str = None, model: str = None, collection: str = None) -> int:\n        # Build pattern for keys to delete\n        pattern_parts = ['ai_response:v' + str(self.version)]  \n        if provider:\n            pattern_parts.append(provider)\n        else:\n            pattern_parts.append('*')\n            \n        if model:\n            pattern_parts.append(model)\n        else:\n            pattern_parts.append('*')\n            \n        pattern_parts.append('*')  # For content hash\n        \n        if collection:\n            pattern_parts.append('collection:' + collection)\n            \n        pattern = ':'.join(pattern_parts)\n        \n        # Get keys matching pattern\n        keys = self.redis.keys(pattern)\n        if keys:\n            return self.redis.delete(*keys)\n        return 0\n    \n    async def warm_cache(self, frequent_content: List[Dict[str, Any]]) -> None:\n        # Pre-populate cache with frequently accessed content\n        # This would typically be called by a scheduled job\n        for item in frequent_content:\n            cached = await self.get_cached_response(\n                item['prompt'],\n                item['provider'],\n                item['model'],\n                item.get('collection')\n            )\n            \n            # If not in cache and we have a response, cache it\n            if not cached and 'response' in item:\n                await self.cache_response(\n                    item['prompt'],\n                    item['provider'],\n                    item['model'],\n                    item['response'],\n                    item.get('collection'),\n                    item.get('content_type', 'standard'),\n                    item.get('confidence', 1.0)\n                )\n    \n    async def get_cache_stats(self, provider: str = None, model: str = None) -> Dict[str, Any]:\n        if provider and model:\n            hits = int(self.redis.get(f'cache:{provider}:{model}:hits') or 0)\n            misses = int(self.redis.get(f'cache:{provider}:{model}:misses') or 0)\n            return {\n                'provider': provider,\n                'model': model,\n                'hits': hits,\n                'misses': misses,\n                'hit_rate': hits / (hits + misses) if (hits + misses) > 0 else 0\n            }\n        elif provider:\n            # Get stats for all models of a provider\n            stats = {'provider': provider, 'models': {}, 'total_hits': 0, 'total_misses': 0}\n            model_keys = self.redis.keys(f'cache:{provider}:*:hits')\n            \n            for key in model_keys:\n                key_parts = key.decode('utf-8').split(':')\n                model = key_parts[2]\n                hits = int(self.redis.get(f'cache:{provider}:{model}:hits') or 0)\n                misses = int(self.redis.get(f'cache:{provider}:{model}:misses') or 0)\n                \n                stats['models'][model] = {\n                    'hits': hits,\n                    'misses': misses,\n                    'hit_rate': hits / (hits + misses) if (hits + misses) > 0 else 0\n                }\n                \n                stats['total_hits'] += hits\n                stats['total_misses'] += misses\n            \n            total = stats['total_hits'] + stats['total_misses']\n            stats['overall_hit_rate'] = stats['total_hits'] / total if total > 0 else 0\n            \n            return stats\n        else:\n            # Get stats for all providers\n            stats = {'providers': {}, 'overall': {'hits': 0, 'misses': 0}}\n            provider_keys = self.redis.keys('cache:*:*:hits')\n            \n            for key in provider_keys:\n                key_parts = key.decode('utf-8').split(':')\n                provider = key_parts[1]\n                model = key_parts[2]\n                hits = int(self.redis.get(f'cache:{provider}:{model}:hits') or 0)\n                misses = int(self.redis.get(f'cache:{provider}:{model}:misses') or 0)\n                \n                if provider not in stats['providers']:\n                    stats['providers'][provider] = {'models': {}, 'total_hits': 0, 'total_misses': 0}\n                \n                stats['providers'][provider]['models'][model] = {\n                    'hits': hits,\n                    'misses': misses,\n                    'hit_rate': hits / (hits + misses) if (hits + misses) > 0 else 0\n                }\n                \n                stats['providers'][provider]['total_hits'] += hits\n                stats['providers'][provider]['total_misses'] += misses\n                stats['overall']['hits'] += hits\n                stats['overall']['misses'] += misses\n            \n            # Calculate overall hit rates for each provider\n            for provider in stats['providers']:\n                total = stats['providers'][provider]['total_hits'] + stats['providers'][provider]['total_misses']\n                stats['providers'][provider]['hit_rate'] = stats['providers'][provider]['total_hits'] / total if total > 0 else 0\n            \n            # Calculate overall hit rate\n            total = stats['overall']['hits'] + stats['overall']['misses']\n            stats['overall']['hit_rate'] = stats['overall']['hits'] / total if total > 0 else 0\n            \n            return stats\n```",
      "testStrategy": "1. Unit test cache key generation with provider and model parameters\n2. Test cache hit and miss scenarios across different AI providers (OpenAI, Anthropic, Mistral, DeepSeek)\n3. Verify dynamic TTL functionality based on content type, provider cost tier, and confidence\n4. Benchmark cache performance with compression enabled\n5. Test cache statistics tracking per provider and model\n6. Validate cache behavior with different content types and sizes\n7. Test concurrent cache access\n8. Test batch response caching efficiency\n9. Verify cache invalidation for specific collections and models\n10. Test cache warming functionality\n11. Validate cache versioning\n12. Benchmark compression/decompression performance\n13. Test Directus collection-specific caching patterns\n14. Verify provider-specific cache isolation\n15. Test cost-aware caching strategies (verify longer TTL for expensive providers)\n16. Validate provider-specific confidence scoring impact on TTL\n17. Test cache behavior with different models from the same provider",
      "subtasks": []
    },
    {
      "id": 4,
      "title": "Implement Field Mapping and Content Processing",
      "description": "Create a system to map and process fields for translation, supporting Directus collection structures with primary fields and translation collections, handling plain text, HTML, and other content types with JSON path support. Focus on Directus-specific patterns and support for structured data from AI providers with batch operations.",
      "status": "pending",
      "dependencies": [
        2,
        3
      ],
      "priority": "medium",
      "details": "1. Implement field mapping configuration storage in PostgreSQL\n2. Create JSON path parser for field selection\n3. Add content type detection and processing (text vs HTML)\n4. Implement field extraction and reassembly logic\n5. Support Directus collection structures with primary fields and translation collections\n6. Handle nested field mapping for complex content structures\n7. Implement field type detection (text, wysiwyg, textarea, etc.)\n8. Support Directus translation interface patterns\n9. Implement language-specific field mapping for RTL languages\n10. Add content sanitization and validation\n11. Handle Directus relation fields and junction tables\n12. Support custom field transformations for different content types\n13. Implement metadata preservation during translation\n14. Support standard Directus translation structure (collection_translations)\n15. Implement handling for language collections in Directus\n16. Add support for structured data from AI providers\n17. Implement batch operations for multiple collection fields\n18. Create optimized processing for Directus-specific field patterns\n\nDatabase model:\n```python\nfrom sqlalchemy import Column, Integer, String, JSON, DateTime, Boolean, ForeignKey\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom datetime import datetime\n\nBase = declarative_base()\n\nclass FieldConfig(Base):\n    __tablename__ = 'field_configs'\n    \n    id = Column(Integer, primary_key=True)\n    client_id = Column(String, nullable=False)\n    collection_name = Column(String, nullable=False)\n    field_paths = Column(JSON, nullable=False)  # JSON array of paths\n    created_at = Column(DateTime, default=datetime.utcnow)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n    is_translation_collection = Column(Boolean, default=False)\n    primary_collection = Column(String, nullable=True)\n    field_types = Column(JSON, nullable=True)  # Maps field paths to their types\n    rtl_field_mapping = Column(JSON, nullable=True)  # Special handling for RTL languages\n    directus_translation_pattern = Column(String, nullable=True)  # 'collection_translations' or 'language_collections'\n    batch_processing = Column(Boolean, default=False)  # Whether to process fields in batch\n```\n\nField mapper implementation:\n```python\nfrom typing import Dict, Any, List, Optional, Tuple\nimport re\nfrom bs4 import BeautifulSoup\nfrom enum import Enum\n\nclass FieldType(Enum):\n    TEXT = \"text\"\n    WYSIWYG = \"wysiwyg\"\n    TEXTAREA = \"textarea\"\n    STRING = \"string\"\n    RELATION = \"relation\"\n    JSON = \"json\"\n\nclass DirectusTranslationPattern(Enum):\n    COLLECTION_TRANSLATIONS = \"collection_translations\"\n    LANGUAGE_COLLECTIONS = \"language_collections\"\n    CUSTOM = \"custom\"\n\nclass FieldMapper:\n    def __init__(self, db_session):\n        self.db_session = db_session\n    \n    async def get_field_config(self, client_id: str, collection_name: str) -> Dict[str, Any]:\n        # Get field paths from database\n        config = self.db_session.query(FieldConfig).filter_by(\n            client_id=client_id,\n            collection_name=collection_name\n        ).first()\n        \n        if not config:\n            return {\n                \"field_paths\": [],\n                \"is_translation_collection\": False,\n                \"field_types\": {},\n                \"rtl_field_mapping\": {},\n                \"directus_translation_pattern\": DirectusTranslationPattern.COLLECTION_TRANSLATIONS.value,\n                \"batch_processing\": False\n            }\n            \n        return {\n            \"field_paths\": config.field_paths,\n            \"is_translation_collection\": config.is_translation_collection,\n            \"primary_collection\": config.primary_collection,\n            \"field_types\": config.field_types or {},\n            \"rtl_field_mapping\": config.rtl_field_mapping or {},\n            \"directus_translation_pattern\": config.directus_translation_pattern or DirectusTranslationPattern.COLLECTION_TRANSLATIONS.value,\n            \"batch_processing\": config.batch_processing or False\n        }\n    \n    async def save_field_config(self, client_id: str, collection_name: str, \n                              field_config: Dict[str, Any]) -> None:\n        # Save field configuration to database\n        config = self.db_session.query(FieldConfig).filter_by(\n            client_id=client_id,\n            collection_name=collection_name\n        ).first()\n        \n        if config:\n            config.field_paths = field_config.get(\"field_paths\", [])\n            config.is_translation_collection = field_config.get(\"is_translation_collection\", False)\n            config.primary_collection = field_config.get(\"primary_collection\")\n            config.field_types = field_config.get(\"field_types\", {})\n            config.rtl_field_mapping = field_config.get(\"rtl_field_mapping\", {})\n            config.directus_translation_pattern = field_config.get(\"directus_translation_pattern\", DirectusTranslationPattern.COLLECTION_TRANSLATIONS.value)\n            config.batch_processing = field_config.get(\"batch_processing\", False)\n            config.updated_at = datetime.utcnow()\n        else:\n            config = FieldConfig(\n                client_id=client_id,\n                collection_name=collection_name,\n                field_paths=field_config.get(\"field_paths\", []),\n                is_translation_collection=field_config.get(\"is_translation_collection\", False),\n                primary_collection=field_config.get(\"primary_collection\"),\n                field_types=field_config.get(\"field_types\", {}),\n                rtl_field_mapping=field_config.get(\"rtl_field_mapping\", {}),\n                directus_translation_pattern=field_config.get(\"directus_translation_pattern\", DirectusTranslationPattern.COLLECTION_TRANSLATIONS.value),\n                batch_processing=field_config.get(\"batch_processing\", False)\n            )\n            self.db_session.add(config)\n        \n        self.db_session.commit()\n    \n    def extract_fields(self, content: Dict[str, Any], field_config: Dict[str, Any], language: str = None) -> Dict[str, Any]:\n        # Extract fields to translate based on paths\n        result = {}\n        field_paths = field_config[\"field_paths\"]\n        field_types = field_config.get(\"field_types\", {})\n        rtl_mapping = field_config.get(\"rtl_field_mapping\", {})\n        batch_processing = field_config.get(\"batch_processing\", False)\n        \n        # Check if we should use RTL-specific mapping\n        if language and language in rtl_mapping:\n            field_paths = rtl_mapping[language].get(\"field_paths\", field_paths)\n        \n        # Handle batch processing if enabled\n        if batch_processing:\n            return self._extract_fields_batch(content, field_paths, field_types, language)\n        \n        # Standard field extraction\n        for path in field_paths:\n            value = self._get_nested_value(content, path)\n            if value is not None:\n                field_type = field_types.get(path, self._detect_field_type(value))\n                result[path] = {\n                    \"value\": value,\n                    \"type\": field_type,\n                    \"metadata\": self._extract_metadata(value, field_type)\n                }\n        return result\n    \n    def _extract_fields_batch(self, content: Dict[str, Any], field_paths: List[str], \n                             field_types: Dict[str, str], language: str = None) -> Dict[str, Any]:\n        # Extract fields in batch for more efficient processing\n        result = {}\n        batch_text = []\n        batch_mapping = {}\n        \n        # First pass: collect all text for batch processing\n        for path in field_paths:\n            value = self._get_nested_value(content, path)\n            if value is not None:\n                field_type = field_types.get(path, self._detect_field_type(value))\n                \n                if field_type in [FieldType.TEXT.value, FieldType.STRING.value, FieldType.TEXTAREA.value]:\n                    # Add to batch for text fields\n                    batch_index = len(batch_text)\n                    batch_text.append(value)\n                    batch_mapping[path] = {\n                        \"index\": batch_index,\n                        \"type\": field_type\n                    }\n                else:\n                    # Process non-text fields individually\n                    result[path] = {\n                        \"value\": value,\n                        \"type\": field_type,\n                        \"metadata\": self._extract_metadata(value, field_type)\n                    }\n        \n        # Add batch text collection to result\n        if batch_text:\n            result[\"__batch__\"] = {\n                \"text\": batch_text,\n                \"mapping\": batch_mapping\n            }\n            \n        return result\n    \n    def _detect_field_type(self, value: Any) -> str:\n        # Auto-detect field type based on content\n        if isinstance(value, str):\n            if self.is_html(value):\n                return FieldType.WYSIWYG.value\n            elif \"\\n\" in value:\n                return FieldType.TEXTAREA.value\n            else:\n                return FieldType.TEXT.value\n        elif isinstance(value, dict):\n            return FieldType.JSON.value\n        else:\n            return FieldType.STRING.value\n    \n    def _extract_metadata(self, value: Any, field_type: str) -> Dict[str, Any]:\n        # Extract metadata to preserve during translation\n        metadata = {}\n        \n        if field_type == FieldType.WYSIWYG.value and isinstance(value, str):\n            metadata[\"html_structure\"] = self._extract_html_structure(value)\n        \n        return metadata\n    \n    def _extract_html_structure(self, html: str) -> Dict[str, Any]:\n        # Extract HTML structure for preservation\n        soup = BeautifulSoup(html, 'html.parser')\n        return {\n            \"tags\": [tag.name for tag in soup.find_all()],\n            \"classes\": [cls for tag in soup.find_all() for cls in tag.get(\"class\", [])],\n            \"attributes\": {tag.name: [attr for attr in tag.attrs if attr != \"class\"] \n                          for tag in soup.find_all() if tag.attrs}\n        }\n    \n    def _get_nested_value(self, data: Dict[str, Any], path: str) -> Any:\n        # Get value from nested dictionary using dot notation and array indices\n        if not data or not path:\n            return None\n            \n        parts = re.findall(r'([^\\[\\]\\.]+)|\\[(\\d+)\\]', path)\n        current = data\n        \n        for part in parts:\n            key, index = part\n            \n            if key and isinstance(current, dict):\n                if key not in current:\n                    return None\n                current = current[key]\n            elif index and isinstance(current, list):\n                idx = int(index)\n                if idx >= len(current):\n                    return None\n                current = current[idx]\n            else:\n                return None\n                \n        return current\n    \n    def is_html(self, text: str) -> bool:\n        # Check if content is HTML\n        return bool(re.search(r'<[^>]+>', text))\n    \n    def extract_text_from_html(self, html: str) -> List[Dict[str, Any]]:\n        # Extract text nodes from HTML for translation\n        soup = BeautifulSoup(html, 'html.parser')\n        text_nodes = []\n        \n        for element in soup.find_all(text=True):\n            if element.strip():\n                text_nodes.append({\n                    'text': element.strip(),\n                    'path': self._get_element_path(element),\n                    'parent_tag': element.parent.name if element.parent else None,\n                    'parent_attrs': element.parent.attrs if element.parent else {}\n                })\n        \n        return text_nodes\n    \n    def _get_element_path(self, element) -> str:\n        # Generate a path to the element for reassembly\n        path = []\n        parent = element.parent\n        while parent:\n            siblings = parent.find_all(parent.name, recursive=False)\n            if len(siblings) > 1:\n                index = siblings.index(parent)\n                path.append(f\"{parent.name}[{index}]\")\n            else:\n                path.append(parent.name)\n            parent = parent.parent\n        return \".\" + \".\".join(reversed(path))\n        \n    def reassemble_html(self, original_html: str, translated_nodes: List[Dict[str, Any]]) -> str:\n        # Reassemble HTML with translated text nodes\n        soup = BeautifulSoup(original_html, 'html.parser')\n        \n        for node in translated_nodes:\n            path = node['path']\n            translated_text = node['translated_text']\n            \n            # Find the element using the path and update it\n            # Implementation depends on how paths are structured\n            # This is a simplified version\n            elements = soup.select(path)\n            if elements:\n                elements[0].string = translated_text\n                \n        return str(soup)\n    \n    def process_ai_structured_data(self, structured_data: Dict[str, Any], field_config: Dict[str, Any]) -> Dict[str, Any]:\n        # Process structured data from AI providers\n        result = {}\n        \n        # Handle different AI provider formats\n        if \"translations\" in structured_data:\n            # Format: {\"translations\": [{\"text\": \"...\", \"detected_language\": \"...\", \"to\": \"...\"}]}\n            translations = structured_data.get(\"translations\", [])\n            for i, translation in enumerate(translations):\n                if i < len(field_config.get(\"field_paths\", [])):\n                    path = field_config[\"field_paths\"][i]\n                    result[path] = {\n                        \"value\": translation.get(\"text\", \"\"),\n                        \"type\": field_config.get(\"field_types\", {}).get(path, FieldType.TEXT.value),\n                        \"metadata\": {\n                            \"detected_language\": translation.get(\"detected_language\"),\n                            \"target_language\": translation.get(\"to\")\n                        }\n                    }\n        elif \"choices\" in structured_data:\n            # Format used by some AI providers with choices array\n            choices = structured_data.get(\"choices\", [])\n            if choices and \"message\" in choices[0]:\n                content = choices[0].get(\"message\", {}).get(\"content\", \"\")\n                # Try to parse as JSON if it looks like JSON\n                if content.strip().startswith(\"{\") and content.strip().endswith(\"}\"):\n                    try:\n                        parsed = json.loads(content)\n                        for path in field_config.get(\"field_paths\", []):\n                            if path in parsed:\n                                result[path] = {\n                                    \"value\": parsed[path],\n                                    \"type\": field_config.get(\"field_types\", {}).get(path, FieldType.TEXT.value),\n                                    \"metadata\": {}\n                                }\n                    except json.JSONDecodeError:\n                        # Not valid JSON, treat as single text\n                        if field_config.get(\"field_paths\"):\n                            result[field_config[\"field_paths\"][0]] = {\n                                \"value\": content,\n                                \"type\": FieldType.TEXT.value,\n                                \"metadata\": {}\n                            }\n        \n        return result\n        \n    def handle_directus_relations(self, content: Dict[str, Any], field_config: Dict[str, Any]) -> Dict[str, Any]:\n        # Process Directus relation fields\n        result = {}\n        relation_fields = [path for path, type_info in field_config.get(\"field_types\", {}).items() \n                          if type_info == FieldType.RELATION.value]\n        \n        for path in relation_fields:\n            relation_data = self._get_nested_value(content, path)\n            if relation_data:\n                # Handle different relation types (o2m, m2o, m2m)\n                if isinstance(relation_data, list):\n                    # o2m or m2m relation\n                    result[path] = [item[\"id\"] for item in relation_data if \"id\" in item]\n                elif isinstance(relation_data, dict) and \"id\" in relation_data:\n                    # m2o relation\n                    result[path] = relation_data[\"id\"]\n                    \n        return result\n    \n    def handle_directus_translations(self, content: Dict[str, Any], field_config: Dict[str, Any], \n                                    language: str) -> Dict[str, Any]:\n        # Handle Directus translation patterns\n        translation_pattern = field_config.get(\"directus_translation_pattern\", \n                                             DirectusTranslationPattern.COLLECTION_TRANSLATIONS.value)\n        \n        if translation_pattern == DirectusTranslationPattern.COLLECTION_TRANSLATIONS.value:\n            # Standard Directus pattern: collection_translations table with languages\n            return self._handle_collection_translations(content, field_config, language)\n        elif translation_pattern == DirectusTranslationPattern.LANGUAGE_COLLECTIONS.value:\n            # Language-specific collections pattern\n            return self._handle_language_collections(content, field_config, language)\n        else:\n            # Custom pattern, use regular field extraction\n            return self.extract_fields(content, field_config, language)\n    \n    def _handle_collection_translations(self, content: Dict[str, Any], field_config: Dict[str, Any], \n                                      language: str) -> Dict[str, Any]:\n        # Handle standard Directus translation pattern with collection_translations\n        result = {}\n        primary_collection = field_config.get(\"primary_collection\")\n        \n        if not primary_collection or not content.get(\"id\"):\n            return result\n            \n        # Structure for collection_translations\n        result = {\n            \"id\": None,  # Will be auto-generated or updated if exists\n            primary_collection + \"_id\": content.get(\"id\"),\n            \"languages_code\": language,\n        }\n        \n        # Add translatable fields\n        extracted = self.extract_fields(content, field_config, language)\n        for path, field_data in extracted.items():\n            if \"__batch__\" not in path:  # Skip batch metadata\n                field_name = path.split(\".\")[-1]  # Get the field name without path\n                result[field_name] = field_data.get(\"value\")\n                \n        return result\n    \n    def _handle_language_collections(self, content: Dict[str, Any], field_config: Dict[str, Any], \n                                   language: str) -> Dict[str, Any]:\n        # Handle language-specific collections pattern\n        result = {}\n        primary_collection = field_config.get(\"primary_collection\")\n        \n        if not primary_collection or not content.get(\"id\"):\n            return result\n            \n        # Structure for language collections (e.g., articles_en, articles_fr)\n        result = {\n            \"id\": content.get(\"id\"),  # Same ID as primary content\n        }\n        \n        # Add translatable fields\n        extracted = self.extract_fields(content, field_config, language)\n        for path, field_data in extracted.items():\n            if \"__batch__\" not in path:  # Skip batch metadata\n                field_name = path.split(\".\")[-1]  # Get the field name without path\n                result[field_name] = field_data.get(\"value\")\n                \n        return result\n        \n    def sanitize_content(self, content: Dict[str, Any], field_config: Dict[str, Any]) -> Dict[str, Any]:\n        # Sanitize content before processing\n        sanitized = {}\n        \n        for path, field_data in content.items():\n            if path == \"__batch__\":\n                # Handle batch data\n                batch_text = field_data.get(\"text\", [])\n                batch_mapping = field_data.get(\"mapping\", {})\n                sanitized_batch = []\n                \n                for text in batch_text:\n                    if isinstance(text, str):\n                        if self.is_html(text):\n                            # Sanitize HTML content\n                            soup = BeautifulSoup(text, 'html.parser')\n                            for script in soup([\"script\", \"style\"]):\n                                script.decompose()\n                            sanitized_batch.append(str(soup))\n                        else:\n                            sanitized_batch.append(text)\n                    else:\n                        sanitized_batch.append(text)\n                        \n                sanitized[\"__batch__\"] = {\n                    \"text\": sanitized_batch,\n                    \"mapping\": batch_mapping\n                }\n            else:\n                value = field_data.get(\"value\")\n                field_type = field_data.get(\"type\")\n                \n                if field_type == FieldType.WYSIWYG.value and isinstance(value, str):\n                    # Sanitize HTML content\n                    soup = BeautifulSoup(value, 'html.parser')\n                    # Remove potentially dangerous tags/attributes\n                    for script in soup([\"script\", \"style\"]):\n                        script.decompose()\n                    sanitized[path] = {\n                        \"value\": str(soup),\n                        \"type\": field_type,\n                        \"metadata\": field_data.get(\"metadata\", {})\n                    }\n                else:\n                    sanitized[path] = field_data\n                    \n        return sanitized\n```",
      "testStrategy": "1. Unit test field extraction from nested objects\n2. Test HTML detection and processing\n3. Verify field configuration storage and retrieval\n4. Test JSON path parsing with various path formats\n5. Validate HTML content extraction and reassembly\n6. Test with different content structures\n7. Verify handling of missing fields\n8. Test Directus collection structure support\n9. Verify translation collection handling\n10. Test field type detection for various content types\n11. Validate RTL language field mapping\n12. Test content sanitization and validation\n13. Verify relation field handling\n14. Test metadata preservation during translation process\n15. Validate custom field transformations\n16. Test with real Directus collection examples\n17. Verify HTML structure preservation during translation\n18. Test standard Directus translation structure (collection_translations)\n19. Verify language collections handling in Directus\n20. Test structured data processing from AI providers\n21. Validate batch operations for multiple collection fields\n22. Test performance of batch vs. individual field processing\n23. Verify correct handling of different Directus translation patterns\n24. Test with complex nested Directus structures",
      "subtasks": []
    },
    {
      "id": 5,
      "title": "Implement Translation API Endpoints",
      "description": "Create the main translation API endpoints including the core translation functionality, provider management, language pair listing, field configuration, and status monitoring.",
      "status": "pending",
      "dependencies": [
        2,
        3,
        4
      ],
      "priority": "high",
      "details": "1. Implement POST /translate endpoint\n2. Create GET /languages endpoint\n3. Add POST /fields/config endpoint\n4. Implement GET /providers and POST /providers endpoints for provider management\n5. Add GET /status endpoint for service monitoring\n6. Integrate all components (AI providers, cache, field mapping)\n7. Implement request/response models with structured response support\n\nAPI implementation:\n```python\nfrom fastapi import FastAPI, Depends, HTTPException, Body, Header\nfrom pydantic import BaseModel, Field\nfrom typing import Dict, List, Any, Optional, Union\n\napp = FastAPI(title=\"LocPlat Translation Service\")\n\nclass TranslationRequest(BaseModel):\n    content: Dict[str, Any]\n    target_language: str\n    source_language: str = \"en\"\n    collection_name: Optional[str] = None\n    client_id: Optional[str] = None\n    provider: Optional[str] = None  # Allow specifying preferred provider\n    structured_response: bool = False  # Support structured responses like directus-translator\n\nclass TranslationResponse(BaseModel):\n    translated_content: Dict[str, Any]\n    source_language: str\n    target_language: str\n    provider_used: str\n    cache_hit: bool\n\nclass FieldConfigRequest(BaseModel):\n    client_id: str\n    collection_name: str\n    field_paths: List[str]\n\nclass LanguagePair(BaseModel):\n    source: str\n    target: str\n    name: str\n\nclass ProviderConfig(BaseModel):\n    name: str  # e.g., \"openai\", \"anthropic\", \"mistral\", \"deepseek\"\n    api_key: str\n    priority: int = 1  # Lower number = higher priority\n    enabled: bool = True\n    model: Optional[str] = None  # Specific model to use\n\nclass ProviderInfo(BaseModel):\n    name: str\n    enabled: bool\n    priority: int\n    model: Optional[str] = None\n\nclass ServiceStatus(BaseModel):\n    status: str\n    providers: Dict[str, bool]  # Provider name -> availability status\n    cache_size: int\n    uptime: float\n\n# Middleware to handle API key authentication\n@app.middleware(\"http\")\nasync def authenticate_client(request, call_next):\n    client_api_key = request.headers.get(\"X-API-Key\")\n    if not client_api_key:\n        return JSONResponse(status_code=401, content={\"detail\": \"API key required\"})\n    \n    # Validate API key against database\n    if not await is_valid_api_key(client_api_key):\n        return JSONResponse(status_code=403, content={\"detail\": \"Invalid API key\"})\n    \n    # Add client info to request state\n    request.state.client_id = await get_client_id_from_key(client_api_key)\n    \n    response = await call_next(request)\n    return response\n\n@app.post(\"/translate\", response_model=TranslationResponse)\nasync def translate_content(\n    request: TranslationRequest,\n    translation_service = Depends(get_translation_service),\n    x_api_key: str = Header(..., alias=\"X-API-Key\")\n):\n    try:\n        result = await translation_service.translate_content(\n            content=request.content,\n            source_lang=request.source_language,\n            target_lang=request.target_language,\n            provider=request.provider,\n            structured_response=request.structured_response,\n            collection_name=request.collection_name,\n            client_id=request.client_id or request.state.client_id\n        )\n        return result\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/languages\", response_model=List[LanguagePair])\nasync def get_languages():\n    return [\n        {\"source\": \"en\", \"target\": \"ar\", \"name\": \"English to Arabic\"},\n        {\"source\": \"en\", \"target\": \"bs\", \"name\": \"English to Bosnian\"},\n        # Add more language pairs\n    ]\n\n@app.post(\"/fields/config\")\nasync def configure_fields(\n    config: FieldConfigRequest,\n    field_mapper = Depends(get_field_mapper)\n):\n    await field_mapper.save_field_config(\n        client_id=config.client_id,\n        collection_name=config.collection_name,\n        field_paths=config.field_paths\n    )\n    return {\"status\": \"success\"}\n\n@app.get(\"/providers\", response_model=List[ProviderInfo])\nasync def get_providers(\n    provider_manager = Depends(get_provider_manager)\n):\n    return await provider_manager.list_providers()\n\n@app.post(\"/providers\")\nasync def configure_provider(\n    config: ProviderConfig,\n    provider_manager = Depends(get_provider_manager)\n):\n    await provider_manager.configure_provider(\n        name=config.name,\n        api_key=config.api_key,\n        priority=config.priority,\n        enabled=config.enabled,\n        model=config.model\n    )\n    return {\"status\": \"success\"}\n\n@app.get(\"/status\", response_model=ServiceStatus)\nasync def get_service_status(\n    service_monitor = Depends(get_service_monitor)\n):\n    return await service_monitor.get_status()\n```\n\nTranslation service implementation:\n```python\nclass TranslationService:\n    def __init__(self, provider_manager, cache, field_mapper):\n        self.provider_manager = provider_manager\n        self.cache = cache\n        self.field_mapper = field_mapper\n    \n    async def translate_content(\n        self, content, source_lang, target_lang, provider=None,\n        structured_response=False, collection_name=None, client_id=None\n    ):\n        # Get field paths if collection_name and client_id provided\n        field_paths = []\n        if collection_name and client_id:\n            field_paths = await self.field_mapper.get_field_config(client_id, collection_name)\n        \n        # If no specific fields configured, translate all string fields\n        if not field_paths:\n            field_paths = self._find_string_fields(content)\n        \n        # Extract fields to translate\n        fields_to_translate = self.field_mapper.extract_fields(content, field_paths)\n        \n        # Create result copy\n        result = content.copy()\n        cache_hit = False\n        provider_used = None\n        \n        # Process each field\n        for path, value in fields_to_translate.items():\n            if not isinstance(value, str):\n                continue\n                \n            # Check cache\n            cached = await self.cache.get_cached_translation(value, source_lang, target_lang)\n            if cached:\n                translated = cached\n                cache_hit = True\n            else:\n                # Get appropriate provider based on preference or availability\n                translation_provider = await self.provider_manager.get_provider(provider)\n                \n                if structured_response:\n                    # Use structured response format (like directus-translator)\n                    translated = await translation_provider.translate_structured(\n                        value, source_lang, target_lang\n                    )\n                else:\n                    # Standard translation\n                    translated = await translation_provider.translate(\n                        value, source_lang, target_lang\n                    )\n                \n                # Cache result\n                await self.cache.cache_translation(value, source_lang, target_lang, translated)\n                \n                # Record which provider was used\n                provider_used = translation_provider.name\n            \n            # Update result\n            self._set_nested_value(result, path, translated)\n        \n        return {\n            \"translated_content\": result,\n            \"source_language\": source_lang,\n            \"target_language\": target_lang,\n            \"provider_used\": provider_used or \"cache\",\n            \"cache_hit\": cache_hit\n        }\n    \n    def _find_string_fields(self, content, prefix=\"\"):\n        # Recursively find all string fields in the content\n        fields = []\n        if isinstance(content, dict):\n            for key, value in content.items():\n                path = f\"{prefix}.{key}\" if prefix else key\n                if isinstance(value, str):\n                    fields.append(path)\n                elif isinstance(value, dict) or isinstance(value, list):\n                    fields.extend(self._find_string_fields(value, path))\n        elif isinstance(content, list):\n            for i, item in enumerate(content):\n                path = f\"{prefix}[{i}]\"\n                fields.extend(self._find_string_fields(item, path))\n        return fields\n    \n    def _set_nested_value(self, data, path, value):\n        # Set value in nested dictionary using dot notation\n        parts = path.split('.')\n        current = data\n        for i, part in enumerate(parts):\n            if i == len(parts) - 1:\n                current[part] = value\n            else:\n                if part not in current:\n                    current[part] = {}\n                current = current[part]\n\nclass ProviderManager:\n    def __init__(self, db_client):\n        self.db_client = db_client\n        self.providers = {}\n        self.provider_classes = {\n            \"openai\": OpenAIProvider,\n            \"anthropic\": AnthropicProvider,\n            \"mistral\": MistralProvider,\n            \"deepseek\": DeepSeekProvider\n        }\n    \n    async def initialize(self):\n        # Load provider configurations from database\n        configs = await self.db_client.get_provider_configs()\n        for config in configs:\n            if config[\"name\"] in self.provider_classes and config[\"enabled\"]:\n                provider_class = self.provider_classes[config[\"name\"]]\n                self.providers[config[\"name\"]] = provider_class(\n                    api_key=config[\"api_key\"],\n                    model=config.get(\"model\")\n                )\n    \n    async def get_provider(self, preferred=None):\n        # Return preferred provider if specified and available\n        if preferred and preferred in self.providers:\n            return self.providers[preferred]\n        \n        # Otherwise return highest priority available provider\n        configs = await self.db_client.get_provider_configs()\n        configs.sort(key=lambda x: x[\"priority\"])\n        \n        for config in configs:\n            if config[\"name\"] in self.providers and config[\"enabled\"]:\n                return self.providers[config[\"name\"]]\n        \n        raise Exception(\"No translation providers available\")\n    \n    async def list_providers(self):\n        configs = await self.db_client.get_provider_configs()\n        return [{\n            \"name\": config[\"name\"],\n            \"enabled\": config[\"enabled\"],\n            \"priority\": config[\"priority\"],\n            \"model\": config.get(\"model\")\n        } for config in configs]\n    \n    async def configure_provider(self, name, api_key, priority=1, enabled=True, model=None):\n        if name not in self.provider_classes:\n            raise ValueError(f\"Unknown provider: {name}\")\n        \n        # Save configuration to database\n        await self.db_client.save_provider_config({\n            \"name\": name,\n            \"api_key\": api_key,\n            \"priority\": priority,\n            \"enabled\": enabled,\n            \"model\": model\n        })\n        \n        # Update in-memory provider if enabled\n        if enabled:\n            provider_class = self.provider_classes[name]\n            self.providers[name] = provider_class(\n                api_key=api_key,\n                model=model\n            )\n        elif name in self.providers:\n            del self.providers[name]\n```",
      "testStrategy": "1. Test POST /translate with various content structures\n2. Verify correct handling of client API keys\n3. Test structured response format compatibility with directus-translator\n4. Test with all supported AI providers (OpenAI, Anthropic, Mistral, DeepSeek)\n5. Test provider fallback behavior when preferred provider is unavailable\n6. Test caching behavior and hit rate\n7. Validate field mapping functionality\n8. Verify language pair endpoint returns correct data\n9. Test field configuration endpoint\n10. Test provider management endpoints (GET/POST /providers)\n11. Verify status monitoring endpoint\n12. Perform integration tests with all components\n13. Benchmark API response times with different providers\n14. Test with actual Directus content structures\n15. Verify proper error handling when providers fail",
      "subtasks": []
    },
    {
      "id": 6,
      "title": "Implement Directus Integration",
      "description": "Create specialized endpoints and functionality for Directus CMS integration, including batch translation support, webhook-based auto-translation, SDK integration, schema introspection, and Directus-specific data formatting.",
      "status": "pending",
      "dependencies": [
        5
      ],
      "priority": "medium",
      "details": "1. Create Directus-specific data models\n2. Implement batch translation for collections\n3. Add Directus field format handling\n4. Create specialized endpoints for Directus\n5. Implement webhook support for auto-translation\n6. Integrate with Directus SDK\n7. Add collection schema introspection\n8. Mirror directus-translator functionality for seamless workflow integration\n\nDirectus models:\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import Dict, List, Any, Optional\n\nclass DirectusItem(BaseModel):\n    id: str\n    collection: str\n    item: Dict[str, Any]\n\nclass DirectusBatchRequest(BaseModel):\n    items: List[DirectusItem]\n    target_language: str\n    openai_key: str\n    google_key: str\n    source_language: str = \"en\"\n    client_id: Optional[str] = None\n\nclass DirectusBatchResponse(BaseModel):\n    items: List[Dict[str, Any]]\n    stats: Dict[str, Any]\n\nclass DirectusWebhookPayload(BaseModel):\n    event: str  # create, update, delete\n    collection: str\n    item: Dict[str, Any]\n    target_languages: List[str]\n    client_id: str\n    openai_key: Optional[str] = None\n    google_key: Optional[str] = None\n\nclass DirectusSchemaRequest(BaseModel):\n    collection: str\n    client_id: str\n```\n\nDirectus integration implementation:\n```python\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom directus_sdk import Directus\n\ndirectus_router = APIRouter(prefix=\"/directus\", tags=[\"directus\"])\n\n@directus_router.post(\"/batch\", response_model=DirectusBatchResponse)\nasync def translate_directus_batch(\n    request: DirectusBatchRequest,\n    translation_service = Depends(get_translation_service)\n):\n    results = []\n    stats = {\n        \"total\": len(request.items),\n        \"successful\": 0,\n        \"failed\": 0,\n        \"cache_hits\": 0\n    }\n    \n    for item in request.items:\n        try:\n            # Get field config for this collection\n            field_paths = []\n            if request.client_id:\n                field_paths = await translation_service.field_mapper.get_field_config(\n                    request.client_id, item.collection\n                )\n            \n            # Translate the item\n            result = await translation_service.translate_content(\n                content=item.item,\n                source_lang=request.source_language,\n                target_lang=request.target_language,\n                openai_key=request.openai_key,\n                google_key=request.google_key,\n                collection_name=item.collection,\n                client_id=request.client_id\n            )\n            \n            # Add to results\n            results.append({\n                \"id\": item.id,\n                \"collection\": item.collection,\n                \"item\": result[\"translated_content\"]\n            })\n            \n            # Update stats\n            stats[\"successful\"] += 1\n            if result[\"cache_hit\"]:\n                stats[\"cache_hits\"] += 1\n                \n        except Exception as e:\n            stats[\"failed\"] += 1\n            # Log the error\n            print(f\"Error translating item {item.id}: {str(e)}\")\n    \n    return {\n        \"items\": results,\n        \"stats\": stats\n    }\n\n@directus_router.post(\"/collection/{collection_name}\")\nasync def configure_directus_collection(\n    collection_name: str,\n    config: Dict[str, Any],\n    field_mapper = Depends(get_field_mapper)\n):\n    # Configure which fields to translate for a Directus collection\n    client_id = config.get(\"client_id\")\n    field_paths = config.get(\"field_paths\", [])\n    \n    if not client_id:\n        raise HTTPException(status_code=400, detail=\"client_id is required\")\n    \n    await field_mapper.save_field_config(\n        client_id=client_id,\n        collection_name=collection_name,\n        field_paths=field_paths\n    )\n    \n    return {\"status\": \"success\"}\n\n@directus_router.post(\"/webhook\")\nasync def directus_webhook_handler(\n    payload: DirectusWebhookPayload,\n    translation_service = Depends(get_translation_service)\n):\n    \"\"\"Handle webhook events from Directus for auto-translation\"\"\"\n    if payload.event not in [\"create\", \"update\"]:\n        return {\"status\": \"ignored\", \"reason\": f\"Event {payload.event} not configured for translation\"}\n    \n    results = []\n    for target_lang in payload.target_languages:\n        try:\n            result = await translation_service.translate_content(\n                content=payload.item,\n                source_lang=\"en\",  # Assuming English is the source\n                target_lang=target_lang,\n                openai_key=payload.openai_key,\n                google_key=payload.google_key,\n                collection_name=payload.collection,\n                client_id=payload.client_id\n            )\n            results.append({\n                \"language\": target_lang,\n                \"status\": \"success\",\n                \"translated_item\": result[\"translated_content\"]\n            })\n        except Exception as e:\n            results.append({\n                \"language\": target_lang,\n                \"status\": \"error\",\n                \"error\": str(e)\n            })\n    \n    return {\"results\": results}\n\n@directus_router.post(\"/schema/introspect\")\nasync def introspect_collection_schema(\n    request: DirectusSchemaRequest,\n    directus_service = Depends(get_directus_service)\n):\n    \"\"\"Introspect a Directus collection schema to identify translatable fields\"\"\"\n    schema = await directus_service.get_collection_schema(\n        client_id=request.client_id,\n        collection=request.collection\n    )\n    \n    # Identify fields that are likely to contain translatable content\n    translatable_fields = []\n    for field in schema[\"fields\"]:\n        if field[\"type\"] in [\"string\", \"text\", \"json\"] or field.get(\"interface\") in [\"input\", \"input-rich-text-md\", \"input-rich-text-html\"]:\n            translatable_fields.append(field[\"field\"])\n    \n    return {\n        \"collection\": request.collection,\n        \"schema\": schema,\n        \"suggested_translatable_fields\": translatable_fields\n    }\n\n# Add the router to the main app\napp.include_router(directus_router)\n```\n\nDirectus helper functions:\n```python\ndef format_directus_response(translated_items, original_items):\n    \"\"\"Format the response in a way that's easy to use with Directus\"\"\"\n    result = []\n    \n    for orig, trans in zip(original_items, translated_items):\n        # Create a new item with the same structure as the original\n        # but with translated content\n        result.append({\n            \"id\": orig[\"id\"],\n            \"collection\": orig[\"collection\"],\n            \"item\": trans[\"translated_content\"]\n        })\n    \n    return result\n\nclass DirectusService:\n    \"\"\"Service for interacting with Directus CMS\"\"\"\n    \n    def __init__(self, db):\n        self.db = db\n        self.client_configs = {}\n    \n    async def get_directus_client(self, client_id):\n        \"\"\"Get or create a Directus SDK client for the given client_id\"\"\"\n        if client_id not in self.client_configs:\n            # Get client config from database\n            config = await self.db.fetch_one(\n                \"SELECT directus_url, directus_token FROM client_configs WHERE client_id = $1\",\n                client_id\n            )\n            \n            if not config:\n                raise ValueError(f\"No Directus configuration found for client {client_id}\")\n            \n            self.client_configs[client_id] = {\n                \"url\": config[\"directus_url\"],\n                \"token\": config[\"directus_token\"],\n                \"client\": Directus(config[\"directus_url\"], token=config[\"directus_token\"])\n            }\n        \n        return self.client_configs[client_id][\"client\"]\n    \n    async def get_collection_schema(self, client_id, collection):\n        \"\"\"Get the schema for a Directus collection\"\"\"\n        client = await self.get_directus_client(client_id)\n        return await client.collections.read_one(collection)\n    \n    async def update_translated_item(self, client_id, collection, item_id, translated_data, language):\n        \"\"\"Update an item with translated content in the specified language\"\"\"\n        client = await self.get_directus_client(client_id)\n        \n        # Assuming Directus uses a language field or has a translations relation\n        # This implementation will vary based on how translations are structured in Directus\n        if collection.endswith(\"_translations\"):\n            # If using a translations collection\n            await client.items(collection).update({\n                \"item_id\": item_id,\n                \"language\": language,\n                **translated_data\n            })\n        else:\n            # If using language-specific fields (e.g., title_fr, description_fr)\n            language_fields = {}\n            for key, value in translated_data.items():\n                language_fields[f\"{key}_{language}\"] = value\n            \n            await client.items(collection).update(item_id, language_fields)\n        \n        return {\"status\": \"success\"}\n```",
      "testStrategy": "1. Test batch translation endpoint with Directus collection data\n2. Verify correct handling of Directus-specific data structures\n3. Test collection configuration endpoint\n4. Validate field mapping works correctly with Directus fields\n5. Test performance with larger batches of items\n6. Verify error handling for partial batch failures\n7. Test with actual Directus API responses\n8. Validate statistics reporting\n9. Test with different collection types and field structures\n10. Verify integration with Directus webhook patterns\n11. Test webhook-based auto-translation functionality\n12. Verify schema introspection correctly identifies translatable fields\n13. Test Directus SDK integration for reading and updating content\n14. Validate the system works with both translations collections and language-specific fields\n15. Test end-to-end workflow from content creation to translation via webhooks\n16. Verify compatibility with directus-translator patterns for seamless migration",
      "subtasks": []
    },
    {
      "id": 7,
      "title": "Address CodeRabbit AI Review Feedback for Translation API",
      "description": "Fix critical security vulnerabilities, code quality issues, and performance optimizations identified in the AI translation provider integration based on CodeRabbit AI review feedback for PR #1.",
      "details": "1. Security Fixes:\n   - Implement proper exception chaining to prevent information leakage\n   - Add input sanitization for all user-provided data, especially API keys and translation content\n   - Configure CORS security headers properly for API endpoints\n   - Validate and sanitize all query parameters and request bodies\n   - Implement rate limiting for API endpoints\n\n2. Code Quality Improvements:\n   - Refactor error handling across provider integrations for consistency\n   - Add comprehensive logging with appropriate log levels\n   - Improve code documentation and type hints\n   - Standardize naming conventions across the codebase\n   - Remove redundant code and consolidate similar functions\n\n3. Performance Optimizations:\n   - Optimize API request patterns to external providers\n   - Improve Redis caching efficiency with better key strategies\n   - Implement connection pooling for external API calls\n   - Add request timeouts and circuit breakers for provider APIs\n   - Optimize batch processing for translation requests\n\n4. Architectural Improvements:\n   - Refactor provider integration to improve maintainability\n   - Enhance fallback mechanisms between providers\n   - Implement better separation of concerns in the translation pipeline\n   - Add metrics collection for performance monitoring\n   - Improve configuration management for provider-specific settings\n\n5. Production Readiness:\n   - Add comprehensive error reporting\n   - Implement graceful degradation when providers are unavailable\n   - Enhance request validation and response formatting\n   - Add health check endpoints with detailed status information\n   - Implement proper API versioning",
      "testStrategy": "1. Security Testing:\n   - Run automated security scanning tools (OWASP ZAP, Bandit) against the API\n   - Perform penetration testing focusing on input validation and authentication\n   - Test exception handling to ensure no sensitive information is leaked\n   - Verify CORS configuration with cross-domain requests\n   - Test rate limiting functionality\n\n2. Code Quality Verification:\n   - Run static code analysis tools (pylint, flake8, mypy)\n   - Perform code review to verify all identified issues are addressed\n   - Verify consistent error handling across all modules\n   - Check logging implementation for appropriate detail and levels\n   - Verify documentation completeness and accuracy\n\n3. Performance Testing:\n   - Benchmark API response times before and after changes\n   - Test Redis caching efficiency with repeated requests\n   - Measure performance under load with concurrent requests\n   - Verify timeout and circuit breaker functionality\n   - Test batch processing efficiency with various payload sizes\n\n4. Integration Testing:\n   - Verify all provider integrations still function correctly\n   - Test fallback mechanisms between providers\n   - Verify Directus integration functionality\n   - Test end-to-end translation workflows\n   - Verify metrics collection accuracy\n\n5. Production Readiness Verification:\n   - Deploy to staging environment and monitor behavior\n   - Test health check endpoints for accurate status reporting\n   - Verify graceful degradation scenarios\n   - Test API versioning compatibility\n   - Perform load testing to verify stability under production-like conditions",
      "status": "done",
      "dependencies": [
        2,
        3,
        4,
        5
      ],
      "priority": "high",
      "subtasks": [
        {
          "id": 1,
          "title": "Fix Exception Chaining Issues (Critical Security)",
          "description": "Fix all exception chaining issues across provider implementations to prevent information leakage and improve debugging. Add 'from e' to all exception raises where original exceptions are wrapped.",
          "details": "Files to fix:\n- app/services/openai_provider.py (lines 52-59, 88)\n- app/services/anthropic_provider.py (lines 46-53, 81)\n- app/services/mistral_provider.py (lines 70-73, 101)\n- app/services/deepseek_provider.py (lines 71-73, 91)\n- app/api/translation.py (lines 121-123, 160-162, 193, 211, 231)\n\nChange pattern from:\nraise ProviderError(self.name, f\"Error: {str(e)}\", e)\n\nTo:\nraise ProviderError(self.name, f\"Error: {str(e)}\", e) from e",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 7
        },
        {
          "id": 2,
          "title": "Implement Input Sanitization (Critical Security)",
          "description": "Implement input sanitization to prevent prompt injection attacks and token limit exceeded errors. Add length limits, control character stripping, and safe text handling.",
          "details": "Critical security vulnerability in app/services/translation_provider.py lines 166-184:\n\nCurrent unsafe code:\n```python\nprompt += f\"\\n\\nText to translate: {text}\"\n```\n\nImplement:\n1. MAX_CHARS = 2000 limit for text and context\n2. Strip control characters and potential injection strings\n3. Truncate user input before embedding in prompts\n4. Validate text content for safety\n\nAdd method:\n```python\ndef _sanitize_text(self, text: str, max_chars: int = 2000) -> str:\n    # Remove control chars, truncate, escape dangerous patterns\n    safe_text = text[:max_chars]\n    safe_text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', safe_text)\n    return safe_text.strip()\n```",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 7
        },
        {
          "id": 3,
          "title": "Fix CORS Security Configuration",
          "description": "Fix CORS security configuration and tighten production security settings. Replace wildcard origins with environment-specific allowed domains.",
          "details": "Security issue in app/main.py lines 31-35:\n\nCurrent unsafe configuration:\n```python\nallow_origins=[\"*\"]  # ❌ Too permissive\n```\n\nFix by:\n1. Create environment-specific CORS settings in app/config.py\n2. Add CORS_ALLOWED_ORIGINS to settings with default secure values\n3. Update main.py to use environment-controlled origins\n\nRecommended implementation:\n```python\n# In config.py\nCORS_ALLOWED_ORIGINS = [\n    \"http://localhost:3000\",  # Development\n    \"https://yourdomain.com\", # Production\n    \"https://admin.yourdomain.com\"\n]\n\n# In main.py\nadd_middleware(\n    CORSMiddleware,\n    allow_origins=settings.CORS_ALLOWED_ORIGINS,\n    allow_credentials=True,\n    allow_methods=[\"GET\", \"POST\"],\n    allow_headers=[\"*\"],\n)\n```",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 7
        },
        {
          "id": 4,
          "title": "Code Hygiene and Formatting Cleanup",
          "description": "Clean up code hygiene issues: remove trailing whitespace, fix unused imports, wrap long lines, and improve code formatting across all files.",
          "details": "Code hygiene issues identified by CodeRabbit:\n\n1. Trailing whitespace (28+ locations):\n   - app/services/openai_provider.py (lines 13, 20, 22-25, 33, 46, 51, 60, 62-65, 72, 78, 89, 104, 108)\n   - app/services/anthropic_provider.py (lines 13, 20, 22-25, 33, 40, 45, 54, 56-59, 66, 71, 82, 97, 101)\n   - app/services/mistral_provider.py (similar pattern)\n   - app/services/translation_provider.py (similar pattern)\n   - tests/test_translation_providers.py (similar pattern)\n\n2. Unused imports to remove:\n   - app/api/translation.py: Remove unused `Depends` and `LanguageDirection`\n   - app/services/flexible_translation_service.py: Remove `asyncio`, `Tuple`, `TranslationProvider`\n   - tests/test_translation_providers.py: Remove `asyncio`, `patch`, `TranslationError`, `ProviderError`\n\n3. Long lines to wrap (>100 chars):\n   - Multiple files have lines exceeding 100 character limit\n\nAutomated fix commands:\n```bash\n# Remove trailing whitespace\nfind . -name \"*.py\" -exec sed -i 's/[[:space:]]*$//' {} \\;\n\n# Use ruff to auto-fix imports\nruff check --fix app/ tests/\n```",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 7
        },
        {
          "id": 5,
          "title": "Performance Optimizations",
          "description": "Optimize performance by implementing concurrent quality assessment and fixing shallow copy issues in collection translation.",
          "details": "Performance optimizations needed:\n\n1. Sequential Quality Assessment Issue (app/services/flexible_translation_service.py:205-209):\n   Current inefficient code:\n   ```python\n   for i, (original, translated) in enumerate(zip(texts, translated_texts)):\n       quality_score = await translation_provider.assess_translation_quality(...)\n   ```\n\n   Fix with concurrent execution:\n   ```python\n   quality_tasks = [\n       translation_provider.assess_translation_quality(orig, trans, source_lang, target_lang)\n       for orig, trans in zip(texts, translated_texts)\n   ]\n   quality_scores = await asyncio.gather(*quality_tasks)\n   ```\n\n2. Shallow Copy Issue (app/services/flexible_translation_service.py:311):\n   Current unsafe code:\n   ```python\n   translated_data = collection_data.copy()  # ❌ Shallow copy\n   ```\n\n   Fix with deep copy:\n   ```python\n   import copy\n   translated_data = copy.deepcopy(collection_data)  # ✅ Deep copy\n   ```\n\n3. Make assess_translation_quality synchronous:\n   Remove unnecessary async from method that does no I/O operations",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 7
        },
        {
          "id": 6,
          "title": "Refactor Provider Architecture (Code Deduplication)",
          "description": "Refactor provider implementations to reduce code duplication by extracting common functionality into a base provider class.",
          "details": "Code duplication issue identified by CodeRabbit:\n\nCurrent problem: OpenAI, Anthropic, Mistral, and DeepSeek providers share significant duplicate code:\n- Same structure and error handling patterns\n- Identical batch_translate implementations\n- Similar validate_api_key methods\n- Repeated exception handling\n\nRecommended solution:\nCreate BaseAsyncProvider class with common functionality:\n\n```python\nclass BaseAsyncProvider(TranslationProvider):\n    \"\"\"Base class for async translation providers with common functionality.\"\"\"\n    \n    async def batch_translate(self, texts, source_lang, target_lang, api_key, context=None):\n        \"\"\"Common batch translation implementation.\"\"\"\n        if not texts:\n            return []\n        \n        tasks = [\n            self.translate(text, source_lang, target_lang, api_key, context)\n            for text in texts\n        ]\n        \n        try:\n            results = await asyncio.gather(*tasks, return_exceptions=True)\n            translations = []\n            for result in results:\n                if isinstance(result, Exception):\n                    raise result\n                translations.append(result)\n            return translations\n        except Exception as e:\n            raise ProviderError(self.name, f\"Batch translation failed: {str(e)}\", e) from e\n```\n\nThis would eliminate duplicate code across all providers and make maintenance easier.",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 7
        },
        {
          "id": 7,
          "title": "Improve Exception Handling Specificity",
          "description": "Improve exception handling specificity by replacing broad Exception catches with specific exception types in provider validation methods.",
          "details": "Issue: Multiple providers catch broad Exception types in validation methods, which can mask unexpected errors.\n\nFiles to fix:\n1. app/services/openai_provider.py (lines 100-103):\n   ```python\n   # ❌ Current\n   except openai.AuthenticationError:\n       return False\n   except Exception:  # Too broad\n       return False\n   \n   # ✅ Should be\n   except openai.AuthenticationError:\n       return False\n   except (openai.APIError, openai.RateLimitError, openai.APIConnectionError):\n       return False\n   ```\n\n2. app/services/anthropic_provider.py (lines 93-96):\n   Similar fix needed for Anthropic-specific exceptions\n\n3. app/services/mistral_provider.py and deepseek_provider.py:\n   Replace generic Exception with specific HTTP and API errors\n\nBenefits:\n- Better error visibility and debugging\n- More precise error handling\n- Prevents masking of unexpected errors\n- Follows Python best practices for exception handling\n\nEach provider should catch only the specific exceptions it expects during validation.",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 7
        }
      ]
    }
  ]
}