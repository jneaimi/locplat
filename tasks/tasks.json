{
  "tasks": [
    {
      "id": 1,
      "title": "Setup FastAPI Project with Docker",
      "description": "Initialize the FastAPI project with PostgreSQL and Redis integration, and create Docker configuration for easy deployment.",
      "details": "1. Create a new FastAPI project structure\n2. Set up PostgreSQL connection using SQLAlchemy ORM\n3. Configure Redis client for caching\n4. Create Docker and docker-compose files for local development\n5. Implement health check endpoint (GET /health)\n\nProject structure:\n```\nlocplat/\n├── app/\n│   ├── __init__.py\n│   ├── main.py\n│   ├── config.py\n│   ├── models/\n│   ├── api/\n│   ├── services/\n│   └── utils/\n├── tests/\n├── Dockerfile\n├── docker-compose.yml\n├── requirements.txt\n└── README.md\n```\n\nMain dependencies:\n- fastapi\n- uvicorn\n- sqlalchemy\n- psycopg2-binary\n- redis\n- pydantic\n\nImplement basic health check endpoint in main.py:\n```python\n@app.get('/health')\ndef health_check():\n    return {'status': 'ok'}\n```",
      "testStrategy": "1. Verify FastAPI server starts correctly\n2. Confirm PostgreSQL connection is established\n3. Validate Redis connection is working\n4. Test health check endpoint returns 200 OK\n5. Ensure Docker containers build and run properly\n6. Validate environment variables are properly loaded",
      "priority": "high",
      "dependencies": [],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 2,
      "title": "Implement Translation Provider Integration",
      "description": "Create services to integrate with multiple AI providers (OpenAI as primary, Anthropic Claude as secondary, Mistral AI as tertiary, and DeepSeek as fallback) using client-provided API keys, supporting the Directus translation interface.",
      "status": "done",
      "dependencies": [
        1
      ],
      "priority": "high",
      "details": "1. Create abstract translation provider interface\n2. Implement OpenAI provider as primary using their API\n3. Implement Anthropic Claude provider as secondary\n4. Implement Mistral AI provider as tertiary\n5. Implement DeepSeek provider as final fallback\n6. Create provider router to handle cascading fallback logic\n7. Add language pair support with proper handling for Arabic (RTL) and Bosnian (Latin/Cyrillic)\n8. Support structured responses compatible with Directus translation interface\n9. Implement collection-specific translations following Directus patterns\n10. Support batch translation for multiple fields\n11. Add language direction support (LTR/RTL)\n12. Implement translation quality assessment and validation\n13. Support nested JSON structures and field mapping\n14. Ensure translation context preservation and formatting maintenance\n15. Implement provider-specific optimizations and prompt engineering\n\nProvider interface:\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any, List\nfrom enum import Enum\n\nclass LanguageDirection(Enum):\n    LTR = \"ltr\"\n    RTL = \"rtl\"\n\nclass TranslationProvider(ABC):\n    @abstractmethod\n    async def translate(self, text: str, source_lang: str, target_lang: str, api_key: str) -> str:\n        pass\n        \n    @abstractmethod\n    async def batch_translate(self, texts: List[str], source_lang: str, target_lang: str, api_key: str) -> List[str]:\n        pass\n\nclass OpenAIProvider(TranslationProvider):\n    async def translate(self, text: str, source_lang: str, target_lang: str, api_key: str) -> str:\n        # OpenAI implementation using their API\n        # Use the provided API key for authentication\n        pass\n        \n    async def batch_translate(self, texts: List[str], source_lang: str, target_lang: str, api_key: str) -> List[str]:\n        # Batch translation implementation\n        pass\n\nclass AnthropicProvider(TranslationProvider):\n    async def translate(self, text: str, source_lang: str, target_lang: str, api_key: str) -> str:\n        # Anthropic Claude implementation\n        pass\n        \n    async def batch_translate(self, texts: List[str], source_lang: str, target_lang: str, api_key: str) -> List[str]:\n        # Batch translation implementation\n        pass\n\nclass MistralProvider(TranslationProvider):\n    async def translate(self, text: str, source_lang: str, target_lang: str, api_key: str) -> str:\n        # Mistral AI implementation\n        pass\n        \n    async def batch_translate(self, texts: List[str], source_lang: str, target_lang: str, api_key: str) -> List[str]:\n        # Batch translation implementation\n        pass\n\nclass DeepSeekProvider(TranslationProvider):\n    async def translate(self, text: str, source_lang: str, target_lang: str, api_key: str) -> str:\n        # DeepSeek implementation\n        pass\n        \n    async def batch_translate(self, texts: List[str], source_lang: str, target_lang: str, api_key: str) -> List[str]:\n        # Batch translation implementation\n        pass\n\nclass ProviderRouter:\n    def __init__(self):\n        self.providers = [\n            OpenAIProvider(),      # Primary\n            AnthropicProvider(),   # Secondary\n            MistralProvider(),     # Tertiary\n            DeepSeekProvider()     # Final fallback\n        ]\n    \n    async def translate(self, text: str, source_lang: str, target_lang: str, \n                        api_keys: Dict[str, str]) -> str:\n        last_error = None\n        for i, provider in enumerate(self.providers):\n            try:\n                provider_name = provider.__class__.__name__.replace('Provider', '').lower()\n                if provider_name in api_keys and api_keys[provider_name]:\n                    return await provider.translate(text, source_lang, target_lang, api_keys[provider_name])\n            except Exception as e:\n                # Log the error\n                last_error = e\n                continue\n        raise Exception(f\"All translation providers failed. Last error: {last_error}\")\n        \n    async def batch_translate(self, texts: List[str], source_lang: str, target_lang: str,\n                             api_keys: Dict[str, str]) -> List[str]:\n        # Similar cascading fallback logic for batch translation\n        pass\n        \n    async def translate_collection(self, collection_data: Dict[str, Any], fields: List[str],\n                                  source_lang: str, target_lang: str, api_keys: Dict[str, str]) -> Dict[str, Any]:\n        # Implement collection-specific translation following Directus patterns\n        pass\n        \n    def get_language_direction(self, lang_code: str) -> LanguageDirection:\n        # Return language direction (RTL for Arabic, LTR for others)\n        rtl_languages = ['ar', 'he', 'fa', 'ur']\n        return LanguageDirection.RTL if lang_code in rtl_languages else LanguageDirection.LTR\n        \n    async def assess_quality(self, original: str, translation: str, source_lang: str, target_lang: str) -> float:\n        # Implement translation quality assessment\n        pass\n        \n    async def translate_nested_json(self, json_data: Dict[str, Any], field_mapping: Dict[str, str],\n                                  source_lang: str, target_lang: str, api_keys: Dict[str, str]) -> Dict[str, Any]:\n        # Handle nested JSON structures with field mapping\n        pass\n        \n    async def optimize_prompt(self, text: str, provider_name: str, source_lang: str, target_lang: str) -> str:\n        # Provider-specific prompt engineering optimizations\n        # Enhance context preservation and cultural sensitivity\n        pass\n```\n\nImplement GET /languages endpoint to return supported language pairs with direction information.\n\nNever store client API keys - they should be provided with each request and used only for that specific translation operation.",
      "testStrategy": "1. Unit test each provider with mock API responses\n2. Test cascading fallback mechanism when providers fail\n3. Verify correct handling of API keys for each provider\n4. Test with actual API keys in development environment\n5. Validate supported language pairs including RTL support for Arabic\n6. Test both Latin and Cyrillic script support for Bosnian\n7. Test error handling for invalid API keys\n8. Measure response times for performance benchmarking\n9. Test batch translation functionality\n10. Verify collection-specific translations follow Directus patterns\n11. Test language direction detection\n12. Validate translation quality assessment\n13. Test structured response compatibility with Directus translation interface\n14. Verify nested JSON structure handling and field mapping\n15. Test translation context preservation across different providers\n16. Validate formatting maintenance in translated content\n17. Test cultural sensitivity handling, especially for Arabic content\n18. Verify API keys are never stored and only used for the current request\n19. Test provider-specific optimizations and prompt engineering effectiveness",
      "subtasks": []
    },
    {
      "id": 3,
      "title": "Implement Redis Caching Layer",
      "description": "Create a caching system using Redis to store AI responses from multiple providers (OpenAI, Anthropic, Mistral, and DeepSeek) to avoid duplicate API calls, reducing costs and improving performance. Optimize for Directus integration patterns with provider-specific caching strategies.",
      "status": "done",
      "dependencies": [
        1
      ],
      "priority": "medium",
      "details": "# Redis Caching Layer Implementation Summary\n\n## ✅ Completed Features\n\n### 1. Core Cache Service (`ai_response_cache.py`)\n- **Multi-provider cache isolation** - Separate cache keys for OpenAI, Anthropic, Mistral, DeepSeek\n- **Cost-aware TTL strategies** - Longer cache times for expensive providers (Claude Opus = 2x TTL, GPT-4 = 1.5x)\n- **Intelligent compression** - Automatic zlib compression for responses >1KB\n- **Provider-specific metrics** - Hit/miss tracking per provider and model\n- **Directus collection support** - Collection-aware cache keys and invalidation\n- **Cache warming and batch operations** - Efficient bulk caching with pipelines\n\n### 2. Cache Integration Service (`cached_translation_service.py`)\n- **Middleware layer** - Wraps existing provider router with caching\n- **Cascading fallback** - Tries cache first, then AI providers in order\n- **Batch translation support** - Efficient caching for multiple requests\n- **Statistics and management** - Cache stats and invalidation methods\n\n### 3. API Endpoints (`/api/cache.py`)\n- `GET /api/v1/cache/stats` - Cache hit/miss statistics\n- `GET /api/v1/cache/info` - Memory usage and key counts\n- `DELETE /api/v1/cache/invalidate` - Targeted cache invalidation\n- `DELETE /api/v1/cache/clear` - Emergency cache clearing\n\n### 4. Application Integration\n- **Lifecycle management** - Proper Redis connection handling in FastAPI\n- **Docker integration** - Redis service configured in docker-compose.yml\n- **Configuration** - Redis URL and cache settings in config.py\n\n### 5. Testing Infrastructure\n- **Unit tests** - Cache key generation, TTL calculation, compression\n- **Integration tests** - Cache hit/miss scenarios with mocked services\n- **Cost tier validation** - Ensures expensive providers get longer TTL\n\n## 📊 Cache Configuration\n\n### Provider Cost Tiers (TTL Multipliers):\n- **Very High (2.0x)**: Claude-3 Opus\n- **High (1.5x)**: GPT-4, Claude-3 Sonnet, Mistral Large  \n- **Medium (1.0x)**: Claude-3 Haiku, DeepSeek models\n- **Low (0.8x)**: GPT-3.5-Turbo, Mistral Tiny\n\n### Content Type TTL:\n- **Critical**: 12 hours (0.5x)\n- **Standard**: 24 hours (1.0x)\n- **Static**: 7 days (7.0x)\n- **Temporary**: 6 hours (0.25x)\n\n## 🔧 Key Technical Decisions\n\n1. **Async Redis client** - Uses redis.asyncio for non-blocking operations\n2. **Content hashing** - MD5 hash of prompt + target language for uniqueness\n3. **Compression threshold** - 1KB threshold balances performance vs storage\n4. **Pipeline operations** - Batch operations use Redis pipelines for efficiency\n5. **Scan-based operations** - Uses SCAN instead of KEYS for better performance\n6. **Singleton pattern** - Global cache instance with proper lifecycle management\n\n## 🚀 Ready for Integration\n\nThe cache layer is now ready to integrate with:\n- Task #2: AI Provider Integration (can now use `CachedTranslationService`)\n- Task #4: Field Mapping (collection-aware caching)\n- Task #5: Translation API Endpoints (cache statistics endpoints)\n- Task #6: Directus Integration (collection invalidation)\n\n## 📈 Performance Benefits\n\n- **Cost savings** - Avoids duplicate API calls to expensive providers\n- **Response time** - Cached responses return instantly\n- **Rate limit protection** - Reduces API requests within rate limits\n- **Intelligent TTL** - Balances freshness vs cost based on provider pricing\n\nOriginal implementation code:\n```python\nimport hashlib\nimport json\nimport zlib\nfrom redis import Redis\nfrom typing import Dict, Any, Optional, List, Tuple\n\nclass AIResponseCache:\n    def __init__(self, redis_client: Redis, default_ttl_seconds: int = 86400):\n        self.redis = redis_client\n        self.default_ttl = default_ttl_seconds\n        self.version = 1  # For cache versioning\n        \n        # Define cost tiers for different providers to influence caching strategy\n        self.provider_cost_tiers = {\n            'openai': {\n                'gpt-3.5-turbo': 'low',\n                'gpt-4': 'high',\n                'gpt-4-turbo': 'high'\n            },\n            'anthropic': {\n                'claude-instant': 'medium',\n                'claude-2': 'high',\n                'claude-3-opus': 'very_high',\n                'claude-3-sonnet': 'high',\n                'claude-3-haiku': 'medium'\n            },\n            'mistral': {\n                'mistral-tiny': 'low',\n                'mistral-small': 'medium',\n                'mistral-medium': 'medium',\n                'mistral-large': 'high'\n            },\n            'deepseek': {\n                'deepseek-coder': 'medium',\n                'deepseek-chat': 'medium'\n            }\n        }\n    \n    def _generate_key(self, prompt: str, provider: str, model: str, collection: str = None) -> str:\n        # Create a unique hash based on prompt, provider, model and optional collection\n        content_hash = hashlib.md5(prompt.encode()).hexdigest()\n        base_key = f\"ai_response:v{self.version}:{provider}:{model}:{content_hash}\"\n        if collection:\n            return f\"{base_key}:collection:{collection}\"\n        return base_key\n    \n    def _compress_content(self, content: str) -> bytes:\n        # Compress large content blocks\n        return zlib.compress(content.encode('utf-8'))\n    \n    def _decompress_content(self, compressed_data: bytes) -> str:\n        # Decompress content\n        return zlib.decompress(compressed_data).decode('utf-8')\n    \n    def _calculate_ttl(self, content_type: str, provider: str, model: str, confidence: float = 1.0) -> int:\n        # Dynamic TTL based on content type, provider cost tier, and confidence\n        base_ttl = self.default_ttl\n        \n        # Adjust TTL based on content type\n        if content_type == 'critical':\n            base_ttl = 43200  # 12 hours for critical content\n        elif content_type == 'static':\n            base_ttl = 604800  # 7 days for static content\n        \n        # Adjust TTL based on provider cost tier\n        cost_tier = 'medium'  # Default\n        if provider in self.provider_cost_tiers and model in self.provider_cost_tiers[provider]:\n            cost_tier = self.provider_cost_tiers[provider][model]\n        \n        # More expensive models get longer cache times to save costs\n        tier_multipliers = {\n            'low': 0.8,\n            'medium': 1.0,\n            'high': 1.5,\n            'very_high': 2.0\n        }\n        \n        tier_factor = tier_multipliers.get(cost_tier, 1.0)\n        confidence_factor = max(0.5, min(1.5, confidence))  # Between 0.5 and 1.5\n        \n        return int(base_ttl * tier_factor * confidence_factor)\n    \n    async def get_cached_response(self, prompt: str, provider: str, model: str, collection: str = None) -> Optional[str]:\n        key = self._generate_key(prompt, provider, model, collection)\n        cached = self.redis.get(key)\n        if cached:\n            # Track cache hit for this provider\n            self.redis.incr(f'cache:{provider}:{model}:hits')\n            # Check if content is compressed\n            try:\n                return self._decompress_content(cached)\n            except zlib.error:\n                # Not compressed\n                return cached.decode('utf-8')\n        # Track cache miss for this provider\n        self.redis.incr(f'cache:{provider}:{model}:misses')\n        return None\n    \n    async def cache_response(self, prompt: str, provider: str, model: str, response: str, collection: str = None, content_type: str = 'standard', confidence: float = 1.0) -> None:\n        key = self._generate_key(prompt, provider, model, collection)\n        ttl = self._calculate_ttl(content_type, provider, model, confidence)\n        \n        # Compress large content\n        if len(response) > 1000:\n            compressed_data = self._compress_content(response)\n            self.redis.set(key, compressed_data, ex=ttl)\n        else:\n            self.redis.set(key, response, ex=ttl)\n    \n    async def cache_batch_responses(self, items: List[Dict[str, Any]]) -> None:\n        pipeline = self.redis.pipeline()\n        for item in items:\n            key = self._generate_key(\n                item['prompt'], \n                item['provider'], \n                item['model'],\n                item.get('collection')\n            )\n            ttl = self._calculate_ttl(\n                item.get('content_type', 'standard'), \n                item['provider'], \n                item['model'],\n                item.get('confidence', 1.0)\n            )\n            response = item['response']\n            \n            # Compress large content\n            if len(response) > 1000:\n                compressed_data = self._compress_content(response)\n                pipeline.set(key, compressed_data, ex=ttl)\n            else:\n                pipeline.set(key, response, ex=ttl)\n        pipeline.execute()\n    \n    async def invalidate_cache(self, provider: str = None, model: str = None, collection: str = None) -> int:\n        # Build pattern for keys to delete\n        pattern_parts = ['ai_response:v' + str(self.version)]  \n        if provider:\n            pattern_parts.append(provider)\n        else:\n            pattern_parts.append('*')\n            \n        if model:\n            pattern_parts.append(model)\n        else:\n            pattern_parts.append('*')\n            \n        pattern_parts.append('*')  # For content hash\n        \n        if collection:\n            pattern_parts.append('collection:' + collection)\n            \n        pattern = ':'.join(pattern_parts)\n        \n        # Get keys matching pattern\n        keys = self.redis.keys(pattern)\n        if keys:\n            return self.redis.delete(*keys)\n        return 0\n    \n    async def warm_cache(self, frequent_content: List[Dict[str, Any]]) -> None:\n        # Pre-populate cache with frequently accessed content\n        # This would typically be called by a scheduled job\n        for item in frequent_content:\n            cached = await self.get_cached_response(\n                item['prompt'],\n                item['provider'],\n                item['model'],\n                item.get('collection')\n            )\n            \n            # If not in cache and we have a response, cache it\n            if not cached and 'response' in item:\n                await self.cache_response(\n                    item['prompt'],\n                    item['provider'],\n                    item['model'],\n                    item['response'],\n                    item.get('collection'),\n                    item.get('content_type', 'standard'),\n                    item.get('confidence', 1.0)\n                )\n    \n    async def get_cache_stats(self, provider: str = None, model: str = None) -> Dict[str, Any]:\n        if provider and model:\n            hits = int(self.redis.get(f'cache:{provider}:{model}:hits') or 0)\n            misses = int(self.redis.get(f'cache:{provider}:{model}:misses') or 0)\n            return {\n                'provider': provider,\n                'model': model,\n                'hits': hits,\n                'misses': misses,\n                'hit_rate': hits / (hits + misses) if (hits + misses) > 0 else 0\n            }\n        elif provider:\n            # Get stats for all models of a provider\n            stats = {'provider': provider, 'models': {}, 'total_hits': 0, 'total_misses': 0}\n            model_keys = self.redis.keys(f'cache:{provider}:*:hits')\n            \n            for key in model_keys:\n                key_parts = key.decode('utf-8').split(':')\n                model = key_parts[2]\n                hits = int(self.redis.get(f'cache:{provider}:{model}:hits') or 0)\n                misses = int(self.redis.get(f'cache:{provider}:{model}:misses') or 0)\n                \n                stats['models'][model] = {\n                    'hits': hits,\n                    'misses': misses,\n                    'hit_rate': hits / (hits + misses) if (hits + misses) > 0 else 0\n                }\n                \n                stats['total_hits'] += hits\n                stats['total_misses'] += misses\n            \n            total = stats['total_hits'] + stats['total_misses']\n            stats['overall_hit_rate'] = stats['total_hits'] / total if total > 0 else 0\n            \n            return stats\n        else:\n            # Get stats for all providers\n            stats = {'providers': {}, 'overall': {'hits': 0, 'misses': 0}}\n            provider_keys = self.redis.keys('cache:*:*:hits')\n            \n            for key in provider_keys:\n                key_parts = key.decode('utf-8').split(':')\n                provider = key_parts[1]\n                model = key_parts[2]\n                hits = int(self.redis.get(f'cache:{provider}:{model}:hits') or 0)\n                misses = int(self.redis.get(f'cache:{provider}:{model}:misses') or 0)\n                \n                if provider not in stats['providers']:\n                    stats['providers'][provider] = {'models': {}, 'total_hits': 0, 'total_misses': 0}\n                \n                stats['providers'][provider]['models'][model] = {\n                    'hits': hits,\n                    'misses': misses,\n                    'hit_rate': hits / (hits + misses) if (hits + misses) > 0 else 0\n                }\n                \n                stats['providers'][provider]['total_hits'] += hits\n                stats['providers'][provider]['total_misses'] += misses\n                stats['overall']['hits'] += hits\n                stats['overall']['misses'] += misses\n            \n            # Calculate overall hit rates for each provider\n            for provider in stats['providers']:\n                total = stats['providers'][provider]['total_hits'] + stats['providers'][provider]['total_misses']\n                stats['providers'][provider]['hit_rate'] = stats['providers'][provider]['total_hits'] / total if total > 0 else 0\n            \n            # Calculate overall hit rate\n            total = stats['overall']['hits'] + stats['overall']['misses']\n            stats['overall']['hit_rate'] = stats['overall']['hits'] / total if total > 0 else 0\n            \n            return stats\n```",
      "testStrategy": "## Completed Testing Strategy\n\n### Unit Tests\n1. ✅ Cache key generation with provider and model parameters\n2. ✅ TTL calculation based on content type, provider cost tier, and confidence\n3. ✅ Compression/decompression functionality for different content sizes\n4. ✅ Provider cost tier validation\n\n### Integration Tests\n5. ✅ Cache hit and miss scenarios across different AI providers (OpenAI, Anthropic, Mistral, DeepSeek)\n6. ✅ Cache statistics tracking per provider and model\n7. ✅ Cache behavior with different content types and sizes\n8. ✅ Concurrent cache access\n9. ✅ Batch response caching efficiency\n10. ✅ Cache invalidation for specific collections and models\n11. ✅ Cache warming functionality\n12. ✅ Cache versioning\n\n### Performance Tests\n13. ✅ Compression/decompression performance benchmarks\n14. ✅ Redis memory usage with and without compression\n15. ✅ Response time comparison: cached vs. non-cached requests\n16. ✅ Pipeline operation efficiency for batch operations\n\n### Directus Integration Tests\n17. ✅ Collection-specific caching patterns\n18. ✅ Collection-based cache invalidation\n\n### Provider-Specific Tests\n19. ✅ Provider-specific cache isolation\n20. ✅ Cost-aware caching strategies (verify longer TTL for expensive providers)\n21. ✅ Provider-specific confidence scoring impact on TTL\n22. ✅ Cache behavior with different models from the same provider\n\nAll tests have been successfully implemented and passed, confirming the Redis caching layer works as expected across all required scenarios.",
      "subtasks": [
        {
          "id": 3.1,
          "title": "Core Cache Service Implementation",
          "description": "Implement the AIResponseCache class with Redis integration, compression, and provider-specific handling",
          "status": "completed"
        },
        {
          "id": 3.2,
          "title": "Cache Integration Service",
          "description": "Create cached_translation_service.py to wrap existing provider router with caching middleware",
          "status": "completed"
        },
        {
          "id": 3.3,
          "title": "API Endpoints for Cache Management",
          "description": "Implement API endpoints for cache statistics, info, invalidation, and clearing",
          "status": "completed"
        },
        {
          "id": 3.4,
          "title": "Application Integration",
          "description": "Set up Redis connection handling in FastAPI, Docker integration, and configuration",
          "status": "completed"
        },
        {
          "id": 3.5,
          "title": "Testing Infrastructure",
          "description": "Create comprehensive test suite for cache functionality, performance, and integration",
          "status": "completed"
        }
      ]
    },
    {
      "id": 4,
      "title": "Implement Field Mapping and Content Processing",
      "description": "Create a system to map and process fields for translation, supporting Directus collection structures with primary fields and translation collections, handling plain text, HTML, and other content types with JSON path support. Focus on Directus-specific patterns and support for structured data from AI providers with batch operations.",
      "status": "in-progress",
      "dependencies": [
        2,
        3
      ],
      "priority": "medium",
      "details": "## Implemented Components (COMPLETED)\n\n### Database Models & Configuration\n- `FieldConfig` model with comprehensive field mapping support\n- `FieldProcessingLog` for operation tracking  \n- Field type enumerations (TEXT, WYSIWYG, HTML, JSON, etc.)\n- Support for RTL languages (Arabic, Hebrew, Farsi, Urdu)\n- Directus translation patterns (collection_translations, language_collections)\n\n### Core Field Mapper Service\n- `FieldMapper` class with extraction and processing logic\n- JSON path parser supporting dot notation and array indices\n- Content type auto-detection (HTML, multiline text, JSON)\n- HTML structure preservation during translation\n- Batch processing for efficient multi-field operations\n- Content sanitization and validation\n\n### Content Processor Service\n- `ContentProcessor` for AI provider response handling\n- Support for OpenAI, Anthropic, Mistral, DeepSeek response formats\n- Structured data parsing from various AI response formats\n- JSON and text-based response parsing\n\n### API Endpoints\n- RESTful API at `/api/v1/field-mapping/`\n- Create/update/delete field configurations\n- Extract translatable fields from content\n- Validate field paths against content\n- List configurations by client\n- Translation endpoints at `/api/v1/translate/` for structured content\n\n### Database Integration\n- Database dependency injection setup\n- Caching system for field configurations (5-minute TTL)\n- Operation logging for monitoring and debugging\n- Migration and initialization scripts\n\n### Translation Integration\n- `IntegratedTranslationService` combining FieldMapper with FlexibleTranslationService\n- Client-specified provider selection (not cascading fallback)\n- Structured content processing with intelligent field extraction\n- Batch translation support for cost efficiency\n- Complete translation workflow pipeline\n\n### Directus Webhook Integration\n- Comprehensive webhook system at `/api/v1/webhooks/directus/translate`\n- HMAC signature verification (SHA-256/SHA-1 support)\n- Support for all Directus translation patterns\n- Configuration validation and testing endpoints\n- Infinite loop prevention for translation collections\n- Complete automation from Directus → LocPlat → Translation → Storage\n\n## Key Features Implemented\n- **Directus CMS Integration**: Native support for collection_translations and language_collections patterns\n- **RTL Language Support**: Special field mapping for Arabic and other RTL languages  \n- **HTML Processing**: Extract/preserve HTML structure during translation\n- **Batch Operations**: Efficient processing of multiple text fields\n- **Content Sanitization**: Security validation for user content\n- **AI Provider Integration**: Handle structured responses from all supported providers\n- **Nested Field Support**: JSON path extraction with array indexing\n- **Type Detection**: Automatic field type detection (text, HTML, JSON, etc.)\n- **End-to-End Translation**: Complete workflow from field extraction to translated content\n- **Preview & Validation**: API endpoints for cost-free field extraction preview and validation\n- **Webhook Automation**: Automatic content processing via Directus webhooks\n- **Production Security**: HMAC signature verification and comprehensive input validation\n\n## Files Created/Modified\n\n### Core Implementation:\n- `app/models/field_config.py` - Database models\n- `app/models/field_types.py` - Type definitions and enums\n- `app/services/field_mapper.py` - Main field processing service\n- `app/services/content_processor.py` - AI response processing\n- `app/services/integrated_translation_service.py` - Translation integration orchestrator\n- `app/database.py` - Database connection setup\n- `app/api/field_mapping.py` - Field mapping REST API endpoints\n- `app/api/translation.py` - Enhanced with structured translation endpoints\n- `app/api/webhooks.py` - Directus webhook integration endpoints\n\n### Integration & Setup:\n- Updated `app/main.py` - Added field mapping and webhook routers\n- Updated `requirements.txt` - Added BeautifulSoup4 dependency\n- `scripts/init_field_mapping.py` - Database initialization\n- `scripts/test_field_mapping.py` - Comprehensive testing\n\n### Documentation:\n- `docs/field-mapping-guide.md` - Complete usage guide\n- `docs/webhook-integration-guide.md` - Directus webhook setup guide\n- `tests/test_field_mapping.py` - Unit tests\n- `tests/test_webhooks.py` - Webhook integration tests\n\n## Remaining Work\n1. Implement the Redis caching layer integration (Task #3)\n2. Perform end-to-end testing of the complete translation workflow\n\nDatabase model:\n```python\nfrom sqlalchemy import Column, Integer, String, JSON, DateTime, Boolean, ForeignKey\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom datetime import datetime\n\nBase = declarative_base()\n\nclass FieldConfig(Base):\n    __tablename__ = 'field_configs'\n    \n    id = Column(Integer, primary_key=True)\n    client_id = Column(String, nullable=False)\n    collection_name = Column(String, nullable=False)\n    field_paths = Column(JSON, nullable=False)  # JSON array of paths\n    created_at = Column(DateTime, default=datetime.utcnow)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n    is_translation_collection = Column(Boolean, default=False)\n    primary_collection = Column(String, nullable=True)\n    field_types = Column(JSON, nullable=True)  # Maps field paths to their types\n    rtl_field_mapping = Column(JSON, nullable=True)  # Special handling for RTL languages\n    directus_translation_pattern = Column(String, nullable=True)  # 'collection_translations' or 'language_collections'\n    batch_processing = Column(Boolean, default=False)  # Whether to process fields in batch\n\nclass FieldProcessingLog(Base):\n    __tablename__ = 'field_processing_logs'\n    \n    id = Column(Integer, primary_key=True)\n    client_id = Column(String, nullable=False)\n    collection_name = Column(String, nullable=False)\n    operation = Column(String, nullable=False)  # 'extract', 'process', 'translate'\n    status = Column(String, nullable=False)  # 'success', 'error'\n    fields_processed = Column(Integer, default=0)\n    error_message = Column(String, nullable=True)\n    created_at = Column(DateTime, default=datetime.utcnow)\n    processing_time_ms = Column(Integer, default=0)  # Processing time in milliseconds\n```\n\nField mapper implementation:\n```python\nfrom typing import Dict, Any, List, Optional, Tuple\nimport re\nfrom bs4 import BeautifulSoup\nfrom enum import Enum\n\nclass FieldType(Enum):\n    TEXT = \"text\"\n    WYSIWYG = \"wysiwyg\"\n    TEXTAREA = \"textarea\"\n    STRING = \"string\"\n    RELATION = \"relation\"\n    JSON = \"json\"\n\nclass DirectusTranslationPattern(Enum):\n    COLLECTION_TRANSLATIONS = \"collection_translations\"\n    LANGUAGE_COLLECTIONS = \"language_collections\"\n    CUSTOM = \"custom\"\n\nclass FieldMapper:\n    def __init__(self, db_session):\n        self.db_session = db_session\n    \n    async def get_field_config(self, client_id: str, collection_name: str) -> Dict[str, Any]:\n        # Get field paths from database\n        config = self.db_session.query(FieldConfig).filter_by(\n            client_id=client_id,\n            collection_name=collection_name\n        ).first()\n        \n        if not config:\n            return {\n                \"field_paths\": [],\n                \"is_translation_collection\": False,\n                \"field_types\": {},\n                \"rtl_field_mapping\": {},\n                \"directus_translation_pattern\": DirectusTranslationPattern.COLLECTION_TRANSLATIONS.value,\n                \"batch_processing\": False\n            }\n            \n        return {\n            \"field_paths\": config.field_paths,\n            \"is_translation_collection\": config.is_translation_collection,\n            \"primary_collection\": config.primary_collection,\n            \"field_types\": config.field_types or {},\n            \"rtl_field_mapping\": config.rtl_field_mapping or {},\n            \"directus_translation_pattern\": config.directus_translation_pattern or DirectusTranslationPattern.COLLECTION_TRANSLATIONS.value,\n            \"batch_processing\": config.batch_processing or False\n        }\n    \n    async def save_field_config(self, client_id: str, collection_name: str, \n                              field_config: Dict[str, Any]) -> None:\n        # Save field configuration to database\n        config = self.db_session.query(FieldConfig).filter_by(\n            client_id=client_id,\n            collection_name=collection_name\n        ).first()\n        \n        if config:\n            config.field_paths = field_config.get(\"field_paths\", [])\n            config.is_translation_collection = field_config.get(\"is_translation_collection\", False)\n            config.primary_collection = field_config.get(\"primary_collection\")\n            config.field_types = field_config.get(\"field_types\", {})\n            config.rtl_field_mapping = field_config.get(\"rtl_field_mapping\", {})\n            config.directus_translation_pattern = field_config.get(\"directus_translation_pattern\", DirectusTranslationPattern.COLLECTION_TRANSLATIONS.value)\n            config.batch_processing = field_config.get(\"batch_processing\", False)\n            config.updated_at = datetime.utcnow()\n        else:\n            config = FieldConfig(\n                client_id=client_id,\n                collection_name=collection_name,\n                field_paths=field_config.get(\"field_paths\", []),\n                is_translation_collection=field_config.get(\"is_translation_collection\", False),\n                primary_collection=field_config.get(\"primary_collection\"),\n                field_types=field_config.get(\"field_types\", {}),\n                rtl_field_mapping=field_config.get(\"rtl_field_mapping\", {}),\n                directus_translation_pattern=field_config.get(\"directus_translation_pattern\", DirectusTranslationPattern.COLLECTION_TRANSLATIONS.value),\n                batch_processing=field_config.get(\"batch_processing\", False)\n            )\n            self.db_session.add(config)\n        \n        self.db_session.commit()\n    \n    def extract_fields(self, content: Dict[str, Any], field_config: Dict[str, Any], language: str = None) -> Dict[str, Any]:\n        # Extract fields to translate based on paths\n        result = {}\n        field_paths = field_config[\"field_paths\"]\n        field_types = field_config.get(\"field_types\", {})\n        rtl_mapping = field_config.get(\"rtl_field_mapping\", {})\n        batch_processing = field_config.get(\"batch_processing\", False)\n        \n        # Check if we should use RTL-specific mapping\n        if language and language in rtl_mapping:\n            field_paths = rtl_mapping[language].get(\"field_paths\", field_paths)\n        \n        # Handle batch processing if enabled\n        if batch_processing:\n            return self._extract_fields_batch(content, field_paths, field_types, language)\n        \n        # Standard field extraction\n        for path in field_paths:\n            value = self._get_nested_value(content, path)\n            if value is not None:\n                field_type = field_types.get(path, self._detect_field_type(value))\n                result[path] = {\n                    \"value\": value,\n                    \"type\": field_type,\n                    \"metadata\": self._extract_metadata(value, field_type)\n                }\n        return result\n    \n    def _extract_fields_batch(self, content: Dict[str, Any], field_paths: List[str], \n                             field_types: Dict[str, str], language: str = None) -> Dict[str, Any]:\n        # Extract fields in batch for more efficient processing\n        result = {}\n        batch_text = []\n        batch_mapping = {}\n        \n        # First pass: collect all text for batch processing\n        for path in field_paths:\n            value = self._get_nested_value(content, path)\n            if value is not None:\n                field_type = field_types.get(path, self._detect_field_type(value))\n                \n                if field_type in [FieldType.TEXT.value, FieldType.STRING.value, FieldType.TEXTAREA.value]:\n                    # Add to batch for text fields\n                    batch_index = len(batch_text)\n                    batch_text.append(value)\n                    batch_mapping[path] = {\n                        \"index\": batch_index,\n                        \"type\": field_type\n                    }\n                else:\n                    # Process non-text fields individually\n                    result[path] = {\n                        \"value\": value,\n                        \"type\": field_type,\n                        \"metadata\": self._extract_metadata(value, field_type)\n                    }\n        \n        # Add batch text collection to result\n        if batch_text:\n            result[\"__batch__\"] = {\n                \"text\": batch_text,\n                \"mapping\": batch_mapping\n            }\n            \n        return result\n    \n    def _detect_field_type(self, value: Any) -> str:\n        # Auto-detect field type based on content\n        if isinstance(value, str):\n            if self.is_html(value):\n                return FieldType.WYSIWYG.value\n            elif \"\\n\" in value:\n                return FieldType.TEXTAREA.value\n            else:\n                return FieldType.TEXT.value\n        elif isinstance(value, dict):\n            return FieldType.JSON.value\n        else:\n            return FieldType.STRING.value\n    \n    def _extract_metadata(self, value: Any, field_type: str) -> Dict[str, Any]:\n        # Extract metadata to preserve during translation\n        metadata = {}\n        \n        if field_type == FieldType.WYSIWYG.value and isinstance(value, str):\n            metadata[\"html_structure\"] = self._extract_html_structure(value)\n        \n        return metadata\n    \n    def _extract_html_structure(self, html: str) -> Dict[str, Any]:\n        # Extract HTML structure for preservation\n        soup = BeautifulSoup(html, 'html.parser')\n        return {\n            \"tags\": [tag.name for tag in soup.find_all()],\n            \"classes\": [cls for tag in soup.find_all() for cls in tag.get(\"class\", [])],\n            \"attributes\": {tag.name: [attr for attr in tag.attrs if attr != \"class\"] \n                          for tag in soup.find_all() if tag.attrs}\n        }\n    \n    def _get_nested_value(self, data: Dict[str, Any], path: str) -> Any:\n        # Get value from nested dictionary using dot notation and array indices\n        if not data or not path:\n            return None\n            \n        parts = re.findall(r'([^\\[\\]\\.]+)|\\[(\\d+)\\]', path)\n        current = data\n        \n        for part in parts:\n            key, index = part\n            \n            if key and isinstance(current, dict):\n                if key not in current:\n                    return None\n                current = current[key]\n            elif index and isinstance(current, list):\n                idx = int(index)\n                if idx >= len(current):\n                    return None\n                current = current[idx]\n            else:\n                return None\n                \n        return current\n    \n    def is_html(self, text: str) -> bool:\n        # Check if content is HTML\n        return bool(re.search(r'<[^>]+>', text))\n    \n    def extract_text_from_html(self, html: str) -> List[Dict[str, Any]]:\n        # Extract text nodes from HTML for translation\n        soup = BeautifulSoup(html, 'html.parser')\n        text_nodes = []\n        \n        for element in soup.find_all(text=True):\n            if element.strip():\n                text_nodes.append({\n                    'text': element.strip(),\n                    'path': self._get_element_path(element),\n                    'parent_tag': element.parent.name if element.parent else None,\n                    'parent_attrs': element.parent.attrs if element.parent else {}\n                })\n        \n        return text_nodes\n    \n    def _get_element_path(self, element) -> str:\n        # Generate a path to the element for reassembly\n        path = []\n        parent = element.parent\n        while parent:\n            siblings = parent.find_all(parent.name, recursive=False)\n            if len(siblings) > 1:\n                index = siblings.index(parent)\n                path.append(f\"{parent.name}[{index}]\")\n            else:\n                path.append(parent.name)\n            parent = parent.parent\n        return \".\" + \".\".join(reversed(path))\n        \n    def reassemble_html(self, original_html: str, translated_nodes: List[Dict[str, Any]]) -> str:\n        # Reassemble HTML with translated text nodes\n        soup = BeautifulSoup(original_html, 'html.parser')\n        \n        for node in translated_nodes:\n            path = node['path']\n            translated_text = node['translated_text']\n            \n            # Find the element using the path and update it\n            # Implementation depends on how paths are structured\n            # This is a simplified version\n            elements = soup.select(path)\n            if elements:\n                elements[0].string = translated_text\n                \n        return str(soup)\n    \n    def process_ai_structured_data(self, structured_data: Dict[str, Any], field_config: Dict[str, Any]) -> Dict[str, Any]:\n        # Process structured data from AI providers\n        result = {}\n        \n        # Handle different AI provider formats\n        if \"translations\" in structured_data:\n            # Format: {\"translations\": [{\"text\": \"...\", \"detected_language\": \"...\", \"to\": \"...\"}]}\n            translations = structured_data.get(\"translations\", [])\n            for i, translation in enumerate(translations):\n                if i < len(field_config.get(\"field_paths\", [])):\n                    path = field_config[\"field_paths\"][i]\n                    result[path] = {\n                        \"value\": translation.get(\"text\", \"\"),\n                        \"type\": field_config.get(\"field_types\", {}).get(path, FieldType.TEXT.value),\n                        \"metadata\": {\n                            \"detected_language\": translation.get(\"detected_language\"),\n                            \"target_language\": translation.get(\"to\")\n                        }\n                    }\n        elif \"choices\" in structured_data:\n            # Format used by some AI providers with choices array\n            choices = structured_data.get(\"choices\", [])\n            if choices and \"message\" in choices[0]:\n                content = choices[0].get(\"message\", {}).get(\"content\", \"\")\n                # Try to parse as JSON if it looks like JSON\n                if content.strip().startswith(\"{\") and content.strip().endswith(\"}\"):\n                    try:\n                        parsed = json.loads(content)\n                        for path in field_config.get(\"field_paths\", []):\n                            if path in parsed:\n                                result[path] = {\n                                    \"value\": parsed[path],\n                                    \"type\": field_config.get(\"field_types\", {}).get(path, FieldType.TEXT.value),\n                                    \"metadata\": {}\n                                }\n                    except json.JSONDecodeError:\n                        # Not valid JSON, treat as single text\n                        if field_config.get(\"field_paths\"):\n                            result[field_config[\"field_paths\"][0]] = {\n                                \"value\": content,\n                                \"type\": FieldType.TEXT.value,\n                                \"metadata\": {}\n                            }\n        \n        return result\n        \n    def handle_directus_relations(self, content: Dict[str, Any], field_config: Dict[str, Any]) -> Dict[str, Any]:\n        # Process Directus relation fields\n        result = {}\n        relation_fields = [path for path, type_info in field_config.get(\"field_types\", {}).items() \n                          if type_info == FieldType.RELATION.value]\n        \n        for path in relation_fields:\n            relation_data = self._get_nested_value(content, path)\n            if relation_data:\n                # Handle different relation types (o2m, m2o, m2m)\n                if isinstance(relation_data, list):\n                    # o2m or m2m relation\n                    result[path] = [item[\"id\"] for item in relation_data if \"id\" in item]\n                elif isinstance(relation_data, dict) and \"id\" in relation_data:\n                    # m2o relation\n                    result[path] = relation_data[\"id\"]\n                    \n        return result\n    \n    def handle_directus_translations(self, content: Dict[str, Any], field_config: Dict[str, Any], \n                                    language: str) -> Dict[str, Any]:\n        # Handle Directus translation patterns\n        translation_pattern = field_config.get(\"directus_translation_pattern\", \n                                             DirectusTranslationPattern.COLLECTION_TRANSLATIONS.value)\n        \n        if translation_pattern == DirectusTranslationPattern.COLLECTION_TRANSLATIONS.value:\n            # Standard Directus pattern: collection_translations table with languages\n            return self._handle_collection_translations(content, field_config, language)\n        elif translation_pattern == DirectusTranslationPattern.LANGUAGE_COLLECTIONS.value:\n            # Language-specific collections pattern\n            return self._handle_language_collections(content, field_config, language)\n        else:\n            # Custom pattern, use regular field extraction\n            return self.extract_fields(content, field_config, language)\n    \n    def _handle_collection_translations(self, content: Dict[str, Any], field_config: Dict[str, Any], \n                                      language: str) -> Dict[str, Any]:\n        # Handle standard Directus translation pattern with collection_translations\n        result = {}\n        primary_collection = field_config.get(\"primary_collection\")\n        \n        if not primary_collection or not content.get(\"id\"):\n            return result\n            \n        # Structure for collection_translations\n        result = {\n            \"id\": None,  # Will be auto-generated or updated if exists\n            primary_collection + \"_id\": content.get(\"id\"),\n            \"languages_code\": language,\n        }\n        \n        # Add translatable fields\n        extracted = self.extract_fields(content, field_config, language)\n        for path, field_data in extracted.items():\n            if \"__batch__\" not in path:  # Skip batch metadata\n                field_name = path.split(\".\")[-1]  # Get the field name without path\n                result[field_name] = field_data.get(\"value\")\n                \n        return result\n    \n    def _handle_language_collections(self, content: Dict[str, Any], field_config: Dict[str, Any], \n                                   language: str) -> Dict[str, Any]:\n        # Handle language-specific collections pattern\n        result = {}\n        primary_collection = field_config.get(\"primary_collection\")\n        \n        if not primary_collection or not content.get(\"id\"):\n            return result\n            \n        # Structure for language collections (e.g., articles_en, articles_fr)\n        result = {\n            \"id\": content.get(\"id\"),  # Same ID as primary content\n        }\n        \n        # Add translatable fields\n        extracted = self.extract_fields(content, field_config, language)\n        for path, field_data in extracted.items():\n            if \"__batch__\" not in path:  # Skip batch metadata\n                field_name = path.split(\".\")[-1]  # Get the field name without path\n                result[field_name] = field_data.get(\"value\")\n                \n        return result\n        \n    def sanitize_content(self, content: Dict[str, Any], field_config: Dict[str, Any]) -> Dict[str, Any]:\n        # Sanitize content before processing\n        sanitized = {}\n        \n        for path, field_data in content.items():\n            if path == \"__batch__\":\n                # Handle batch data\n                batch_text = field_data.get(\"text\", [])\n                batch_mapping = field_data.get(\"mapping\", {})\n                sanitized_batch = []\n                \n                for text in batch_text:\n                    if isinstance(text, str):\n                        if self.is_html(text):\n                            # Sanitize HTML content\n                            soup = BeautifulSoup(text, 'html.parser')\n                            for script in soup([\"script\", \"style\"]):\n                                script.decompose()\n                            sanitized_batch.append(str(soup))\n                        else:\n                            sanitized_batch.append(text)\n                    else:\n                        sanitized_batch.append(text)\n                        \n                sanitized[\"__batch__\"] = {\n                    \"text\": sanitized_batch,\n                    \"mapping\": batch_mapping\n                }\n            else:\n                value = field_data.get(\"value\")\n                field_type = field_data.get(\"type\")\n                \n                if field_type == FieldType.WYSIWYG.value and isinstance(value, str):\n                    # Sanitize HTML content\n                    soup = BeautifulSoup(value, 'html.parser')\n                    # Remove potentially dangerous tags/attributes\n                    for script in soup([\"script\", \"style\"]):\n                        script.decompose()\n                    sanitized[path] = {\n                        \"value\": str(soup),\n                        \"type\": field_type,\n                        \"metadata\": field_data.get(\"metadata\", {})\n                    }\n                else:\n                    sanitized[path] = field_data\n                    \n        return sanitized\n```",
      "testStrategy": "1. Unit test field extraction from nested objects\n2. Test HTML detection and processing\n3. Verify field configuration storage and retrieval\n4. Test JSON path parsing with various path formats\n5. Validate HTML content extraction and reassembly\n6. Test with different content structures\n7. Verify handling of missing fields\n8. Test Directus collection structure support\n9. Verify translation collection handling\n10. Test field type detection for various content types\n11. Validate RTL language field mapping\n12. Test content sanitization and validation\n13. Verify relation field handling\n14. Test metadata preservation during translation process\n15. Validate custom field transformations\n16. Test with real Directus collection examples\n17. Verify HTML structure preservation during translation\n18. Test standard Directus translation structure (collection_translations)\n19. Verify language collections handling in Directus\n20. Test structured data processing from AI providers\n21. Validate batch operations for multiple collection fields\n22. Test performance of batch vs. individual field processing\n23. Verify correct handling of different Directus translation patterns\n24. Test with complex nested Directus structures\n25. Test API endpoints for field mapping configuration\n26. Verify caching system for field configurations\n27. Test operation logging functionality\n28. Validate support for OpenAI, Anthropic, Mistral, and DeepSeek response formats\n29. Test database initialization scripts\n30. Verify integration with existing translation services\n31. End-to-end testing of the complete translation workflow\n32. Test structured translation API endpoints\n33. Verify preview and validation endpoints functionality\n34. Test IntegratedTranslationService with various content types\n35. Validate client-specified provider selection\n36. Test batch translation support for cost efficiency\n37. Test Directus webhook endpoints for automatic content processing\n38. Verify HMAC signature verification for webhook security\n39. Test webhook integration with different Directus translation patterns\n40. Validate webhook error handling and infinite loop prevention",
      "subtasks": [
        {
          "id": 4.1,
          "title": "Test database initialization",
          "description": "✅ COMPLETED: Database initialization testing successful!\n\n## Test Results:\n- ✅ Database tables already exist and are accessible\n- ✅ Field mapping API endpoints working perfectly  \n- ✅ Field configuration save/retrieve operations working\n- ✅ Field extraction with batch processing working\n- ✅ HTML content detection and processing working\n- ✅ Processing logs being created properly\n\n## API Tests Performed:\n1. **Created field configuration**: Successfully created config for \"test_client/articles\" with 3 field paths\n2. **Retrieved configuration**: Successfully retrieved saved configuration  \n3. **Field extraction**: Successfully extracted fields from sample content with:\n   - Batch processing for text fields (title, description)\n   - Individual processing for HTML content (content.text) \n   - Proper field type detection (text, textarea, wysiwyg)\n   - Processing time: 21ms\n\n## Database Status:\n- Tables: field_configs (3 records), field_processing_logs (8 records)\n- Connection: Working from Docker app container\n- Field mapping fully operational\n\nThe database initialization is complete and functional. Ready for Subtask 4.2: Integration with translation services.",
          "status": "completed",
          "priority": "high"
        },
        {
          "id": 4.2,
          "title": "Integrate with translation services",
          "description": "✅ COMPLETED: Successfully integrated the field mapping system with flexible AI translation providers, creating a complete end-to-end translation workflow that follows the current architecture approach.\n\n## ✅ Components Completed in Subtask 4.2\n\n### 1. IntegratedTranslationService Implementation\n- **Core service** combining FieldMapper + FlexibleTranslationService\n- **Client-specified providers** - Correctly uses flexible selection (NOT cascading fallback)\n- **Structured content processing** with intelligent field extraction\n- **Batch translation support** for cost efficiency\n- **HTML structure preservation** during translation\n- **Directus translation patterns** (collection_translations, language_collections)\n\n### 2. New API Endpoints Added (to `/api/translation.py`)\n- `POST /api/v1/translate/structured` - Complete structured content translation\n- `POST /api/v1/translate/preview` - Preview extractable fields without translation cost\n- `POST /api/v1/translate/validate` - Validate translation requests before processing\n\n### 3. Complete Translation Workflow Pipeline\n1. **Field Configuration** → Retrieved from database per client/collection\n2. **Field Extraction** → Smart extraction using FieldMapper with type detection\n3. **Content Sanitization** → Security cleaning of HTML/dangerous content\n4. **AI Translation** → Uses FlexibleTranslationService with client's provider choice\n5. **Content Reconstruction** → Rebuilds content with translations\n6. **Directus Patterns** → Applies appropriate collection structure format\n\n## ✅ Live Testing Results\n\n**Integration Testing**: ✅ All Systems Working\n- Field config retrieval: 3 field paths configured ✅\n- Field extraction: 2 extractable fields detected ✅\n- API endpoint: HTTP 200 successful response ✅\n\n**API Response Example**: Field extraction correctly identifying:\n- `title` (text type, batch processing enabled)\n- `description` (textarea type, batch processing enabled)  \n- `content.text` (wysiwyg type, HTML structure preserved)\n\n## 🔧 Architecture Alignment Confirmed\n\n**Successfully implemented current project approach**:\n- ✅ **Flexible provider selection** instead of cascading fallback\n- ✅ **Client-provided API keys** per request (never stored)\n- ✅ **Field mapping configuration** per client/collection from database\n- ✅ **Cost-aware optimization** through existing Redis caching system\n- ✅ **Directus CMS native integration** patterns\n\n## 📁 New Files Created\n\n- `app/services/integrated_translation_service.py` - Main integration orchestrator (473 lines)\n- Enhanced `app/api/translation.py` - Added structured translation endpoints (+150 lines)\n- Docker environment testing confirmed working\n\nCore translation functionality is now operational and ready for Directus webhook integration!",
          "status": "completed",
          "priority": "high"
        },
        {
          "id": 4.3,
          "title": "Add Directus webhook endpoints",
          "description": "✅ COMPLETED: Successfully implemented comprehensive Directus webhook system with the following components:\n\n### 1. Complete Webhook API (`app/api/webhooks.py`)\n- **Main translation webhook** at `/api/v1/webhooks/directus/translate`\n- **Configuration validation** endpoint for pre-setup testing\n- **Testing endpoint** with dry-run support for safe testing\n- **Information endpoint** with complete setup documentation\n- **Health monitoring** endpoint for webhook service status\n\n### 2. Production-Ready Security\n- **HMAC signature verification** (SHA-256/SHA-1 support)\n- **Comprehensive input validation** and sanitization\n- **API key validation** integration \n- **Infinite loop prevention** for translation collections\n- **Error handling** with detailed metadata\n\n### 3. Directus Integration Patterns\n- **Standard collection_translations** pattern (recommended)\n- **Language-specific collections** pattern (articles_ar, articles_bs)\n- **Custom translation patterns** for flexible workflows\n- **Complete automation** from Directus → LocPlat → Translation → Storage\n\n### 4. Testing & Validation\n- ✅ **All webhook tests passing** in Docker environment\n- ✅ **Request/response validation** working correctly\n- ✅ **Security signature verification** tested and confirmed\n- ✅ **FastAPI integration** - all routes accessible\n- ✅ **Field mapping integration** confirmed working\n\n### 5. Documentation Created\n- **Complete webhook integration guide** with Directus Flow examples\n- **Security best practices** and production configuration\n- **Testing procedures** and troubleshooting guidance\n- **API documentation** with request/response examples\n\nThe webhook system is now fully operational and ready for production Directus integration!",
          "status": "completed",
          "priority": "medium"
        },
        {
          "id": 4.4,
          "title": "Implement Redis caching layer",
          "description": "Connect with Redis caching system from Task #3 to improve performance of field mapping operations. This implementation should include:\n\n1. Integration with the existing Redis caching layer for field configurations\n2. Cache field mapping configurations with appropriate TTL settings\n3. Implement cache invalidation strategies for configuration updates\n4. Add cache hit/miss metrics for monitoring\n5. Optimize batch processing with Redis pipeline operations\n6. Ensure thread-safe cache operations\n7. Implement fallback mechanisms for Redis connection failures\n8. Add configuration options for cache behavior\n9. Document Redis integration patterns for field mapping",
          "status": "to-do",
          "priority": "medium",
          "dependencies": []
        },
        {
          "id": 4.5,
          "title": "End-to-end testing",
          "description": "Perform comprehensive end-to-end testing of the complete translation workflow with field mapping. Testing should cover:\n\n1. Complete workflow from content submission to translated content storage\n2. Directus webhook integration with various collection patterns\n3. Field extraction and mapping for different content types\n4. Translation service integration with multiple AI providers\n5. Error handling and recovery mechanisms\n6. Performance testing with large content volumes\n7. Security testing of webhook endpoints\n8. Redis caching integration testing\n9. Validation of HTML structure preservation\n10. Testing of RTL language support\n11. Batch processing optimization verification\n12. Documentation of test results and performance metrics",
          "status": "to-do",
          "priority": "high"
        }
      ]
    },
    {
      "id": 5,
      "title": "Implement Translation API Endpoints",
      "description": "Create the main translation API endpoints including the core translation functionality, provider management, language pair listing, field configuration, and status monitoring.",
      "status": "pending",
      "dependencies": [
        2,
        3,
        4
      ],
      "priority": "high",
      "details": "1. Implement POST /translate endpoint\n2. Create GET /languages endpoint\n3. Add POST /fields/config endpoint\n4. Implement GET /providers and POST /providers endpoints for provider management\n5. Add GET /status endpoint for service monitoring\n6. Integrate all components (AI providers, cache, field mapping)\n7. Implement request/response models with structured response support\n\nAPI implementation:\n```python\nfrom fastapi import FastAPI, Depends, HTTPException, Body, Header\nfrom pydantic import BaseModel, Field\nfrom typing import Dict, List, Any, Optional, Union\n\napp = FastAPI(title=\"LocPlat Translation Service\")\n\nclass TranslationRequest(BaseModel):\n    content: Dict[str, Any]\n    target_language: str\n    source_language: str = \"en\"\n    collection_name: Optional[str] = None\n    client_id: Optional[str] = None\n    provider: Optional[str] = None  # Allow specifying preferred provider\n    structured_response: bool = False  # Support structured responses like directus-translator\n\nclass TranslationResponse(BaseModel):\n    translated_content: Dict[str, Any]\n    source_language: str\n    target_language: str\n    provider_used: str\n    cache_hit: bool\n\nclass FieldConfigRequest(BaseModel):\n    client_id: str\n    collection_name: str\n    field_paths: List[str]\n\nclass LanguagePair(BaseModel):\n    source: str\n    target: str\n    name: str\n\nclass ProviderConfig(BaseModel):\n    name: str  # e.g., \"openai\", \"anthropic\", \"mistral\", \"deepseek\"\n    api_key: str\n    priority: int = 1  # Lower number = higher priority\n    enabled: bool = True\n    model: Optional[str] = None  # Specific model to use\n\nclass ProviderInfo(BaseModel):\n    name: str\n    enabled: bool\n    priority: int\n    model: Optional[str] = None\n\nclass ServiceStatus(BaseModel):\n    status: str\n    providers: Dict[str, bool]  # Provider name -> availability status\n    cache_size: int\n    uptime: float\n\n# Middleware to handle API key authentication\n@app.middleware(\"http\")\nasync def authenticate_client(request, call_next):\n    client_api_key = request.headers.get(\"X-API-Key\")\n    if not client_api_key:\n        return JSONResponse(status_code=401, content={\"detail\": \"API key required\"})\n    \n    # Validate API key against database\n    if not await is_valid_api_key(client_api_key):\n        return JSONResponse(status_code=403, content={\"detail\": \"Invalid API key\"})\n    \n    # Add client info to request state\n    request.state.client_id = await get_client_id_from_key(client_api_key)\n    \n    response = await call_next(request)\n    return response\n\n@app.post(\"/translate\", response_model=TranslationResponse)\nasync def translate_content(\n    request: TranslationRequest,\n    translation_service = Depends(get_translation_service),\n    x_api_key: str = Header(..., alias=\"X-API-Key\")\n):\n    try:\n        result = await translation_service.translate_content(\n            content=request.content,\n            source_lang=request.source_language,\n            target_lang=request.target_language,\n            provider=request.provider,\n            structured_response=request.structured_response,\n            collection_name=request.collection_name,\n            client_id=request.client_id or request.state.client_id\n        )\n        return result\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/languages\", response_model=List[LanguagePair])\nasync def get_languages():\n    return [\n        {\"source\": \"en\", \"target\": \"ar\", \"name\": \"English to Arabic\"},\n        {\"source\": \"en\", \"target\": \"bs\", \"name\": \"English to Bosnian\"},\n        # Add more language pairs\n    ]\n\n@app.post(\"/fields/config\")\nasync def configure_fields(\n    config: FieldConfigRequest,\n    field_mapper = Depends(get_field_mapper)\n):\n    await field_mapper.save_field_config(\n        client_id=config.client_id,\n        collection_name=config.collection_name,\n        field_paths=config.field_paths\n    )\n    return {\"status\": \"success\"}\n\n@app.get(\"/providers\", response_model=List[ProviderInfo])\nasync def get_providers(\n    provider_manager = Depends(get_provider_manager)\n):\n    return await provider_manager.list_providers()\n\n@app.post(\"/providers\")\nasync def configure_provider(\n    config: ProviderConfig,\n    provider_manager = Depends(get_provider_manager)\n):\n    await provider_manager.configure_provider(\n        name=config.name,\n        api_key=config.api_key,\n        priority=config.priority,\n        enabled=config.enabled,\n        model=config.model\n    )\n    return {\"status\": \"success\"}\n\n@app.get(\"/status\", response_model=ServiceStatus)\nasync def get_service_status(\n    service_monitor = Depends(get_service_monitor)\n):\n    return await service_monitor.get_status()\n```\n\nTranslation service implementation:\n```python\nclass TranslationService:\n    def __init__(self, provider_manager, cache, field_mapper):\n        self.provider_manager = provider_manager\n        self.cache = cache\n        self.field_mapper = field_mapper\n    \n    async def translate_content(\n        self, content, source_lang, target_lang, provider=None,\n        structured_response=False, collection_name=None, client_id=None\n    ):\n        # Get field paths if collection_name and client_id provided\n        field_paths = []\n        if collection_name and client_id:\n            field_paths = await self.field_mapper.get_field_config(client_id, collection_name)\n        \n        # If no specific fields configured, translate all string fields\n        if not field_paths:\n            field_paths = self._find_string_fields(content)\n        \n        # Extract fields to translate\n        fields_to_translate = self.field_mapper.extract_fields(content, field_paths)\n        \n        # Create result copy\n        result = content.copy()\n        cache_hit = False\n        provider_used = None\n        \n        # Process each field\n        for path, value in fields_to_translate.items():\n            if not isinstance(value, str):\n                continue\n                \n            # Check cache\n            cached = await self.cache.get_cached_translation(value, source_lang, target_lang)\n            if cached:\n                translated = cached\n                cache_hit = True\n            else:\n                # Get appropriate provider based on preference or availability\n                translation_provider = await self.provider_manager.get_provider(provider)\n                \n                if structured_response:\n                    # Use structured response format (like directus-translator)\n                    translated = await translation_provider.translate_structured(\n                        value, source_lang, target_lang\n                    )\n                else:\n                    # Standard translation\n                    translated = await translation_provider.translate(\n                        value, source_lang, target_lang\n                    )\n                \n                # Cache result\n                await self.cache.cache_translation(value, source_lang, target_lang, translated)\n                \n                # Record which provider was used\n                provider_used = translation_provider.name\n            \n            # Update result\n            self._set_nested_value(result, path, translated)\n        \n        return {\n            \"translated_content\": result,\n            \"source_language\": source_lang,\n            \"target_language\": target_lang,\n            \"provider_used\": provider_used or \"cache\",\n            \"cache_hit\": cache_hit\n        }\n    \n    def _find_string_fields(self, content, prefix=\"\"):\n        # Recursively find all string fields in the content\n        fields = []\n        if isinstance(content, dict):\n            for key, value in content.items():\n                path = f\"{prefix}.{key}\" if prefix else key\n                if isinstance(value, str):\n                    fields.append(path)\n                elif isinstance(value, dict) or isinstance(value, list):\n                    fields.extend(self._find_string_fields(value, path))\n        elif isinstance(content, list):\n            for i, item in enumerate(content):\n                path = f\"{prefix}[{i}]\"\n                fields.extend(self._find_string_fields(item, path))\n        return fields\n    \n    def _set_nested_value(self, data, path, value):\n        # Set value in nested dictionary using dot notation\n        parts = path.split('.')\n        current = data\n        for i, part in enumerate(parts):\n            if i == len(parts) - 1:\n                current[part] = value\n            else:\n                if part not in current:\n                    current[part] = {}\n                current = current[part]\n\nclass ProviderManager:\n    def __init__(self, db_client):\n        self.db_client = db_client\n        self.providers = {}\n        self.provider_classes = {\n            \"openai\": OpenAIProvider,\n            \"anthropic\": AnthropicProvider,\n            \"mistral\": MistralProvider,\n            \"deepseek\": DeepSeekProvider\n        }\n    \n    async def initialize(self):\n        # Load provider configurations from database\n        configs = await self.db_client.get_provider_configs()\n        for config in configs:\n            if config[\"name\"] in self.provider_classes and config[\"enabled\"]:\n                provider_class = self.provider_classes[config[\"name\"]]\n                self.providers[config[\"name\"]] = provider_class(\n                    api_key=config[\"api_key\"],\n                    model=config.get(\"model\")\n                )\n    \n    async def get_provider(self, preferred=None):\n        # Return preferred provider if specified and available\n        if preferred and preferred in self.providers:\n            return self.providers[preferred]\n        \n        # Otherwise return highest priority available provider\n        configs = await self.db_client.get_provider_configs()\n        configs.sort(key=lambda x: x[\"priority\"])\n        \n        for config in configs:\n            if config[\"name\"] in self.providers and config[\"enabled\"]:\n                return self.providers[config[\"name\"]]\n        \n        raise Exception(\"No translation providers available\")\n    \n    async def list_providers(self):\n        configs = await self.db_client.get_provider_configs()\n        return [{\n            \"name\": config[\"name\"],\n            \"enabled\": config[\"enabled\"],\n            \"priority\": config[\"priority\"],\n            \"model\": config.get(\"model\")\n        } for config in configs]\n    \n    async def configure_provider(self, name, api_key, priority=1, enabled=True, model=None):\n        if name not in self.provider_classes:\n            raise ValueError(f\"Unknown provider: {name}\")\n        \n        # Save configuration to database\n        await self.db_client.save_provider_config({\n            \"name\": name,\n            \"api_key\": api_key,\n            \"priority\": priority,\n            \"enabled\": enabled,\n            \"model\": model\n        })\n        \n        # Update in-memory provider if enabled\n        if enabled:\n            provider_class = self.provider_classes[name]\n            self.providers[name] = provider_class(\n                api_key=api_key,\n                model=model\n            )\n        elif name in self.providers:\n            del self.providers[name]\n```",
      "testStrategy": "1. Test POST /translate with various content structures\n2. Verify correct handling of client API keys\n3. Test structured response format compatibility with directus-translator\n4. Test with all supported AI providers (OpenAI, Anthropic, Mistral, DeepSeek)\n5. Test provider fallback behavior when preferred provider is unavailable\n6. Test caching behavior and hit rate\n7. Validate field mapping functionality\n8. Verify language pair endpoint returns correct data\n9. Test field configuration endpoint\n10. Test provider management endpoints (GET/POST /providers)\n11. Verify status monitoring endpoint\n12. Perform integration tests with all components\n13. Benchmark API response times with different providers\n14. Test with actual Directus content structures\n15. Verify proper error handling when providers fail",
      "subtasks": []
    },
    {
      "id": 6,
      "title": "Implement Directus Integration",
      "description": "Create specialized endpoints and functionality for Directus CMS integration, including batch translation support, webhook-based auto-translation, SDK integration, schema introspection, and Directus-specific data formatting.",
      "status": "pending",
      "dependencies": [
        5
      ],
      "priority": "medium",
      "details": "1. Create Directus-specific data models\n2. Implement batch translation for collections\n3. Add Directus field format handling\n4. Create specialized endpoints for Directus\n5. Implement webhook support for auto-translation\n6. Integrate with Directus SDK\n7. Add collection schema introspection\n8. Mirror directus-translator functionality for seamless workflow integration\n\nDirectus models:\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import Dict, List, Any, Optional\n\nclass DirectusItem(BaseModel):\n    id: str\n    collection: str\n    item: Dict[str, Any]\n\nclass DirectusBatchRequest(BaseModel):\n    items: List[DirectusItem]\n    target_language: str\n    openai_key: str\n    google_key: str\n    source_language: str = \"en\"\n    client_id: Optional[str] = None\n\nclass DirectusBatchResponse(BaseModel):\n    items: List[Dict[str, Any]]\n    stats: Dict[str, Any]\n\nclass DirectusWebhookPayload(BaseModel):\n    event: str  # create, update, delete\n    collection: str\n    item: Dict[str, Any]\n    target_languages: List[str]\n    client_id: str\n    openai_key: Optional[str] = None\n    google_key: Optional[str] = None\n\nclass DirectusSchemaRequest(BaseModel):\n    collection: str\n    client_id: str\n```\n\nDirectus integration implementation:\n```python\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom directus_sdk import Directus\n\ndirectus_router = APIRouter(prefix=\"/directus\", tags=[\"directus\"])\n\n@directus_router.post(\"/batch\", response_model=DirectusBatchResponse)\nasync def translate_directus_batch(\n    request: DirectusBatchRequest,\n    translation_service = Depends(get_translation_service)\n):\n    results = []\n    stats = {\n        \"total\": len(request.items),\n        \"successful\": 0,\n        \"failed\": 0,\n        \"cache_hits\": 0\n    }\n    \n    for item in request.items:\n        try:\n            # Get field config for this collection\n            field_paths = []\n            if request.client_id:\n                field_paths = await translation_service.field_mapper.get_field_config(\n                    request.client_id, item.collection\n                )\n            \n            # Translate the item\n            result = await translation_service.translate_content(\n                content=item.item,\n                source_lang=request.source_language,\n                target_lang=request.target_language,\n                openai_key=request.openai_key,\n                google_key=request.google_key,\n                collection_name=item.collection,\n                client_id=request.client_id\n            )\n            \n            # Add to results\n            results.append({\n                \"id\": item.id,\n                \"collection\": item.collection,\n                \"item\": result[\"translated_content\"]\n            })\n            \n            # Update stats\n            stats[\"successful\"] += 1\n            if result[\"cache_hit\"]:\n                stats[\"cache_hits\"] += 1\n                \n        except Exception as e:\n            stats[\"failed\"] += 1\n            # Log the error\n            print(f\"Error translating item {item.id}: {str(e)}\")\n    \n    return {\n        \"items\": results,\n        \"stats\": stats\n    }\n\n@directus_router.post(\"/collection/{collection_name}\")\nasync def configure_directus_collection(\n    collection_name: str,\n    config: Dict[str, Any],\n    field_mapper = Depends(get_field_mapper)\n):\n    # Configure which fields to translate for a Directus collection\n    client_id = config.get(\"client_id\")\n    field_paths = config.get(\"field_paths\", [])\n    \n    if not client_id:\n        raise HTTPException(status_code=400, detail=\"client_id is required\")\n    \n    await field_mapper.save_field_config(\n        client_id=client_id,\n        collection_name=collection_name,\n        field_paths=field_paths\n    )\n    \n    return {\"status\": \"success\"}\n\n@directus_router.post(\"/webhook\")\nasync def directus_webhook_handler(\n    payload: DirectusWebhookPayload,\n    translation_service = Depends(get_translation_service)\n):\n    \"\"\"Handle webhook events from Directus for auto-translation\"\"\"\n    if payload.event not in [\"create\", \"update\"]:\n        return {\"status\": \"ignored\", \"reason\": f\"Event {payload.event} not configured for translation\"}\n    \n    results = []\n    for target_lang in payload.target_languages:\n        try:\n            result = await translation_service.translate_content(\n                content=payload.item,\n                source_lang=\"en\",  # Assuming English is the source\n                target_lang=target_lang,\n                openai_key=payload.openai_key,\n                google_key=payload.google_key,\n                collection_name=payload.collection,\n                client_id=payload.client_id\n            )\n            results.append({\n                \"language\": target_lang,\n                \"status\": \"success\",\n                \"translated_item\": result[\"translated_content\"]\n            })\n        except Exception as e:\n            results.append({\n                \"language\": target_lang,\n                \"status\": \"error\",\n                \"error\": str(e)\n            })\n    \n    return {\"results\": results}\n\n@directus_router.post(\"/schema/introspect\")\nasync def introspect_collection_schema(\n    request: DirectusSchemaRequest,\n    directus_service = Depends(get_directus_service)\n):\n    \"\"\"Introspect a Directus collection schema to identify translatable fields\"\"\"\n    schema = await directus_service.get_collection_schema(\n        client_id=request.client_id,\n        collection=request.collection\n    )\n    \n    # Identify fields that are likely to contain translatable content\n    translatable_fields = []\n    for field in schema[\"fields\"]:\n        if field[\"type\"] in [\"string\", \"text\", \"json\"] or field.get(\"interface\") in [\"input\", \"input-rich-text-md\", \"input-rich-text-html\"]:\n            translatable_fields.append(field[\"field\"])\n    \n    return {\n        \"collection\": request.collection,\n        \"schema\": schema,\n        \"suggested_translatable_fields\": translatable_fields\n    }\n\n# Add the router to the main app\napp.include_router(directus_router)\n```\n\nDirectus helper functions:\n```python\ndef format_directus_response(translated_items, original_items):\n    \"\"\"Format the response in a way that's easy to use with Directus\"\"\"\n    result = []\n    \n    for orig, trans in zip(original_items, translated_items):\n        # Create a new item with the same structure as the original\n        # but with translated content\n        result.append({\n            \"id\": orig[\"id\"],\n            \"collection\": orig[\"collection\"],\n            \"item\": trans[\"translated_content\"]\n        })\n    \n    return result\n\nclass DirectusService:\n    \"\"\"Service for interacting with Directus CMS\"\"\"\n    \n    def __init__(self, db):\n        self.db = db\n        self.client_configs = {}\n    \n    async def get_directus_client(self, client_id):\n        \"\"\"Get or create a Directus SDK client for the given client_id\"\"\"\n        if client_id not in self.client_configs:\n            # Get client config from database\n            config = await self.db.fetch_one(\n                \"SELECT directus_url, directus_token FROM client_configs WHERE client_id = $1\",\n                client_id\n            )\n            \n            if not config:\n                raise ValueError(f\"No Directus configuration found for client {client_id}\")\n            \n            self.client_configs[client_id] = {\n                \"url\": config[\"directus_url\"],\n                \"token\": config[\"directus_token\"],\n                \"client\": Directus(config[\"directus_url\"], token=config[\"directus_token\"])\n            }\n        \n        return self.client_configs[client_id][\"client\"]\n    \n    async def get_collection_schema(self, client_id, collection):\n        \"\"\"Get the schema for a Directus collection\"\"\"\n        client = await self.get_directus_client(client_id)\n        return await client.collections.read_one(collection)\n    \n    async def update_translated_item(self, client_id, collection, item_id, translated_data, language):\n        \"\"\"Update an item with translated content in the specified language\"\"\"\n        client = await self.get_directus_client(client_id)\n        \n        # Assuming Directus uses a language field or has a translations relation\n        # This implementation will vary based on how translations are structured in Directus\n        if collection.endswith(\"_translations\"):\n            # If using a translations collection\n            await client.items(collection).update({\n                \"item_id\": item_id,\n                \"language\": language,\n                **translated_data\n            })\n        else:\n            # If using language-specific fields (e.g., title_fr, description_fr)\n            language_fields = {}\n            for key, value in translated_data.items():\n                language_fields[f\"{key}_{language}\"] = value\n            \n            await client.items(collection).update(item_id, language_fields)\n        \n        return {\"status\": \"success\"}\n```",
      "testStrategy": "1. Test batch translation endpoint with Directus collection data\n2. Verify correct handling of Directus-specific data structures\n3. Test collection configuration endpoint\n4. Validate field mapping works correctly with Directus fields\n5. Test performance with larger batches of items\n6. Verify error handling for partial batch failures\n7. Test with actual Directus API responses\n8. Validate statistics reporting\n9. Test with different collection types and field structures\n10. Verify integration with Directus webhook patterns\n11. Test webhook-based auto-translation functionality\n12. Verify schema introspection correctly identifies translatable fields\n13. Test Directus SDK integration for reading and updating content\n14. Validate the system works with both translations collections and language-specific fields\n15. Test end-to-end workflow from content creation to translation via webhooks\n16. Verify compatibility with directus-translator patterns for seamless migration",
      "subtasks": []
    },
    {
      "id": 7,
      "title": "Address CodeRabbit AI Review Feedback for Translation API",
      "description": "Fix critical security vulnerabilities, code quality issues, and performance optimizations identified in the AI translation provider integration based on CodeRabbit AI review feedback for PR #1.",
      "details": "1. Security Fixes:\n   - Implement proper exception chaining to prevent information leakage\n   - Add input sanitization for all user-provided data, especially API keys and translation content\n   - Configure CORS security headers properly for API endpoints\n   - Validate and sanitize all query parameters and request bodies\n   - Implement rate limiting for API endpoints\n\n2. Code Quality Improvements:\n   - Refactor error handling across provider integrations for consistency\n   - Add comprehensive logging with appropriate log levels\n   - Improve code documentation and type hints\n   - Standardize naming conventions across the codebase\n   - Remove redundant code and consolidate similar functions\n\n3. Performance Optimizations:\n   - Optimize API request patterns to external providers\n   - Improve Redis caching efficiency with better key strategies\n   - Implement connection pooling for external API calls\n   - Add request timeouts and circuit breakers for provider APIs\n   - Optimize batch processing for translation requests\n\n4. Architectural Improvements:\n   - Refactor provider integration to improve maintainability\n   - Enhance fallback mechanisms between providers\n   - Implement better separation of concerns in the translation pipeline\n   - Add metrics collection for performance monitoring\n   - Improve configuration management for provider-specific settings\n\n5. Production Readiness:\n   - Add comprehensive error reporting\n   - Implement graceful degradation when providers are unavailable\n   - Enhance request validation and response formatting\n   - Add health check endpoints with detailed status information\n   - Implement proper API versioning",
      "testStrategy": "1. Security Testing:\n   - Run automated security scanning tools (OWASP ZAP, Bandit) against the API\n   - Perform penetration testing focusing on input validation and authentication\n   - Test exception handling to ensure no sensitive information is leaked\n   - Verify CORS configuration with cross-domain requests\n   - Test rate limiting functionality\n\n2. Code Quality Verification:\n   - Run static code analysis tools (pylint, flake8, mypy)\n   - Perform code review to verify all identified issues are addressed\n   - Verify consistent error handling across all modules\n   - Check logging implementation for appropriate detail and levels\n   - Verify documentation completeness and accuracy\n\n3. Performance Testing:\n   - Benchmark API response times before and after changes\n   - Test Redis caching efficiency with repeated requests\n   - Measure performance under load with concurrent requests\n   - Verify timeout and circuit breaker functionality\n   - Test batch processing efficiency with various payload sizes\n\n4. Integration Testing:\n   - Verify all provider integrations still function correctly\n   - Test fallback mechanisms between providers\n   - Verify Directus integration functionality\n   - Test end-to-end translation workflows\n   - Verify metrics collection accuracy\n\n5. Production Readiness Verification:\n   - Deploy to staging environment and monitor behavior\n   - Test health check endpoints for accurate status reporting\n   - Verify graceful degradation scenarios\n   - Test API versioning compatibility\n   - Perform load testing to verify stability under production-like conditions",
      "status": "done",
      "dependencies": [
        2,
        3,
        4,
        5
      ],
      "priority": "high",
      "subtasks": [
        {
          "id": 1,
          "title": "Fix Exception Chaining Issues (Critical Security)",
          "description": "Fix all exception chaining issues across provider implementations to prevent information leakage and improve debugging. Add 'from e' to all exception raises where original exceptions are wrapped.",
          "details": "Files to fix:\n- app/services/openai_provider.py (lines 52-59, 88)\n- app/services/anthropic_provider.py (lines 46-53, 81)\n- app/services/mistral_provider.py (lines 70-73, 101)\n- app/services/deepseek_provider.py (lines 71-73, 91)\n- app/api/translation.py (lines 121-123, 160-162, 193, 211, 231)\n\nChange pattern from:\nraise ProviderError(self.name, f\"Error: {str(e)}\", e)\n\nTo:\nraise ProviderError(self.name, f\"Error: {str(e)}\", e) from e",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 7
        },
        {
          "id": 2,
          "title": "Implement Input Sanitization (Critical Security)",
          "description": "Implement input sanitization to prevent prompt injection attacks and token limit exceeded errors. Add length limits, control character stripping, and safe text handling.",
          "details": "Critical security vulnerability in app/services/translation_provider.py lines 166-184:\n\nCurrent unsafe code:\n```python\nprompt += f\"\\n\\nText to translate: {text}\"\n```\n\nImplement:\n1. MAX_CHARS = 2000 limit for text and context\n2. Strip control characters and potential injection strings\n3. Truncate user input before embedding in prompts\n4. Validate text content for safety\n\nAdd method:\n```python\ndef _sanitize_text(self, text: str, max_chars: int = 2000) -> str:\n    # Remove control chars, truncate, escape dangerous patterns\n    safe_text = text[:max_chars]\n    safe_text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', safe_text)\n    return safe_text.strip()\n```",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 7
        },
        {
          "id": 3,
          "title": "Fix CORS Security Configuration",
          "description": "Fix CORS security configuration and tighten production security settings. Replace wildcard origins with environment-specific allowed domains.",
          "details": "Security issue in app/main.py lines 31-35:\n\nCurrent unsafe configuration:\n```python\nallow_origins=[\"*\"]  # ❌ Too permissive\n```\n\nFix by:\n1. Create environment-specific CORS settings in app/config.py\n2. Add CORS_ALLOWED_ORIGINS to settings with default secure values\n3. Update main.py to use environment-controlled origins\n\nRecommended implementation:\n```python\n# In config.py\nCORS_ALLOWED_ORIGINS = [\n    \"http://localhost:3000\",  # Development\n    \"https://yourdomain.com\", # Production\n    \"https://admin.yourdomain.com\"\n]\n\n# In main.py\nadd_middleware(\n    CORSMiddleware,\n    allow_origins=settings.CORS_ALLOWED_ORIGINS,\n    allow_credentials=True,\n    allow_methods=[\"GET\", \"POST\"],\n    allow_headers=[\"*\"],\n)\n```",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 7
        },
        {
          "id": 4,
          "title": "Code Hygiene and Formatting Cleanup",
          "description": "Clean up code hygiene issues: remove trailing whitespace, fix unused imports, wrap long lines, and improve code formatting across all files.",
          "details": "Code hygiene issues identified by CodeRabbit:\n\n1. Trailing whitespace (28+ locations):\n   - app/services/openai_provider.py (lines 13, 20, 22-25, 33, 46, 51, 60, 62-65, 72, 78, 89, 104, 108)\n   - app/services/anthropic_provider.py (lines 13, 20, 22-25, 33, 40, 45, 54, 56-59, 66, 71, 82, 97, 101)\n   - app/services/mistral_provider.py (similar pattern)\n   - app/services/translation_provider.py (similar pattern)\n   - tests/test_translation_providers.py (similar pattern)\n\n2. Unused imports to remove:\n   - app/api/translation.py: Remove unused `Depends` and `LanguageDirection`\n   - app/services/flexible_translation_service.py: Remove `asyncio`, `Tuple`, `TranslationProvider`\n   - tests/test_translation_providers.py: Remove `asyncio`, `patch`, `TranslationError`, `ProviderError`\n\n3. Long lines to wrap (>100 chars):\n   - Multiple files have lines exceeding 100 character limit\n\nAutomated fix commands:\n```bash\n# Remove trailing whitespace\nfind . -name \"*.py\" -exec sed -i 's/[[:space:]]*$//' {} \\;\n\n# Use ruff to auto-fix imports\nruff check --fix app/ tests/\n```",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 7
        },
        {
          "id": 5,
          "title": "Performance Optimizations",
          "description": "Optimize performance by implementing concurrent quality assessment and fixing shallow copy issues in collection translation.",
          "details": "Performance optimizations needed:\n\n1. Sequential Quality Assessment Issue (app/services/flexible_translation_service.py:205-209):\n   Current inefficient code:\n   ```python\n   for i, (original, translated) in enumerate(zip(texts, translated_texts)):\n       quality_score = await translation_provider.assess_translation_quality(...)\n   ```\n\n   Fix with concurrent execution:\n   ```python\n   quality_tasks = [\n       translation_provider.assess_translation_quality(orig, trans, source_lang, target_lang)\n       for orig, trans in zip(texts, translated_texts)\n   ]\n   quality_scores = await asyncio.gather(*quality_tasks)\n   ```\n\n2. Shallow Copy Issue (app/services/flexible_translation_service.py:311):\n   Current unsafe code:\n   ```python\n   translated_data = collection_data.copy()  # ❌ Shallow copy\n   ```\n\n   Fix with deep copy:\n   ```python\n   import copy\n   translated_data = copy.deepcopy(collection_data)  # ✅ Deep copy\n   ```\n\n3. Make assess_translation_quality synchronous:\n   Remove unnecessary async from method that does no I/O operations",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 7
        },
        {
          "id": 6,
          "title": "Refactor Provider Architecture (Code Deduplication)",
          "description": "Refactor provider implementations to reduce code duplication by extracting common functionality into a base provider class.",
          "details": "Code duplication issue identified by CodeRabbit:\n\nCurrent problem: OpenAI, Anthropic, Mistral, and DeepSeek providers share significant duplicate code:\n- Same structure and error handling patterns\n- Identical batch_translate implementations\n- Similar validate_api_key methods\n- Repeated exception handling\n\nRecommended solution:\nCreate BaseAsyncProvider class with common functionality:\n\n```python\nclass BaseAsyncProvider(TranslationProvider):\n    \"\"\"Base class for async translation providers with common functionality.\"\"\"\n    \n    async def batch_translate(self, texts, source_lang, target_lang, api_key, context=None):\n        \"\"\"Common batch translation implementation.\"\"\"\n        if not texts:\n            return []\n        \n        tasks = [\n            self.translate(text, source_lang, target_lang, api_key, context)\n            for text in texts\n        ]\n        \n        try:\n            results = await asyncio.gather(*tasks, return_exceptions=True)\n            translations = []\n            for result in results:\n                if isinstance(result, Exception):\n                    raise result\n                translations.append(result)\n            return translations\n        except Exception as e:\n            raise ProviderError(self.name, f\"Batch translation failed: {str(e)}\", e) from e\n```\n\nThis would eliminate duplicate code across all providers and make maintenance easier.",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 7
        },
        {
          "id": 7,
          "title": "Improve Exception Handling Specificity",
          "description": "Improve exception handling specificity by replacing broad Exception catches with specific exception types in provider validation methods.",
          "details": "Issue: Multiple providers catch broad Exception types in validation methods, which can mask unexpected errors.\n\nFiles to fix:\n1. app/services/openai_provider.py (lines 100-103):\n   ```python\n   # ❌ Current\n   except openai.AuthenticationError:\n       return False\n   except Exception:  # Too broad\n       return False\n   \n   # ✅ Should be\n   except openai.AuthenticationError:\n       return False\n   except (openai.APIError, openai.RateLimitError, openai.APIConnectionError):\n       return False\n   ```\n\n2. app/services/anthropic_provider.py (lines 93-96):\n   Similar fix needed for Anthropic-specific exceptions\n\n3. app/services/mistral_provider.py and deepseek_provider.py:\n   Replace generic Exception with specific HTTP and API errors\n\nBenefits:\n- Better error visibility and debugging\n- More precise error handling\n- Prevents masking of unexpected errors\n- Follows Python best practices for exception handling\n\nEach provider should catch only the specific exceptions it expects during validation.",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 7
        }
      ]
    },
    {
      "id": 8,
      "title": "Implement Recursive Translation System with Dynamic Configuration",
      "description": "Create a comprehensive configurable recursive translation system that can dynamically process nested JSON structures with configurable field mapping, supporting Arabic and Bosnian languages while integrating with the multi-provider AI system and Redis caching.",
      "details": "This task involves building a flexible recursive translation system that addresses the limitations of the existing directus-translator project:\n\n1. **Dynamic Field Configuration System**:\n   - Replace hardcoded field lists with a database-backed configuration system\n   - Create models for storing translation configurations (collections, fields, rules)\n   - Implement configuration loading and validation services\n\n2. **Recursive Translation Engine**:\n   - Develop a recursive traversal algorithm for nested JSON structures\n   - Support configurable depth limits to prevent infinite recursion\n   - Implement pattern matching for field inclusion/exclusion\n   - Handle special cases for arrays, objects, and primitive values\n\n3. **Cultural Context Handling**:\n   - Add language-specific context providers for Arabic and Bosnian\n   - Implement right-to-left text handling for Arabic\n   - Create culturally-sensitive prompt templates for each language pair\n\n4. **Integration with Provider System**:\n   - Connect to the existing multi-provider system (Task 2)\n   - Implement provider selection logic based on content complexity\n   - Add fallback mechanisms when primary providers fail\n\n5. **Configuration API Endpoints**:\n   - Create REST endpoints for managing translation configurations\n   - Implement real-time configuration updates without service restart\n   - Add validation middleware for configuration changes\n\n6. **Complex Structure Handling**:\n   - Support Directus-specific data structures (collections, relations, blocks)\n   - Implement special handlers for FAQs, steps, rows, and other nested structures\n   - Add content type detection and appropriate processing\n\n7. **Performance Optimization**:\n   - Implement batch translation for efficiency\n   - Utilize the Redis caching layer (Task 3) for cost optimization\n   - Add metrics collection for translation performance\n\n8. **Implementation Details**:\n   ```python\n   # Example recursive translation function\n   async def translate_recursive(\n       data: Dict[str, Any], \n       config: TranslationConfig,\n       source_lang: str,\n       target_lang: str,\n       depth: int = 0\n   ) -> Dict[str, Any]:\n       \"\"\"Recursively translate a nested data structure based on configuration.\"\"\"\n       if depth > config.max_depth:\n           return data\n           \n       result = {}\n       for key, value in data.items():\n           # Check if this field should be translated based on config\n           if not should_translate_field(key, config):\n               result[key] = value\n               continue\n               \n           if isinstance(value, dict):\n               # Recursive case for nested objects\n               result[key] = await translate_recursive(\n                   value, config, source_lang, target_lang, depth + 1\n               )\n           elif isinstance(value, list):\n               # Handle arrays of objects or strings\n               result[key] = await translate_array(\n                   value, config, source_lang, target_lang, depth + 1\n               )\n           elif isinstance(value, str) and value.strip():\n               # Translate leaf string values\n               result[key] = await translation_service.translate(\n                   value, source_lang, target_lang, \n                   context=get_field_context(key, config)\n               )\n           else:\n               # Pass through non-string primitives\n               result[key] = value\n               \n       return result\n   ```\n\n9. **Configuration Schema**:\n   ```python\n   # Example configuration model\n   class TranslationFieldConfig(BaseModel):\n       collection: str\n       field: str\n       include_pattern: Optional[str] = None\n       exclude_pattern: Optional[str] = None\n       max_depth: int = 5\n       context_template: Optional[str] = None\n       custom_rules: Dict[str, Any] = Field(default_factory=dict)\n   ```\n\n10. **API Endpoint Implementation**:\n    - GET /api/translation/config - List all configurations\n    - POST /api/translation/config - Create new configuration\n    - PUT /api/translation/config/{id} - Update configuration\n    - DELETE /api/translation/config/{id} - Remove configuration\n    - POST /api/translation/translate - Perform translation with current config",
      "testStrategy": "1. **Unit Testing**:\n   - Create unit tests for the recursive translation algorithm with various nested structures\n   - Test field inclusion/exclusion patterns with different configuration settings\n   - Verify correct handling of arrays, objects, and primitive values\n   - Test edge cases like empty objects, null values, and maximum depth scenarios\n\n2. **Integration Testing**:\n   - Test integration with the translation provider system (Task 2)\n   - Verify proper utilization of the Redis caching layer (Task 3)\n   - Test the complete translation pipeline with real Directus data structures\n   - Verify fallback behavior when primary providers fail\n\n3. **Configuration API Testing**:\n   - Test all CRUD operations on the configuration API endpoints\n   - Verify real-time configuration updates are applied without service restart\n   - Test validation of invalid configuration submissions\n   - Verify proper error handling and response formats\n\n4. **Language-Specific Testing**:\n   - Test Arabic translation with right-to-left text handling\n   - Test Bosnian translation with appropriate cultural context\n   - Verify correct handling of language-specific formatting and conventions\n   - Test with real-world content examples for each language\n\n5. **Performance Testing**:\n   - Measure translation throughput with various content sizes\n   - Test caching effectiveness for repeated translations\n   - Verify resource usage under load\n   - Test with large nested structures to ensure efficient processing\n\n6. **End-to-End Testing**:\n   - Create test scenarios that simulate real Directus usage patterns\n   - Test with complex nested structures (FAQs, steps, rows)\n   - Verify correct translation of all configured fields\n   - Test with actual content from production or staging environments\n\n7. **Manual Testing Checklist**:\n   - Verify translations maintain proper formatting and structure\n   - Check cultural appropriateness of translations\n   - Test configuration changes through the API\n   - Verify proper handling of HTML content within translations\n\n8. **Acceptance Criteria Verification**:\n   - Confirm dynamic field configuration works as expected\n   - Verify multi-language support with cultural sensitivity\n   - Test recursive translation at various depths\n   - Confirm integration with all provider fallbacks\n   - Verify Redis caching reduces duplicate API calls\n   - Test real-time configuration updates",
      "status": "pending",
      "dependencies": [
        4
      ],
      "priority": "high",
      "subtasks": [
        {
          "id": 1,
          "title": "Create Database Models for Translation Configuration",
          "description": "Design and implement database models to store translation configurations including collections, fields, rules, and context templates.",
          "dependencies": [],
          "details": "Create SQLAlchemy models for TranslationConfig, TranslationFieldConfig, and ContextTemplate. TranslationConfig should include collection name, language pairs, and global settings. TranslationFieldConfig should store field-specific rules including patterns for inclusion/exclusion and max depth. ContextTemplate should store language-specific prompt templates.",
          "status": "pending",
          "testStrategy": "Write unit tests to verify model creation, relationships, and constraint validation. Test database migrations and ensure proper indexing for query performance."
        },
        {
          "id": 2,
          "title": "Implement Configuration Management Service",
          "description": "Create a service layer to manage translation configurations with CRUD operations and validation logic.",
          "dependencies": [
            1
          ],
          "details": "Develop ConfigurationService class with methods for loading, validating, creating, updating, and deleting configurations. Implement validation logic to ensure configurations are well-formed. Add methods to retrieve configurations by collection, field, or language pair. Include caching mechanism to avoid frequent database queries.",
          "status": "pending",
          "testStrategy": "Test configuration loading with various parameters. Verify validation logic catches invalid configurations. Test cache invalidation when configurations change."
        },
        {
          "id": 3,
          "title": "Develop Core Recursive Translation Algorithm",
          "description": "Implement the recursive traversal algorithm that can process nested JSON structures according to configuration rules.",
          "dependencies": [
            1,
            2
          ],
          "details": "Create TranslationEngine class with translate_recursive method as shown in the example. Implement helper methods for handling different data types (objects, arrays, primitives). Add depth tracking and limits to prevent infinite recursion. Implement pattern matching for field inclusion/exclusion based on configuration.",
          "status": "pending",
          "testStrategy": "Test with various nested structures of different depths. Verify pattern matching works correctly. Test edge cases like empty objects, null values, and maximum depth scenarios."
        },
        {
          "id": 4,
          "title": "Build Provider Integration Layer",
          "description": "Create an adapter layer to connect the translation engine with multiple AI providers and implement provider selection logic.",
          "dependencies": [
            3
          ],
          "details": "Implement ProviderManager class that interfaces with the multi-provider AI system. Add logic to select appropriate providers based on content complexity, language pair, and cost considerations. Implement fallback mechanisms when primary providers fail. Add retry logic with exponential backoff for transient errors.",
          "status": "pending",
          "testStrategy": "Test provider selection with different content types. Mock provider responses to test fallback scenarios. Verify retry logic works correctly under various failure conditions."
        },
        {
          "id": 5,
          "title": "Implement Language-Specific Context Handlers",
          "description": "Create specialized context handlers for Arabic and Bosnian languages with culturally appropriate translation rules.",
          "dependencies": [
            2
          ],
          "details": "Develop ContextHandler classes for each supported language pair. Implement right-to-left text handling for Arabic. Create culturally-sensitive prompt templates. Add special case handling for idioms, formal/informal speech, and gender-specific language. Implement context injection into translation requests.",
          "status": "pending",
          "testStrategy": "Test with language-specific edge cases. Verify RTL handling works correctly for Arabic. Test with culturally specific phrases to ensure appropriate translations."
        },
        {
          "id": 6,
          "title": "Develop Redis Caching Integration",
          "description": "Integrate the translation system with Redis for caching translations to improve performance and reduce costs.",
          "dependencies": [
            3,
            4
          ],
          "details": "Create CacheManager class that interfaces with Redis. Implement intelligent caching strategies with appropriate TTL values. Add cache key generation based on content hash, language pair, and configuration version. Implement batch retrieval and storage operations for efficiency. Add cache invalidation mechanisms when configurations change.",
          "status": "pending",
          "testStrategy": "Test cache hit/miss scenarios. Verify cached translations are returned correctly. Measure performance improvements with caching enabled vs. disabled."
        },
        {
          "id": 7,
          "title": "Create REST API Endpoints for Translation and Configuration",
          "description": "Implement REST API endpoints for translation operations and configuration management.",
          "dependencies": [
            2,
            3,
            4,
            6
          ],
          "details": "Create FastAPI/Flask routes for translation and configuration management. Implement endpoints listed in the task description (GET/POST/PUT/DELETE for configurations, POST for translation). Add request validation, error handling, and appropriate HTTP status codes. Implement pagination for configuration listing. Add authentication and authorization middleware.",
          "status": "pending",
          "testStrategy": "Test API endpoints with various request payloads. Verify error responses for invalid inputs. Test authentication and authorization scenarios. Perform load testing to ensure endpoint performance."
        },
        {
          "id": 8,
          "title": "Implement Special Handlers for Complex Structures",
          "description": "Create specialized handlers for Directus-specific data structures and complex nested content types.",
          "dependencies": [
            3,
            5
          ],
          "details": "Develop structure-specific handlers for FAQs, steps, rows, and other common content patterns. Implement content type detection to automatically apply appropriate handlers. Create TranslationStrategy classes for different structure types. Add support for Directus blocks and relations with proper reference handling. Implement custom field processors for markdown, HTML, and other formatted content.",
          "status": "pending",
          "testStrategy": "Test with real-world Directus content structures. Verify complex nested structures are correctly traversed and translated. Test with various content types to ensure appropriate handling."
        }
      ]
    },
    {
      "id": 9,
      "title": "Create Migration and Compatibility Layer for Translation System",
      "description": "Develop a comprehensive migration and compatibility layer to transition from the hardcoded field approach in the existing directus-translator to the new configurable recursive translation system, ensuring backward compatibility and smooth transition for existing users.",
      "details": "This task involves creating a complete migration path from the existing hardcoded translation system to the new configurable recursive approach:\n\n1. **Analysis and Extraction Tool**:\n   - Create a utility that scans the existing directus-translator codebase to identify all hardcoded field mappings\n   - Implement parsers to extract field paths, content types, and translation rules\n   - Generate equivalent configuration entries for the new system's database\n   - Support batch conversion of multiple projects/configurations\n\n2. **Backward Compatibility Layer**:\n   - Implement adapter endpoints that match the existing API interface signatures\n   - Create request/response transformers to map between old and new formats\n   - Ensure all existing client code continues to function without modification\n   - Add deprecation notices and migration guidance in responses\n\n3. **Migration Scripts**:\n   - Develop database migration scripts to preserve existing translation data\n   - Create tools to transfer translation history and caching information\n   - Implement validation to ensure no data loss during migration\n   - Support rollback capabilities if migration encounters issues\n\n4. **Configuration Import/Export**:\n   - Create JSON schema for configuration export format\n   - Implement export functionality to generate portable configuration files\n   - Build import tools to apply configurations across environments\n   - Add validation during import to prevent invalid configurations\n\n5. **Validation and Verification Tools**:\n   - Develop comparison utilities to verify translation equivalence\n   - Create side-by-side testing framework for old vs. new system\n   - Implement automated verification for all supported content types\n   - Add reporting for any discrepancies found during validation\n\n6. **Performance Benchmarking**:\n   - Create benchmarking suite to compare old vs. new implementation\n   - Measure translation throughput, latency, and resource utilization\n   - Generate performance reports with recommendations for optimization\n   - Identify any performance regressions and implement mitigations\n\n7. **Documentation and Migration Guides**:\n   - Create comprehensive migration documentation\n   - Develop step-by-step guides for transitioning existing projects\n   - Add examples for common migration scenarios\n   - Create troubleshooting guides for common migration issues\n\n8. **Compatibility Configuration UI**:\n   - Add UI components to visualize and edit migration mappings\n   - Create dashboard for monitoring migration progress\n   - Implement configuration wizards for common migration patterns\n   - Support bulk operations for configuration management",
      "testStrategy": "The migration and compatibility layer will be tested through a comprehensive verification process:\n\n1. **Functional Testing**:\n   - Create test suite with examples of all supported field types and content formats\n   - Verify that the analysis tool correctly extracts all hardcoded field mappings\n   - Confirm that extracted configurations produce identical results in the new system\n   - Test backward compatibility endpoints with existing client code\n   - Validate that all API responses match the expected format\n\n2. **Migration Testing**:\n   - Create test databases with production-like data\n   - Execute migration scripts and verify data integrity\n   - Test rollback functionality to ensure it restores the original state\n   - Verify that all translation history and cache data is preserved\n\n3. **Configuration Management Testing**:\n   - Export configurations from multiple environments\n   - Import configurations into clean environments\n   - Verify that imported configurations produce identical results\n   - Test validation of malformed or incomplete configurations\n\n4. **Performance Testing**:\n   - Run benchmark suite against both old and new implementations\n   - Measure and compare response times for various request types and volumes\n   - Test under load to identify scaling differences\n   - Document performance characteristics and any optimizations needed\n\n5. **Integration Testing**:\n   - Test with actual Directus instances using the existing translator\n   - Verify seamless transition when switching to the new system\n   - Confirm that all existing workflows continue to function\n   - Test with multiple language pairs and content types\n\n6. **User Acceptance Testing**:\n   - Provide migration tools to select users for beta testing\n   - Collect feedback on migration experience and documentation clarity\n   - Address any issues reported during the testing phase\n   - Verify that the migration process is intuitive and well-documented\n\n7. **Regression Testing**:\n   - Create automated test suite comparing outputs from both systems\n   - Run regression tests for each supported language pair and content type\n   - Verify that translation quality remains consistent or improves\n   - Document any differences in behavior between the systems",
      "status": "pending",
      "dependencies": [
        8
      ],
      "priority": "medium",
      "subtasks": []
    }
  ]
}