{
  "tasks": [
    {
      "id": 1,
      "title": "Setup FastAPI Project with Docker",
      "description": "Initialize the FastAPI project with PostgreSQL and Redis integration, and create Docker configuration for easy deployment.",
      "details": "1. Create a new FastAPI project structure\n2. Set up PostgreSQL connection using SQLAlchemy ORM\n3. Configure Redis client for caching\n4. Create Docker and docker-compose files for local development\n5. Implement health check endpoint (GET /health)\n\nProject structure:\n```\nlocplat/\n├── app/\n│   ├── __init__.py\n│   ├── main.py\n│   ├── config.py\n│   ├── models/\n│   ├── api/\n│   ├── services/\n│   └── utils/\n├── tests/\n├── Dockerfile\n├── docker-compose.yml\n├── requirements.txt\n└── README.md\n```\n\nMain dependencies:\n- fastapi\n- uvicorn\n- sqlalchemy\n- psycopg2-binary\n- redis\n- pydantic\n\nImplement basic health check endpoint in main.py:\n```python\n@app.get('/health')\ndef health_check():\n    return {'status': 'ok'}\n```",
      "testStrategy": "1. Verify FastAPI server starts correctly\n2. Confirm PostgreSQL connection is established\n3. Validate Redis connection is working\n4. Test health check endpoint returns 200 OK\n5. Ensure Docker containers build and run properly\n6. Validate environment variables are properly loaded",
      "priority": "high",
      "dependencies": [],
      "status": "done",
      "subtasks": []
    },
    {
      "id": 2,
      "title": "Implement Translation Provider Integration",
      "description": "Create services to integrate with OpenAI (primary) and Google Translate (fallback) translation providers using client-provided API keys.",
      "details": "1. Create abstract translation provider interface\n2. Implement OpenAI provider using their API\n3. Implement Google Translate provider as fallback\n4. Create provider router to handle fallback logic\n5. Add language pair support (English → Arabic, English → Bosnian)\n\nProvider interface:\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any\n\nclass TranslationProvider(ABC):\n    @abstractmethod\n    async def translate(self, text: str, source_lang: str, target_lang: str, api_key: str) -> str:\n        pass\n\nclass OpenAIProvider(TranslationProvider):\n    async def translate(self, text: str, source_lang: str, target_lang: str, api_key: str) -> str:\n        # OpenAI implementation using their API\n        # Use the provided API key for authentication\n        pass\n\nclass GoogleTranslateProvider(TranslationProvider):\n    async def translate(self, text: str, source_lang: str, target_lang: str, api_key: str) -> str:\n        # Google Translate implementation\n        pass\n\nclass ProviderRouter:\n    def __init__(self):\n        self.primary = OpenAIProvider()\n        self.fallback = GoogleTranslateProvider()\n    \n    async def translate(self, text: str, source_lang: str, target_lang: str, \n                        openai_key: str, google_key: str) -> str:\n        try:\n            return await self.primary.translate(text, source_lang, target_lang, openai_key)\n        except Exception as e:\n            # Log the error\n            return await self.fallback.translate(text, source_lang, target_lang, google_key)\n```\n\nImplement GET /languages endpoint to return supported language pairs.",
      "testStrategy": "1. Unit test each provider with mock API responses\n2. Test fallback mechanism when primary provider fails\n3. Verify correct handling of API keys\n4. Test with actual API keys in development environment\n5. Validate supported language pairs\n6. Test error handling for invalid API keys\n7. Measure response times for performance benchmarking",
      "priority": "high",
      "dependencies": [
        1
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 3,
      "title": "Implement Redis Caching Layer",
      "description": "Create a caching system using Redis to store translation results and avoid duplicate API calls, reducing costs and improving performance.",
      "details": "1. Design cache key generation based on content hash\n2. Implement Redis cache operations (get, set, delete)\n3. Add TTL (time-to-live) for cached translations\n4. Create cache hit/miss tracking for metrics\n\nCache implementation:\n```python\nimport hashlib\nimport json\nfrom redis import Redis\nfrom typing import Dict, Any, Optional\n\nclass TranslationCache:\n    def __init__(self, redis_client: Redis, ttl_seconds: int = 86400):\n        self.redis = redis_client\n        self.ttl = ttl_seconds\n    \n    def _generate_key(self, content: str, source_lang: str, target_lang: str) -> str:\n        # Create a unique hash based on content and language pair\n        content_hash = hashlib.md5(content.encode()).hexdigest()\n        return f\"translation:{source_lang}:{target_lang}:{content_hash}\"\n    \n    async def get_cached_translation(self, content: str, source_lang: str, target_lang: str) -> Optional[str]:\n        key = self._generate_key(content, source_lang, target_lang)\n        cached = self.redis.get(key)\n        if cached:\n            # Track cache hit\n            self.redis.incr('cache:hits')\n            return cached.decode('utf-8')\n        # Track cache miss\n        self.redis.incr('cache:misses')\n        return None\n    \n    async def cache_translation(self, content: str, source_lang: str, target_lang: str, translation: str) -> None:\n        key = self._generate_key(content, source_lang, target_lang)\n        self.redis.set(key, translation, ex=self.ttl)\n    \n    async def get_cache_stats(self) -> Dict[str, int]:\n        hits = int(self.redis.get('cache:hits') or 0)\n        misses = int(self.redis.get('cache:misses') or 0)\n        return {\n            'hits': hits,\n            'misses': misses,\n            'hit_rate': hits / (hits + misses) if (hits + misses) > 0 else 0\n        }\n```",
      "testStrategy": "1. Unit test cache key generation\n2. Test cache hit and miss scenarios\n3. Verify TTL functionality\n4. Benchmark cache performance\n5. Test cache statistics tracking\n6. Validate cache behavior with different content types\n7. Test concurrent cache access",
      "priority": "medium",
      "dependencies": [
        1
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 4,
      "title": "Implement Field Mapping and Content Processing",
      "description": "Create a system to map and process fields for translation, supporting plain text and basic HTML content types with JSON path support.",
      "details": "1. Implement field mapping configuration storage in PostgreSQL\n2. Create JSON path parser for field selection\n3. Add content type detection and processing (text vs HTML)\n4. Implement field extraction and reassembly logic\n\nDatabase model:\n```python\nfrom sqlalchemy import Column, Integer, String, JSON, DateTime\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom datetime import datetime\n\nBase = declarative_base()\n\nclass FieldConfig(Base):\n    __tablename__ = 'field_configs'\n    \n    id = Column(Integer, primary_key=True)\n    client_id = Column(String, nullable=False)\n    collection_name = Column(String, nullable=False)\n    field_paths = Column(JSON, nullable=False)  # JSON array of paths\n    created_at = Column(DateTime, default=datetime.utcnow)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n```\n\nField mapper implementation:\n```python\nfrom typing import Dict, Any, List\nimport re\nfrom bs4 import BeautifulSoup\n\nclass FieldMapper:\n    def __init__(self, db_session):\n        self.db_session = db_session\n    \n    async def get_field_config(self, client_id: str, collection_name: str) -> List[str]:\n        # Get field paths from database\n        config = self.db_session.query(FieldConfig).filter_by(\n            client_id=client_id,\n            collection_name=collection_name\n        ).first()\n        \n        return config.field_paths if config else []\n    \n    async def save_field_config(self, client_id: str, collection_name: str, field_paths: List[str]) -> None:\n        # Save field configuration to database\n        config = self.db_session.query(FieldConfig).filter_by(\n            client_id=client_id,\n            collection_name=collection_name\n        ).first()\n        \n        if config:\n            config.field_paths = field_paths\n            config.updated_at = datetime.utcnow()\n        else:\n            config = FieldConfig(\n                client_id=client_id,\n                collection_name=collection_name,\n                field_paths=field_paths\n            )\n            self.db_session.add(config)\n        \n        self.db_session.commit()\n    \n    def extract_fields(self, content: Dict[str, Any], field_paths: List[str]) -> Dict[str, Any]:\n        # Extract fields to translate based on paths\n        result = {}\n        for path in field_paths:\n            value = self._get_nested_value(content, path)\n            if value:\n                result[path] = value\n        return result\n    \n    def _get_nested_value(self, data: Dict[str, Any], path: str) -> Any:\n        # Get value from nested dictionary using dot notation\n        parts = path.split('.')\n        current = data\n        for part in parts:\n            if isinstance(current, dict) and part in current:\n                current = current[part]\n            else:\n                return None\n        return current\n    \n    def is_html(self, text: str) -> bool:\n        # Simple check if content is HTML\n        return bool(re.search(r'<[^>]+>', text))\n    \n    def extract_text_from_html(self, html: str) -> List[Dict[str, Any]]:\n        # Extract text nodes from HTML for translation\n        soup = BeautifulSoup(html, 'html.parser')\n        text_nodes = []\n        \n        for element in soup.find_all(text=True):\n            if element.strip():\n                parent = element.parent.name if element.parent else None\n                text_nodes.append({\n                    'text': element.strip(),\n                    'path': self._get_element_path(element)\n                })\n        \n        return text_nodes\n    \n    def _get_element_path(self, element) -> str:\n        # Generate a path to the element for reassembly\n        # Simplified implementation\n        return ''\n```",
      "testStrategy": "1. Unit test field extraction from nested objects\n2. Test HTML detection and processing\n3. Verify field configuration storage and retrieval\n4. Test JSON path parsing with various path formats\n5. Validate HTML content extraction and reassembly\n6. Test with different content structures\n7. Verify handling of missing fields",
      "priority": "medium",
      "dependencies": [
        2,
        3
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 5,
      "title": "Implement Translation API Endpoints",
      "description": "Create the main translation API endpoints including the core translation functionality, language pair listing, and field configuration.",
      "details": "1. Implement POST /translate endpoint\n2. Create GET /languages endpoint\n3. Add POST /fields/config endpoint\n4. Integrate all components (providers, cache, field mapping)\n5. Implement request/response models\n\nAPI implementation:\n```python\nfrom fastapi import FastAPI, Depends, HTTPException, Body\nfrom pydantic import BaseModel, Field\nfrom typing import Dict, List, Any, Optional\n\napp = FastAPI(title=\"LocPlat Translation Service\")\n\nclass TranslationRequest(BaseModel):\n    content: Dict[str, Any]\n    target_language: str\n    openai_key: str\n    google_key: str\n    source_language: str = \"en\"\n    collection_name: Optional[str] = None\n    client_id: Optional[str] = None\n\nclass TranslationResponse(BaseModel):\n    translated_content: Dict[str, Any]\n    source_language: str\n    target_language: str\n    provider_used: str\n    cache_hit: bool\n\nclass FieldConfigRequest(BaseModel):\n    client_id: str\n    collection_name: str\n    field_paths: List[str]\n\nclass LanguagePair(BaseModel):\n    source: str\n    target: str\n    name: str\n\n@app.post(\"/translate\", response_model=TranslationResponse)\nasync def translate_content(\n    request: TranslationRequest,\n    translation_service = Depends(get_translation_service)\n):\n    try:\n        result = await translation_service.translate_content(\n            content=request.content,\n            source_lang=request.source_language,\n            target_lang=request.target_language,\n            openai_key=request.openai_key,\n            google_key=request.google_key,\n            collection_name=request.collection_name,\n            client_id=request.client_id\n        )\n        return result\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get(\"/languages\", response_model=List[LanguagePair])\nasync def get_languages():\n    return [\n        {\"source\": \"en\", \"target\": \"ar\", \"name\": \"English to Arabic\"},\n        {\"source\": \"en\", \"target\": \"bs\", \"name\": \"English to Bosnian\"}\n    ]\n\n@app.post(\"/fields/config\")\nasync def configure_fields(\n    config: FieldConfigRequest,\n    field_mapper = Depends(get_field_mapper)\n):\n    await field_mapper.save_field_config(\n        client_id=config.client_id,\n        collection_name=config.collection_name,\n        field_paths=config.field_paths\n    )\n    return {\"status\": \"success\"}\n```\n\nTranslation service implementation:\n```python\nclass TranslationService:\n    def __init__(self, provider_router, cache, field_mapper):\n        self.provider_router = provider_router\n        self.cache = cache\n        self.field_mapper = field_mapper\n    \n    async def translate_content(\n        self, content, source_lang, target_lang, openai_key, google_key, \n        collection_name=None, client_id=None\n    ):\n        # Get field paths if collection_name and client_id provided\n        field_paths = []\n        if collection_name and client_id:\n            field_paths = await self.field_mapper.get_field_config(client_id, collection_name)\n        \n        # If no specific fields configured, translate all string fields\n        if not field_paths:\n            field_paths = self._find_string_fields(content)\n        \n        # Extract fields to translate\n        fields_to_translate = self.field_mapper.extract_fields(content, field_paths)\n        \n        # Create result copy\n        result = content.copy()\n        cache_hit = False\n        provider_used = \"openai\"\n        \n        # Process each field\n        for path, value in fields_to_translate.items():\n            if not isinstance(value, str):\n                continue\n                \n            # Check cache\n            cached = await self.cache.get_cached_translation(value, source_lang, target_lang)\n            if cached:\n                translated = cached\n                cache_hit = True\n            else:\n                # Translate with provider router\n                translated = await self.provider_router.translate(\n                    value, source_lang, target_lang, openai_key, google_key\n                )\n                # Cache result\n                await self.cache.cache_translation(value, source_lang, target_lang, translated)\n                \n                # Determine which provider was used (simplified)\n                if hasattr(self.provider_router, 'last_provider_used'):\n                    provider_used = self.provider_router.last_provider_used\n            \n            # Update result\n            self._set_nested_value(result, path, translated)\n        \n        return {\n            \"translated_content\": result,\n            \"source_language\": source_lang,\n            \"target_language\": target_lang,\n            \"provider_used\": provider_used,\n            \"cache_hit\": cache_hit\n        }\n    \n    def _find_string_fields(self, content, prefix=\"\"):\n        # Recursively find all string fields in the content\n        fields = []\n        if isinstance(content, dict):\n            for key, value in content.items():\n                path = f\"{prefix}.{key}\" if prefix else key\n                if isinstance(value, str):\n                    fields.append(path)\n                elif isinstance(value, dict) or isinstance(value, list):\n                    fields.extend(self._find_string_fields(value, path))\n        elif isinstance(content, list):\n            for i, item in enumerate(content):\n                path = f\"{prefix}[{i}]\"\n                fields.extend(self._find_string_fields(item, path))\n        return fields\n    \n    def _set_nested_value(self, data, path, value):\n        # Set value in nested dictionary using dot notation\n        parts = path.split('.')\n        current = data\n        for i, part in enumerate(parts):\n            if i == len(parts) - 1:\n                current[part] = value\n            else:\n                if part not in current:\n                    current[part] = {}\n                current = current[part]\n```",
      "testStrategy": "1. Test POST /translate with various content structures\n2. Verify correct handling of API keys\n3. Test caching behavior and hit rate\n4. Validate field mapping functionality\n5. Test error handling and fallback to Google Translate\n6. Verify language pair endpoint returns correct data\n7. Test field configuration endpoint\n8. Perform integration tests with all components\n9. Benchmark API response times\n10. Test with actual Directus content structures",
      "priority": "high",
      "dependencies": [
        2,
        3,
        4
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 6,
      "title": "Implement Directus Integration",
      "description": "Create specialized endpoints and functionality for Directus CMS integration, including batch translation support and Directus-specific data formatting.",
      "details": "1. Create Directus-specific data models\n2. Implement batch translation for collections\n3. Add Directus field format handling\n4. Create specialized endpoints for Directus\n\nDirectus models:\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import Dict, List, Any, Optional\n\nclass DirectusItem(BaseModel):\n    id: str\n    collection: str\n    item: Dict[str, Any]\n\nclass DirectusBatchRequest(BaseModel):\n    items: List[DirectusItem]\n    target_language: str\n    openai_key: str\n    google_key: str\n    source_language: str = \"en\"\n    client_id: Optional[str] = None\n\nclass DirectusBatchResponse(BaseModel):\n    items: List[Dict[str, Any]]\n    stats: Dict[str, Any]\n```\n\nDirectus integration implementation:\n```python\nfrom fastapi import APIRouter, Depends, HTTPException\n\ndirectus_router = APIRouter(prefix=\"/directus\", tags=[\"directus\"])\n\n@directus_router.post(\"/batch\", response_model=DirectusBatchResponse)\nasync def translate_directus_batch(\n    request: DirectusBatchRequest,\n    translation_service = Depends(get_translation_service)\n):\n    results = []\n    stats = {\n        \"total\": len(request.items),\n        \"successful\": 0,\n        \"failed\": 0,\n        \"cache_hits\": 0\n    }\n    \n    for item in request.items:\n        try:\n            # Get field config for this collection\n            field_paths = []\n            if request.client_id:\n                field_paths = await translation_service.field_mapper.get_field_config(\n                    request.client_id, item.collection\n                )\n            \n            # Translate the item\n            result = await translation_service.translate_content(\n                content=item.item,\n                source_lang=request.source_language,\n                target_lang=request.target_language,\n                openai_key=request.openai_key,\n                google_key=request.google_key,\n                collection_name=item.collection,\n                client_id=request.client_id\n            )\n            \n            # Add to results\n            results.append({\n                \"id\": item.id,\n                \"collection\": item.collection,\n                \"item\": result[\"translated_content\"]\n            })\n            \n            # Update stats\n            stats[\"successful\"] += 1\n            if result[\"cache_hit\"]:\n                stats[\"cache_hits\"] += 1\n                \n        except Exception as e:\n            stats[\"failed\"] += 1\n            # Log the error\n            print(f\"Error translating item {item.id}: {str(e)}\")\n    \n    return {\n        \"items\": results,\n        \"stats\": stats\n    }\n\n@directus_router.post(\"/collection/{collection_name}\")\nasync def configure_directus_collection(\n    collection_name: str,\n    config: Dict[str, Any],\n    field_mapper = Depends(get_field_mapper)\n):\n    # Configure which fields to translate for a Directus collection\n    client_id = config.get(\"client_id\")\n    field_paths = config.get(\"field_paths\", [])\n    \n    if not client_id:\n        raise HTTPException(status_code=400, detail=\"client_id is required\")\n    \n    await field_mapper.save_field_config(\n        client_id=client_id,\n        collection_name=collection_name,\n        field_paths=field_paths\n    )\n    \n    return {\"status\": \"success\"}\n\n# Add the router to the main app\napp.include_router(directus_router)\n```\n\nDirectus helper functions:\n```python\ndef format_directus_response(translated_items, original_items):\n    \"\"\"Format the response in a way that's easy to use with Directus\"\"\"\n    result = []\n    \n    for orig, trans in zip(original_items, translated_items):\n        # Create a new item with the same structure as the original\n        # but with translated content\n        result.append({\n            \"id\": orig[\"id\"],\n            \"collection\": orig[\"collection\"],\n            \"item\": trans[\"translated_content\"]\n        })\n    \n    return result\n```",
      "testStrategy": "1. Test batch translation endpoint with Directus collection data\n2. Verify correct handling of Directus-specific data structures\n3. Test collection configuration endpoint\n4. Validate field mapping works correctly with Directus fields\n5. Test performance with larger batches of items\n6. Verify error handling for partial batch failures\n7. Test with actual Directus API responses\n8. Validate statistics reporting\n9. Test with different collection types and field structures\n10. Verify integration with Directus webhook patterns",
      "priority": "medium",
      "dependencies": [
        5
      ],
      "status": "pending",
      "subtasks": []
    }
  ]
}