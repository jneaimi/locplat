# Task ID: 4
# Title: Implement Field Mapping and Content Processing
# Status: in-progress
# Dependencies: 2, 3
# Priority: medium
# Description: Create a system to map and process fields for translation, supporting Directus collection structures with primary fields and translation collections, handling plain text, HTML, and other content types with JSON path support. Focus on Directus-specific patterns and support for structured data from AI providers with batch operations.
# Details:
## Implemented Components (COMPLETED)

### Database Models & Configuration
- `FieldConfig` model with comprehensive field mapping support
- `FieldProcessingLog` for operation tracking  
- Field type enumerations (TEXT, WYSIWYG, HTML, JSON, etc.)
- Support for RTL languages (Arabic, Hebrew, Farsi, Urdu)
- Directus translation patterns (collection_translations, language_collections)

### Core Field Mapper Service
- `FieldMapper` class with extraction and processing logic
- JSON path parser supporting dot notation and array indices
- Content type auto-detection (HTML, multiline text, JSON)
- HTML structure preservation during translation
- Batch processing for efficient multi-field operations
- Content sanitization and validation

### Content Processor Service
- `ContentProcessor` for AI provider response handling
- Support for OpenAI, Anthropic, Mistral, DeepSeek response formats
- Structured data parsing from various AI response formats
- JSON and text-based response parsing

### API Endpoints
- RESTful API at `/api/v1/field-mapping/`
- Create/update/delete field configurations
- Extract translatable fields from content
- Validate field paths against content
- List configurations by client
- Translation endpoints at `/api/v1/translate/` for structured content

### Database Integration
- Database dependency injection setup
- Caching system for field configurations (5-minute TTL)
- Operation logging for monitoring and debugging
- Migration and initialization scripts

### Translation Integration
- `IntegratedTranslationService` combining FieldMapper with FlexibleTranslationService
- Client-specified provider selection (not cascading fallback)
- Structured content processing with intelligent field extraction
- Batch translation support for cost efficiency
- Complete translation workflow pipeline

### Directus Webhook Integration
- Comprehensive webhook system at `/api/v1/webhooks/directus/translate`
- HMAC signature verification (SHA-256/SHA-1 support)
- Support for all Directus translation patterns
- Configuration validation and testing endpoints
- Infinite loop prevention for translation collections
- Complete automation from Directus → LocPlat → Translation → Storage

## Key Features Implemented
- **Directus CMS Integration**: Native support for collection_translations and language_collections patterns
- **RTL Language Support**: Special field mapping for Arabic and other RTL languages  
- **HTML Processing**: Extract/preserve HTML structure during translation
- **Batch Operations**: Efficient processing of multiple text fields
- **Content Sanitization**: Security validation for user content
- **AI Provider Integration**: Handle structured responses from all supported providers
- **Nested Field Support**: JSON path extraction with array indexing
- **Type Detection**: Automatic field type detection (text, HTML, JSON, etc.)
- **End-to-End Translation**: Complete workflow from field extraction to translated content
- **Preview & Validation**: API endpoints for cost-free field extraction preview and validation
- **Webhook Automation**: Automatic content processing via Directus webhooks
- **Production Security**: HMAC signature verification and comprehensive input validation

## Files Created/Modified

### Core Implementation:
- `app/models/field_config.py` - Database models
- `app/models/field_types.py` - Type definitions and enums
- `app/services/field_mapper.py` - Main field processing service
- `app/services/content_processor.py` - AI response processing
- `app/services/integrated_translation_service.py` - Translation integration orchestrator
- `app/database.py` - Database connection setup
- `app/api/field_mapping.py` - Field mapping REST API endpoints
- `app/api/translation.py` - Enhanced with structured translation endpoints
- `app/api/webhooks.py` - Directus webhook integration endpoints

### Integration & Setup:
- Updated `app/main.py` - Added field mapping and webhook routers
- Updated `requirements.txt` - Added BeautifulSoup4 dependency
- `scripts/init_field_mapping.py` - Database initialization
- `scripts/test_field_mapping.py` - Comprehensive testing

### Documentation:
- `docs/field-mapping-guide.md` - Complete usage guide
- `docs/webhook-integration-guide.md` - Directus webhook setup guide
- `tests/test_field_mapping.py` - Unit tests
- `tests/test_webhooks.py` - Webhook integration tests

## Remaining Work
1. Implement the Redis caching layer integration (Task #3)
2. Perform end-to-end testing of the complete translation workflow

Database model:
```python
from sqlalchemy import Column, Integer, String, JSON, DateTime, Boolean, ForeignKey
from sqlalchemy.ext.declarative import declarative_base
from datetime import datetime

Base = declarative_base()

class FieldConfig(Base):
    __tablename__ = 'field_configs'
    
    id = Column(Integer, primary_key=True)
    client_id = Column(String, nullable=False)
    collection_name = Column(String, nullable=False)
    field_paths = Column(JSON, nullable=False)  # JSON array of paths
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    is_translation_collection = Column(Boolean, default=False)
    primary_collection = Column(String, nullable=True)
    field_types = Column(JSON, nullable=True)  # Maps field paths to their types
    rtl_field_mapping = Column(JSON, nullable=True)  # Special handling for RTL languages
    directus_translation_pattern = Column(String, nullable=True)  # 'collection_translations' or 'language_collections'
    batch_processing = Column(Boolean, default=False)  # Whether to process fields in batch

class FieldProcessingLog(Base):
    __tablename__ = 'field_processing_logs'
    
    id = Column(Integer, primary_key=True)
    client_id = Column(String, nullable=False)
    collection_name = Column(String, nullable=False)
    operation = Column(String, nullable=False)  # 'extract', 'process', 'translate'
    status = Column(String, nullable=False)  # 'success', 'error'
    fields_processed = Column(Integer, default=0)
    error_message = Column(String, nullable=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    processing_time_ms = Column(Integer, default=0)  # Processing time in milliseconds
```

Field mapper implementation:
```python
from typing import Dict, Any, List, Optional, Tuple
import re
from bs4 import BeautifulSoup
from enum import Enum

class FieldType(Enum):
    TEXT = "text"
    WYSIWYG = "wysiwyg"
    TEXTAREA = "textarea"
    STRING = "string"
    RELATION = "relation"
    JSON = "json"

class DirectusTranslationPattern(Enum):
    COLLECTION_TRANSLATIONS = "collection_translations"
    LANGUAGE_COLLECTIONS = "language_collections"
    CUSTOM = "custom"

class FieldMapper:
    def __init__(self, db_session):
        self.db_session = db_session
    
    async def get_field_config(self, client_id: str, collection_name: str) -> Dict[str, Any]:
        # Get field paths from database
        config = self.db_session.query(FieldConfig).filter_by(
            client_id=client_id,
            collection_name=collection_name
        ).first()
        
        if not config:
            return {
                "field_paths": [],
                "is_translation_collection": False,
                "field_types": {},
                "rtl_field_mapping": {},
                "directus_translation_pattern": DirectusTranslationPattern.COLLECTION_TRANSLATIONS.value,
                "batch_processing": False
            }
            
        return {
            "field_paths": config.field_paths,
            "is_translation_collection": config.is_translation_collection,
            "primary_collection": config.primary_collection,
            "field_types": config.field_types or {},
            "rtl_field_mapping": config.rtl_field_mapping or {},
            "directus_translation_pattern": config.directus_translation_pattern or DirectusTranslationPattern.COLLECTION_TRANSLATIONS.value,
            "batch_processing": config.batch_processing or False
        }
    
    async def save_field_config(self, client_id: str, collection_name: str, 
                              field_config: Dict[str, Any]) -> None:
        # Save field configuration to database
        config = self.db_session.query(FieldConfig).filter_by(
            client_id=client_id,
            collection_name=collection_name
        ).first()
        
        if config:
            config.field_paths = field_config.get("field_paths", [])
            config.is_translation_collection = field_config.get("is_translation_collection", False)
            config.primary_collection = field_config.get("primary_collection")
            config.field_types = field_config.get("field_types", {})
            config.rtl_field_mapping = field_config.get("rtl_field_mapping", {})
            config.directus_translation_pattern = field_config.get("directus_translation_pattern", DirectusTranslationPattern.COLLECTION_TRANSLATIONS.value)
            config.batch_processing = field_config.get("batch_processing", False)
            config.updated_at = datetime.utcnow()
        else:
            config = FieldConfig(
                client_id=client_id,
                collection_name=collection_name,
                field_paths=field_config.get("field_paths", []),
                is_translation_collection=field_config.get("is_translation_collection", False),
                primary_collection=field_config.get("primary_collection"),
                field_types=field_config.get("field_types", {}),
                rtl_field_mapping=field_config.get("rtl_field_mapping", {}),
                directus_translation_pattern=field_config.get("directus_translation_pattern", DirectusTranslationPattern.COLLECTION_TRANSLATIONS.value),
                batch_processing=field_config.get("batch_processing", False)
            )
            self.db_session.add(config)
        
        self.db_session.commit()
    
    def extract_fields(self, content: Dict[str, Any], field_config: Dict[str, Any], language: str = None) -> Dict[str, Any]:
        # Extract fields to translate based on paths
        result = {}
        field_paths = field_config["field_paths"]
        field_types = field_config.get("field_types", {})
        rtl_mapping = field_config.get("rtl_field_mapping", {})
        batch_processing = field_config.get("batch_processing", False)
        
        # Check if we should use RTL-specific mapping
        if language and language in rtl_mapping:
            field_paths = rtl_mapping[language].get("field_paths", field_paths)
        
        # Handle batch processing if enabled
        if batch_processing:
            return self._extract_fields_batch(content, field_paths, field_types, language)
        
        # Standard field extraction
        for path in field_paths:
            value = self._get_nested_value(content, path)
            if value is not None:
                field_type = field_types.get(path, self._detect_field_type(value))
                result[path] = {
                    "value": value,
                    "type": field_type,
                    "metadata": self._extract_metadata(value, field_type)
                }
        return result
    
    def _extract_fields_batch(self, content: Dict[str, Any], field_paths: List[str], 
                             field_types: Dict[str, str], language: str = None) -> Dict[str, Any]:
        # Extract fields in batch for more efficient processing
        result = {}
        batch_text = []
        batch_mapping = {}
        
        # First pass: collect all text for batch processing
        for path in field_paths:
            value = self._get_nested_value(content, path)
            if value is not None:
                field_type = field_types.get(path, self._detect_field_type(value))
                
                if field_type in [FieldType.TEXT.value, FieldType.STRING.value, FieldType.TEXTAREA.value]:
                    # Add to batch for text fields
                    batch_index = len(batch_text)
                    batch_text.append(value)
                    batch_mapping[path] = {
                        "index": batch_index,
                        "type": field_type
                    }
                else:
                    # Process non-text fields individually
                    result[path] = {
                        "value": value,
                        "type": field_type,
                        "metadata": self._extract_metadata(value, field_type)
                    }
        
        # Add batch text collection to result
        if batch_text:
            result["__batch__"] = {
                "text": batch_text,
                "mapping": batch_mapping
            }
            
        return result
    
    def _detect_field_type(self, value: Any) -> str:
        # Auto-detect field type based on content
        if isinstance(value, str):
            if self.is_html(value):
                return FieldType.WYSIWYG.value
            elif "\n" in value:
                return FieldType.TEXTAREA.value
            else:
                return FieldType.TEXT.value
        elif isinstance(value, dict):
            return FieldType.JSON.value
        else:
            return FieldType.STRING.value
    
    def _extract_metadata(self, value: Any, field_type: str) -> Dict[str, Any]:
        # Extract metadata to preserve during translation
        metadata = {}
        
        if field_type == FieldType.WYSIWYG.value and isinstance(value, str):
            metadata["html_structure"] = self._extract_html_structure(value)
        
        return metadata
    
    def _extract_html_structure(self, html: str) -> Dict[str, Any]:
        # Extract HTML structure for preservation
        soup = BeautifulSoup(html, 'html.parser')
        return {
            "tags": [tag.name for tag in soup.find_all()],
            "classes": [cls for tag in soup.find_all() for cls in tag.get("class", [])],
            "attributes": {tag.name: [attr for attr in tag.attrs if attr != "class"] 
                          for tag in soup.find_all() if tag.attrs}
        }
    
    def _get_nested_value(self, data: Dict[str, Any], path: str) -> Any:
        # Get value from nested dictionary using dot notation and array indices
        if not data or not path:
            return None
            
        parts = re.findall(r'([^\[\]\.]+)|\[(\d+)\]', path)
        current = data
        
        for part in parts:
            key, index = part
            
            if key and isinstance(current, dict):
                if key not in current:
                    return None
                current = current[key]
            elif index and isinstance(current, list):
                idx = int(index)
                if idx >= len(current):
                    return None
                current = current[idx]
            else:
                return None
                
        return current
    
    def is_html(self, text: str) -> bool:
        # Check if content is HTML
        return bool(re.search(r'<[^>]+>', text))
    
    def extract_text_from_html(self, html: str) -> List[Dict[str, Any]]:
        # Extract text nodes from HTML for translation
        soup = BeautifulSoup(html, 'html.parser')
        text_nodes = []
        
        for element in soup.find_all(text=True):
            if element.strip():
                text_nodes.append({
                    'text': element.strip(),
                    'path': self._get_element_path(element),
                    'parent_tag': element.parent.name if element.parent else None,
                    'parent_attrs': element.parent.attrs if element.parent else {}
                })
        
        return text_nodes
    
    def _get_element_path(self, element) -> str:
        # Generate a path to the element for reassembly
        path = []
        parent = element.parent
        while parent:
            siblings = parent.find_all(parent.name, recursive=False)
            if len(siblings) > 1:
                index = siblings.index(parent)
                path.append(f"{parent.name}[{index}]")
            else:
                path.append(parent.name)
            parent = parent.parent
        return "." + ".".join(reversed(path))
        
    def reassemble_html(self, original_html: str, translated_nodes: List[Dict[str, Any]]) -> str:
        # Reassemble HTML with translated text nodes
        soup = BeautifulSoup(original_html, 'html.parser')
        
        for node in translated_nodes:
            path = node['path']
            translated_text = node['translated_text']
            
            # Find the element using the path and update it
            # Implementation depends on how paths are structured
            # This is a simplified version
            elements = soup.select(path)
            if elements:
                elements[0].string = translated_text
                
        return str(soup)
    
    def process_ai_structured_data(self, structured_data: Dict[str, Any], field_config: Dict[str, Any]) -> Dict[str, Any]:
        # Process structured data from AI providers
        result = {}
        
        # Handle different AI provider formats
        if "translations" in structured_data:
            # Format: {"translations": [{"text": "...", "detected_language": "...", "to": "..."}]}
            translations = structured_data.get("translations", [])
            for i, translation in enumerate(translations):
                if i < len(field_config.get("field_paths", [])):
                    path = field_config["field_paths"][i]
                    result[path] = {
                        "value": translation.get("text", ""),
                        "type": field_config.get("field_types", {}).get(path, FieldType.TEXT.value),
                        "metadata": {
                            "detected_language": translation.get("detected_language"),
                            "target_language": translation.get("to")
                        }
                    }
        elif "choices" in structured_data:
            # Format used by some AI providers with choices array
            choices = structured_data.get("choices", [])
            if choices and "message" in choices[0]:
                content = choices[0].get("message", {}).get("content", "")
                # Try to parse as JSON if it looks like JSON
                if content.strip().startswith("{") and content.strip().endswith("}"):
                    try:
                        parsed = json.loads(content)
                        for path in field_config.get("field_paths", []):
                            if path in parsed:
                                result[path] = {
                                    "value": parsed[path],
                                    "type": field_config.get("field_types", {}).get(path, FieldType.TEXT.value),
                                    "metadata": {}
                                }
                    except json.JSONDecodeError:
                        # Not valid JSON, treat as single text
                        if field_config.get("field_paths"):
                            result[field_config["field_paths"][0]] = {
                                "value": content,
                                "type": FieldType.TEXT.value,
                                "metadata": {}
                            }
        
        return result
        
    def handle_directus_relations(self, content: Dict[str, Any], field_config: Dict[str, Any]) -> Dict[str, Any]:
        # Process Directus relation fields
        result = {}
        relation_fields = [path for path, type_info in field_config.get("field_types", {}).items() 
                          if type_info == FieldType.RELATION.value]
        
        for path in relation_fields:
            relation_data = self._get_nested_value(content, path)
            if relation_data:
                # Handle different relation types (o2m, m2o, m2m)
                if isinstance(relation_data, list):
                    # o2m or m2m relation
                    result[path] = [item["id"] for item in relation_data if "id" in item]
                elif isinstance(relation_data, dict) and "id" in relation_data:
                    # m2o relation
                    result[path] = relation_data["id"]
                    
        return result
    
    def handle_directus_translations(self, content: Dict[str, Any], field_config: Dict[str, Any], 
                                    language: str) -> Dict[str, Any]:
        # Handle Directus translation patterns
        translation_pattern = field_config.get("directus_translation_pattern", 
                                             DirectusTranslationPattern.COLLECTION_TRANSLATIONS.value)
        
        if translation_pattern == DirectusTranslationPattern.COLLECTION_TRANSLATIONS.value:
            # Standard Directus pattern: collection_translations table with languages
            return self._handle_collection_translations(content, field_config, language)
        elif translation_pattern == DirectusTranslationPattern.LANGUAGE_COLLECTIONS.value:
            # Language-specific collections pattern
            return self._handle_language_collections(content, field_config, language)
        else:
            # Custom pattern, use regular field extraction
            return self.extract_fields(content, field_config, language)
    
    def _handle_collection_translations(self, content: Dict[str, Any], field_config: Dict[str, Any], 
                                      language: str) -> Dict[str, Any]:
        # Handle standard Directus translation pattern with collection_translations
        result = {}
        primary_collection = field_config.get("primary_collection")
        
        if not primary_collection or not content.get("id"):
            return result
            
        # Structure for collection_translations
        result = {
            "id": None,  # Will be auto-generated or updated if exists
            primary_collection + "_id": content.get("id"),
            "languages_code": language,
        }
        
        # Add translatable fields
        extracted = self.extract_fields(content, field_config, language)
        for path, field_data in extracted.items():
            if "__batch__" not in path:  # Skip batch metadata
                field_name = path.split(".")[-1]  # Get the field name without path
                result[field_name] = field_data.get("value")
                
        return result
    
    def _handle_language_collections(self, content: Dict[str, Any], field_config: Dict[str, Any], 
                                   language: str) -> Dict[str, Any]:
        # Handle language-specific collections pattern
        result = {}
        primary_collection = field_config.get("primary_collection")
        
        if not primary_collection or not content.get("id"):
            return result
            
        # Structure for language collections (e.g., articles_en, articles_fr)
        result = {
            "id": content.get("id"),  # Same ID as primary content
        }
        
        # Add translatable fields
        extracted = self.extract_fields(content, field_config, language)
        for path, field_data in extracted.items():
            if "__batch__" not in path:  # Skip batch metadata
                field_name = path.split(".")[-1]  # Get the field name without path
                result[field_name] = field_data.get("value")
                
        return result
        
    def sanitize_content(self, content: Dict[str, Any], field_config: Dict[str, Any]) -> Dict[str, Any]:
        # Sanitize content before processing
        sanitized = {}
        
        for path, field_data in content.items():
            if path == "__batch__":
                # Handle batch data
                batch_text = field_data.get("text", [])
                batch_mapping = field_data.get("mapping", {})
                sanitized_batch = []
                
                for text in batch_text:
                    if isinstance(text, str):
                        if self.is_html(text):
                            # Sanitize HTML content
                            soup = BeautifulSoup(text, 'html.parser')
                            for script in soup(["script", "style"]):
                                script.decompose()
                            sanitized_batch.append(str(soup))
                        else:
                            sanitized_batch.append(text)
                    else:
                        sanitized_batch.append(text)
                        
                sanitized["__batch__"] = {
                    "text": sanitized_batch,
                    "mapping": batch_mapping
                }
            else:
                value = field_data.get("value")
                field_type = field_data.get("type")
                
                if field_type == FieldType.WYSIWYG.value and isinstance(value, str):
                    # Sanitize HTML content
                    soup = BeautifulSoup(value, 'html.parser')
                    # Remove potentially dangerous tags/attributes
                    for script in soup(["script", "style"]):
                        script.decompose()
                    sanitized[path] = {
                        "value": str(soup),
                        "type": field_type,
                        "metadata": field_data.get("metadata", {})
                    }
                else:
                    sanitized[path] = field_data
                    
        return sanitized
```

# Test Strategy:
1. Unit test field extraction from nested objects
2. Test HTML detection and processing
3. Verify field configuration storage and retrieval
4. Test JSON path parsing with various path formats
5. Validate HTML content extraction and reassembly
6. Test with different content structures
7. Verify handling of missing fields
8. Test Directus collection structure support
9. Verify translation collection handling
10. Test field type detection for various content types
11. Validate RTL language field mapping
12. Test content sanitization and validation
13. Verify relation field handling
14. Test metadata preservation during translation process
15. Validate custom field transformations
16. Test with real Directus collection examples
17. Verify HTML structure preservation during translation
18. Test standard Directus translation structure (collection_translations)
19. Verify language collections handling in Directus
20. Test structured data processing from AI providers
21. Validate batch operations for multiple collection fields
22. Test performance of batch vs. individual field processing
23. Verify correct handling of different Directus translation patterns
24. Test with complex nested Directus structures
25. Test API endpoints for field mapping configuration
26. Verify caching system for field configurations
27. Test operation logging functionality
28. Validate support for OpenAI, Anthropic, Mistral, and DeepSeek response formats
29. Test database initialization scripts
30. Verify integration with existing translation services
31. End-to-end testing of the complete translation workflow
32. Test structured translation API endpoints
33. Verify preview and validation endpoints functionality
34. Test IntegratedTranslationService with various content types
35. Validate client-specified provider selection
36. Test batch translation support for cost efficiency
37. Test Directus webhook endpoints for automatic content processing
38. Verify HMAC signature verification for webhook security
39. Test webhook integration with different Directus translation patterns
40. Validate webhook error handling and infinite loop prevention

# Subtasks:
## 4.1. Test database initialization [completed]
### Dependencies: None
### Description: ✅ COMPLETED: Database initialization testing successful!

## Test Results:
- ✅ Database tables already exist and are accessible
- ✅ Field mapping API endpoints working perfectly  
- ✅ Field configuration save/retrieve operations working
- ✅ Field extraction with batch processing working
- ✅ HTML content detection and processing working
- ✅ Processing logs being created properly

## API Tests Performed:
1. **Created field configuration**: Successfully created config for "test_client/articles" with 3 field paths
2. **Retrieved configuration**: Successfully retrieved saved configuration  
3. **Field extraction**: Successfully extracted fields from sample content with:
   - Batch processing for text fields (title, description)
   - Individual processing for HTML content (content.text) 
   - Proper field type detection (text, textarea, wysiwyg)
   - Processing time: 21ms

## Database Status:
- Tables: field_configs (3 records), field_processing_logs (8 records)
- Connection: Working from Docker app container
- Field mapping fully operational

The database initialization is complete and functional. Ready for Subtask 4.2: Integration with translation services.
### Details:


## 4.2. Integrate with translation services [completed]
### Dependencies: None
### Description: ✅ COMPLETED: Successfully integrated the field mapping system with flexible AI translation providers, creating a complete end-to-end translation workflow that follows the current architecture approach.

## ✅ Components Completed in Subtask 4.2

### 1. IntegratedTranslationService Implementation
- **Core service** combining FieldMapper + FlexibleTranslationService
- **Client-specified providers** - Correctly uses flexible selection (NOT cascading fallback)
- **Structured content processing** with intelligent field extraction
- **Batch translation support** for cost efficiency
- **HTML structure preservation** during translation
- **Directus translation patterns** (collection_translations, language_collections)

### 2. New API Endpoints Added (to `/api/translation.py`)
- `POST /api/v1/translate/structured` - Complete structured content translation
- `POST /api/v1/translate/preview` - Preview extractable fields without translation cost
- `POST /api/v1/translate/validate` - Validate translation requests before processing

### 3. Complete Translation Workflow Pipeline
1. **Field Configuration** → Retrieved from database per client/collection
2. **Field Extraction** → Smart extraction using FieldMapper with type detection
3. **Content Sanitization** → Security cleaning of HTML/dangerous content
4. **AI Translation** → Uses FlexibleTranslationService with client's provider choice
5. **Content Reconstruction** → Rebuilds content with translations
6. **Directus Patterns** → Applies appropriate collection structure format

## ✅ Live Testing Results

**Integration Testing**: ✅ All Systems Working
- Field config retrieval: 3 field paths configured ✅
- Field extraction: 2 extractable fields detected ✅
- API endpoint: HTTP 200 successful response ✅

**API Response Example**: Field extraction correctly identifying:
- `title` (text type, batch processing enabled)
- `description` (textarea type, batch processing enabled)  
- `content.text` (wysiwyg type, HTML structure preserved)

## 🔧 Architecture Alignment Confirmed

**Successfully implemented current project approach**:
- ✅ **Flexible provider selection** instead of cascading fallback
- ✅ **Client-provided API keys** per request (never stored)
- ✅ **Field mapping configuration** per client/collection from database
- ✅ **Cost-aware optimization** through existing Redis caching system
- ✅ **Directus CMS native integration** patterns

## 📁 New Files Created

- `app/services/integrated_translation_service.py` - Main integration orchestrator (473 lines)
- Enhanced `app/api/translation.py` - Added structured translation endpoints (+150 lines)
- Docker environment testing confirmed working

Core translation functionality is now operational and ready for Directus webhook integration!
### Details:


## 4.3. Add Directus webhook endpoints [completed]
### Dependencies: None
### Description: ✅ COMPLETED: Successfully implemented comprehensive Directus webhook system with the following components:

### 1. Complete Webhook API (`app/api/webhooks.py`)
- **Main translation webhook** at `/api/v1/webhooks/directus/translate`
- **Configuration validation** endpoint for pre-setup testing
- **Testing endpoint** with dry-run support for safe testing
- **Information endpoint** with complete setup documentation
- **Health monitoring** endpoint for webhook service status

### 2. Production-Ready Security
- **HMAC signature verification** (SHA-256/SHA-1 support)
- **Comprehensive input validation** and sanitization
- **API key validation** integration 
- **Infinite loop prevention** for translation collections
- **Error handling** with detailed metadata

### 3. Directus Integration Patterns
- **Standard collection_translations** pattern (recommended)
- **Language-specific collections** pattern (articles_ar, articles_bs)
- **Custom translation patterns** for flexible workflows
- **Complete automation** from Directus → LocPlat → Translation → Storage

### 4. Testing & Validation
- ✅ **All webhook tests passing** in Docker environment
- ✅ **Request/response validation** working correctly
- ✅ **Security signature verification** tested and confirmed
- ✅ **FastAPI integration** - all routes accessible
- ✅ **Field mapping integration** confirmed working

### 5. Documentation Created
- **Complete webhook integration guide** with Directus Flow examples
- **Security best practices** and production configuration
- **Testing procedures** and troubleshooting guidance
- **API documentation** with request/response examples

The webhook system is now fully operational and ready for production Directus integration!
### Details:


## 4.4. Implement Redis caching layer [to-do]
### Dependencies: None
### Description: Connect with Redis caching system from Task #3 to improve performance of field mapping operations. This implementation should include:

1. Integration with the existing Redis caching layer for field configurations
2. Cache field mapping configurations with appropriate TTL settings
3. Implement cache invalidation strategies for configuration updates
4. Add cache hit/miss metrics for monitoring
5. Optimize batch processing with Redis pipeline operations
6. Ensure thread-safe cache operations
7. Implement fallback mechanisms for Redis connection failures
8. Add configuration options for cache behavior
9. Document Redis integration patterns for field mapping
### Details:


## 4.5. End-to-end testing [to-do]
### Dependencies: None
### Description: Perform comprehensive end-to-end testing of the complete translation workflow with field mapping. Testing should cover:

1. Complete workflow from content submission to translated content storage
2. Directus webhook integration with various collection patterns
3. Field extraction and mapping for different content types
4. Translation service integration with multiple AI providers
5. Error handling and recovery mechanisms
6. Performance testing with large content volumes
7. Security testing of webhook endpoints
8. Redis caching integration testing
9. Validation of HTML structure preservation
10. Testing of RTL language support
11. Batch processing optimization verification
12. Documentation of test results and performance metrics
### Details:


